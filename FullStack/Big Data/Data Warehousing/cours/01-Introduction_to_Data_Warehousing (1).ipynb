{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3925fe8-76db-471f-8261-4033bcd297fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction to Data Warehousing\n",
    "\n",
    "The concept of Data Warehousing originated at IBM in the 80's. The goal of the initial research was to provide a framework to transfer data from operational systems to business intelligence departments, avoiding the cost and technical challenges of high redundancy.\n",
    "\n",
    "## What will you learn in this course? 🧐🧐\n",
    "\n",
    "This lecture will introduce the concept of data warehousing and why do we need it. Here's the outline:\n",
    "\n",
    "* Why analysts cannot work directly on business databases?\n",
    "* Data Warehouse VS Data Lake\n",
    "* Data Warehouse VS traditional databases\n",
    "    * Key differences\n",
    "* Cloud vendors\n",
    "* Amazon Redshift\n",
    "    * Setup your own Redshift cluster\n",
    "    * Tear down your Redshift cluster when you are done\n",
    "* Using Redshift in PySpark\n",
    "    * Writing to Redshift from PySpark DataFrame\n",
    "    * Reading from Redshift onto a PySpark DataFrame\n",
    "\n",
    "## Why analysts cannot work directly on business databases? 🤔🤔\n",
    "\n",
    "Business databases must stay clean at all cost: allowing Data Analysis or Data Scientist to access it introduces a breach.\n",
    "\n",
    "Moreover, most of the time, unstructured data (i.e., not stored in any kind of databases) is required to do performant analysis. \n",
    "\n",
    "A Warehousing solution allows the company to aggregate and store its data needed for analysis, without altering the databases used for operations.\n",
    "\n",
    "## Data Warehouse VS Data Lake 🗄️🆚🌊\n",
    "\n",
    "You often hear both when discussing Big Data, however they are very different.\n",
    "\n",
    "Data Lakes are a big pool of raw data, with no defined purposes: we store this unstructured data in prevision of future usage.\n",
    "\n",
    "Data Warehouse holds **processed** and **structured** data, ready to be used for advanced analytics. \n",
    "\n",
    "Most of the time, data that ends up in the Warehouse was previously stored in the Lake. \n",
    "\n",
    "- Step 1: Data is collected and stored in its raw form in a Data Lake,\n",
    "- Step 2: Data is extracted from the Lake, cleaned and processed,\n",
    "- Step 3: Data is loaded in the warehouse, ready to be queried.\n",
    "\n",
    "## Data Warehouse VS traditional databases 🗄️🗄️🗄️🆚🗄️\n",
    "\n",
    "Roughly, a Data Warehouse **is** a relational database. It's just a little more than that.\n",
    "\n",
    "### Key differences 🔑\n",
    "\n",
    "1. The Warehouse can hold data from many databases.\n",
    "2. Any data stored in the Warehouse is stored for **analytics purposes only**.\n",
    "3. Data within a warehouse has been processed to simplify the analysis, and avoid the need for SQL queries that spread on 300 lines.\n",
    "4. Whereas databases are optimized for extracting rows (or observations), data warehouses are optimized to have a performance boost on columns (or fields).\n",
    "\n",
    "In a nutshell: warehouses are optimized for performant analysis.\n",
    "\n",
    "**A warehouse is the perfect candidate for `LOAD` destination in ETL pipelines.**\n",
    "\n",
    "## Cloud vendors ☁️☁️\n",
    "\n",
    "- BigQuery, owned by Google, and part of the Google Cloud Platform,\n",
    "- Redshift, owned by Amazon and part of the AWS platform,\n",
    "- Snowflake,\n",
    "- ...\n",
    "\n",
    "As always when choosing between different vendors, the cost structure is one the most important aspects to check. For instance, BigQuery storage is **much** cheaper than Redshift, but querying data on Redshift is **free** whereas it costs about 5 dollars/TB on BigQuery. Depending on your need, one solution might be more suitable than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Redshift 🔴🔴\n",
    "\n",
    "Redshift is the Data Warehousing solution from Amazon Web Services. As every services of the AWS family, Redshift is **Cloud-based**: you only pay for the compute and storage, and you don't have to take care of maintenance costs, or scaling the hardware to support an increasing load.\n",
    "\n",
    "### Creating your Redshift\n",
    "\n",
    "Go to your AWS Console and look for Redshift. Check you are on a good location, here `Paris`. Click on _Create a cluster_!\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift001.png)\n",
    "\n",
    "Be sure to **select the ⚠️ free tier ⚠️!**\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift002.png)\n",
    "\n",
    "Enter a user (or leave the default one) and a password:\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift003.png)\n",
    "\n",
    "Click on **Create a cluster**!\n",
    "\n",
    "Wait ⏳ your cluster to be on stage:\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift004.png)\n",
    "\n",
    "When the status is green 🟢 it means your cluster is ready! Well, almost. We are going to open our Redshift to the world, so as to push data easily from anywhere.\n",
    "\n",
    "To do so click on your cluster, then on the panel choose _Actions_ and _Modify public access_.\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift005.png)\n",
    "\n",
    "Activate the access:\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift006.png)\n",
    "\n",
    "And that's it! 🎉\n",
    "\n",
    "The only information we are going to need later on is located in the 👉 **URL JDBC**, located here:\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error\n",
    "\n",
    "If you fall on your this error:\n",
    " \n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift003b-error-cluster-subnet.png)\n",
    " \n",
    "Go to Config, then Subnet group:\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift003b1-error-cluster-subnet.png)\n",
    "\n",
    "Then create a Subnet group:\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift003b2-error-cluster-subnet.png)\n",
    "\n",
    "Simply put a description, select a VPC (the default one is okay), disponibility zone, sub-network, click on Add a subnet and finally create the group.\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/Redshift003b3-error-cluster-subnet.png)\n",
    "\n",
    "You are done! 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to tear down your Redshift?\n",
    "\n",
    "When you have finished working with your Redshift cluster we advise your to ⚠️ **tear down your Redshift cluster so as to avoid too much costs.** ⚠️\n",
    "\n",
    "It is easy, just follow the following steps.\n",
    "\n",
    "Go to your Redshift clusters panel and select your cluster. Then go to _Actions_ and click _Delete_:\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/RedshiftDown001.png)\n",
    "\n",
    "👉 A prompt will ask you to confirm, ⚠️ **you have to deselect the _Create a instant snapshot_ option.** ⚠️ Otherwise you will be charged for this storage.\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/RedshiftDown002.png)\n",
    "\n",
    "In the following screenshot, we are good to go:\n",
    "\n",
    "![](https://full-stack-assets.s3.eu-west-3.amazonaws.com/M05-D03-Redshift/RedshiftDown003.png)\n",
    "\n",
    "Finally, you can also delete the group subnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Redshift in PySpark\n",
    "\n",
    "### Writing to Redshift from PySpark DataFrame ✨➡🔴\n",
    "\n",
    "Let's show you how to use Redshift with PySpark. First, we are creating a simple Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dict = {'a': [1,2,3], 'b': [2,3,4], 'c': [3,4,5], 'd':[np.NaN,0,1], 'e':[\"apple\",\"banana\",\"orange\"]}\n",
    "\n",
    "pandas_df = pd.DataFrame.from_dict(\n",
    "    data_dict\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to fill some informations:\n",
    "\n",
    "> The `redshift_path_full` is the URL JDBC from the cluster panel we mentioned above 👆. Remember? 🙂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDSHIFT_USER = 'YOUR_REDSHIFT_USERNAME'\n",
    "REDSHIFT_PASSWORD = 'YOUR_REDSHIFT_PASSWORD'\n",
    "\n",
    "REDSHIFT_FULL_PATH = \"URL_JDBC\" # don't forget to replace \"redshift\" by \"postgresql\"\n",
    "                                # for example it'll look like:\n",
    "                                # \"jdbc:postgresql://redshift-cluster-1.csssws1edn9m.eu-west-3.redshift.amazonaws.com:5439/dev\"\n",
    "REDSHIFT_TABLE = 'NAME_OF_THE_TABLE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write to our Redshift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"overwrite\"\n",
    "\n",
    "properties = {\"user\": REDSHIFT_USER, \"password\": REDSHIFT_PASSWORD, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "df.write.jdbc(url=REDSHIFT_FULL_PATH, table=REDSHIFT_TABLE, mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 `mode` to choose from are:\n",
    "\n",
    "- `overwrite`: drop the table if it exists, then load the data in a new one,\n",
    "- `append`: create the table if it does not exists, else append the data to the existing table,\n",
    "- `error` (default): create the table or raise an error if it exists,\n",
    "- `ignore`: same as `overwrite`, but does nothing if table already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from Redshift onto a PySpark DataFrame 🔴➡✨\n",
    "\n",
    "We can read from our Redshift in few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\"user\": REDSHIFT_USER, \"password\": REDSHIFT_PASSWORD, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "table = sqlContext.read.jdbc(url=redshift_path_full, table=REDSHIFT_TABLE, properties=properties)\n",
    "\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this can be useful, it is also possible to query your database using the Redshift query editor directly, which is most likely what data analysts and business analysts would be doing in a real-life context.\n",
    "\n",
    "Congrats! 👏 You just created your first data warehouse using Redshift! Do not forget to 👉 **[tear down your Redshift cluster](#how-to-tear-down-your-redshift)** 👈 or you run the risk of being charged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources 📚📚\n",
    "\n",
    "- [A nice article on Alooma's blog](https://www.alooma.com/blog/database-vs-data-warehouse)\n",
    "- [Amazon Redshift](https://docs.databricks.com/data/data-sources/aws/amazon-redshift.html#setting-a-custom-column-type)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-Introduction_to_Data_Warehousing",
   "notebookOrigID": 4407832410620818,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
