{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01-Translation_with_attention.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5NFuUYKHABlD"},"source":["# Translations 2.0 - Neural Machine Translation\n","\n","According to the Google paper [*Attention is all you need*](https://arxiv.org/abs/1706.03762), you only need layers of Attention to make a Deep Learning model understand the complexity of a sentence. We will try to implement this type of model for our translator. \n","\n","### Data import \n","\n","You will have the same `.txt` file containing a sentence with its translation separated by a tab (`\\t`). You will have to import this data and read it via `pandas`.\n","\n","Your data can be found on this link: https://go.aws/38ECHUB\n","\n","### Preprocessing \n","\n","The whole purpose of your preprocessing is to express your (French) entry sentence in a sequence of clues.\n","\n","i.e. :\n","\n","* je suis heureux---> `[123, 21, 34, 0, 0, 0, 0, 0]`\n","\n","This gives a *shape* -> `(batch_size, max_len_of_a_sentence)`.\n","\n","The zeros correspond to what are called [*padded_sequences*](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) which allow all word sequences to have the same length across a set of sequences (mandatory for your algorithm). \n","\n","You will run the same preprocessing on the target sequences, and add a `<start>` token at the beginning of each sequence.\n","\n","* `<start>` I am happy ---> `[1, 43, 2, 42, 0, 0]`\n","\n","### Modeling \n","\n","For modeling, you will need to set up layers of attention. You'll need to: \n","\n","* Create an `Encoder` class that inherits from `tf.keras.Model`.\n","* Create a Bahdanau Attention Layer that will be a class that inherits `tf.keras.layers.Layer`\n","* Finally create a `Decoder` class that inherits from `tf.keras.Model`.\n","\n","\n","You will need to create your own cost function as well as your own training loop. \n","\n","\n","### Tips \n","\n","Don't take the whole dataset at the beginning for your experiments, just take 5000 or even 3000 sentences. This will allow you to iterate faster and avoid bugs simply related to your need for computing power, and memory space.\n","\n","Good Luck!\n"]},{"cell_type":"code","metadata":{"id":"2qUhyNPnhBtk","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1636996857858,"user_tz":-60,"elapsed":198,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"16dcd3fe-be81-472b-a345-b80033b28729"},"source":["# Import necessaries librairies\n","import pandas as pd\n","import numpy as np \n","import tensorflow_datasets as tfds\n","import tensorflow as tf \n","tf.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.7.0'"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"VSWse8KgHmQw"},"source":["## Importing data & Preprocessing\n","\n","1. Load the data using the following url https://go.aws/38ECHUB you can read this using `pd.read_csv` with the `\"\\t\"` delimiter and `header=None`"]},{"cell_type":"code","metadata":{"id":"U2-Sd6lq_8ax"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yj34GLiihGw1","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1636996860208,"user_tz":-60,"elapsed":1699,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"8d16ddaa-c4a5-49f6-c76d-c2a1cf97be4f"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0           1\n","0   Go.        Va !\n","1   Hi.     Salut !\n","2  Run!     Cours !\n","3  Run!    Courez !\n","4  Wow!  Ça alors !"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"2jiXcCi0FFSr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636996860210,"user_tz":-60,"elapsed":51,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"676cf847-b5b4-4149-9ac4-3cf205cb8fc0"},"source":["len(doc)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["160538"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"syRLJiG0YkXR"},"source":["2. Create an object `doc` containing the first 5000 rows from the file."]},{"cell_type":"code","metadata":{"id":"YrcFSfBZMuQ7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8CKnrs_wYuYn"},"source":["3. Add the word `<start>` to the beginning of each target sentence in order to create a new column named `padded_en`"]},{"cell_type":"code","metadata":{"id":"7Z1Ih3M2jVr_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVMl6744jmrq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2fZfDCqFPko","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1636996860215,"user_tz":-60,"elapsed":23,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"8ad461b9-b087-4555-e7b0-a066323956ed"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;start&gt; Go.</td>\n","      <td>Va !</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;start&gt; Hi.</td>\n","      <td>Salut !</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>Cours !</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>Courez !</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>&lt;start&gt; Wow!</td>\n","      <td>Ça alors !</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4995</th>\n","      <td>&lt;start&gt; I am so sorry.</td>\n","      <td>Je suis tellement désolé !</td>\n","    </tr>\n","    <tr>\n","      <th>4996</th>\n","      <td>&lt;start&gt; I am so sorry.</td>\n","      <td>Je suis tellement désolée !</td>\n","    </tr>\n","    <tr>\n","      <th>4997</th>\n","      <td>&lt;start&gt; I am very sad.</td>\n","      <td>Je suis très triste.</td>\n","    </tr>\n","    <tr>\n","      <th>4998</th>\n","      <td>&lt;start&gt; I ate a donut.</td>\n","      <td>J'ai mangé un beignet.</td>\n","    </tr>\n","    <tr>\n","      <th>4999</th>\n","      <td>&lt;start&gt; I ate quickly.</td>\n","      <td>J'ai mangé rapidement.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5000 rows × 2 columns</p>\n","</div>"],"text/plain":["                           0                            1\n","0                <start> Go.                         Va !\n","1                <start> Hi.                      Salut !\n","2               <start> Run!                      Cours !\n","3               <start> Run!                     Courez !\n","4               <start> Wow!                   Ça alors !\n","...                      ...                          ...\n","4995  <start> I am so sorry.   Je suis tellement désolé !\n","4996  <start> I am so sorry.  Je suis tellement désolée !\n","4997  <start> I am very sad.         Je suis très triste.\n","4998  <start> I ate a donut.       J'ai mangé un beignet.\n","4999  <start> I ate quickly.       J'ai mangé rapidement.\n","\n","[5000 rows x 2 columns]"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"Z4gI_rGRY1Zr"},"source":["4. Create two objects : `tokenizer_fr` and `tokenizer_en` that will be instances of the `tf.keras.preprocessing.text.Tokenizer` class. \n","\n","Be careful! Since we added a special token containing special characters, make sure you setup the tokenizers right so this token is well interpreted! (use the `filters` argument for example)."]},{"cell_type":"code","metadata":{"id":"bQQOU7OI7-xV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zdcfQ9k9Y_Jo"},"source":["5. Fit the tokenizers on the french, and english sentences respectively."]},{"cell_type":"code","metadata":{"id":"CwCN5z21xo5H"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HKHvD-syZH4_"},"source":["6. Create three new columns in your Dataframe for the encoded french, english sentences."]},{"cell_type":"code","metadata":{"id":"zIwlGhNDykzn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUjLaIjB0e-e","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1636996860778,"user_tz":-60,"elapsed":63,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"094461c2-5357-48b5-f436-33bffa481ff3"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>fr_indices</th>\n","      <th>en_indices</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;start&gt; Go.</td>\n","      <td>Va !</td>\n","      <td>[36]</td>\n","      <td>[1, 11]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;start&gt; Hi.</td>\n","      <td>Salut !</td>\n","      <td>[404]</td>\n","      <td>[1, 616]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>Cours !</td>\n","      <td>[1212]</td>\n","      <td>[1, 111]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>Courez !</td>\n","      <td>[1213]</td>\n","      <td>[1, 111]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>&lt;start&gt; Wow!</td>\n","      <td>Ça alors !</td>\n","      <td>[22, 1214]</td>\n","      <td>[1, 872]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              0           1  fr_indices en_indices\n","0   <start> Go.        Va !        [36]    [1, 11]\n","1   <start> Hi.     Salut !       [404]   [1, 616]\n","2  <start> Run!     Cours !      [1212]   [1, 111]\n","3  <start> Run!    Courez !      [1213]   [1, 111]\n","4  <start> Wow!  Ça alors !  [22, 1214]   [1, 872]"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"b88_o-58ZVjG"},"source":["7. It's rather difficult to work with sequences with variable length, use zero-padding to normalize the length of all the sequences in each category."]},{"cell_type":"code","metadata":{"id":"-6miVbPY0xZw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tUOuoN8TZc5M"},"source":["8. What are the shapes of the arrays you just created for the french, and english sentences?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"va8k_PnnZhw1","executionInfo":{"status":"ok","timestamp":1636996860780,"user_tz":-60,"elapsed":57,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"b4148049-9613-466f-8bfe-942ac36a0e31"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 10)"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1BIxc5eeZkrf","executionInfo":{"status":"ok","timestamp":1636996860780,"user_tz":-60,"elapsed":45,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"ad7eed35-a484-4374-ba9d-c86927013320"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 5)"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"yXmSlILiZoZW"},"source":["9. Use `sklearn` `train_test_split` function to divide your sample into train and validation sets."]},{"cell_type":"code","metadata":{"id":"VrJp0Jk_B8g7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zn1bP-0hZwH9"},"source":["10. Set a `BATCH_SIZE` then create a `train`, and `val` tensor datasets, apply `.shuffle` on the `train` set and `.batch` on both sets."]},{"cell_type":"code","metadata":{"id":"_blzVK_dCIvi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oQ-tj3tBbHrg"},"source":["## Modeling\n","\n","1. Set up the following variables:\n","  * `n_embed` for the models' embedding output dimensions\n","  * `n_gru` for the models' gru number of units\n","  * `vocab_inp_size` for the french vocab size\n","  * `vocab_tar_size` for the english vocab size"]},{"cell_type":"code","metadata":{"id":"ueIigsED1nWq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l6fPfVW8cdNh"},"source":["### Encoder\n","\n","2. Define a class `encoder_maker` inheriting from `tf.keras.Model` that can instanciate and encoder type model according to the following schema: \n","\n","![bahdanau](https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Deep+Learning/attention/Attention-encoder-decoder.drawio.png)"]},{"cell_type":"code","metadata":{"id":"XbbOi7sYVDU2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SGfDVRdddYZH"},"source":["3. Define an instance of the class called... `encoder`!"]},{"cell_type":"code","metadata":{"id":"wYTHi-p7DRN7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hY2pMguseEuH"},"source":["4. Use the `__call__` method of `encoder` on some data to create an object `encoder_output`, and an `encoder_state` (remember your encoder has two different outputs!). Then print out `encoder_output`, and `encoder_state`."]},{"cell_type":"code","metadata":{"id":"4cmD6jUP74Rx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xzno1nYRDeEF","executionInfo":{"status":"ok","timestamp":1636996861011,"user_tz":-60,"elapsed":262,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"22feeb46-27c6-429a-8065-c77b0797ec1a"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 10, 256), dtype=float32, numpy=\n","array([[[-0.00253955,  0.01535846, -0.01031921, ..., -0.0036075 ,\n","          0.00527863, -0.03740017],\n","        [ 0.00914116,  0.01068447, -0.01797251, ..., -0.00501267,\n","         -0.02302877, -0.01950926],\n","        [-0.01169751,  0.00807895, -0.02574131, ..., -0.00959103,\n","         -0.0026719 , -0.02316577],\n","        ...,\n","        [ 0.01122705,  0.00844394,  0.0134197 , ...,  0.00524029,\n","          0.07471443, -0.0542349 ],\n","        [ 0.01121978,  0.00809082,  0.01428621, ...,  0.00589937,\n","          0.07630372, -0.05507544],\n","        [ 0.01115769,  0.00785431,  0.01476404, ...,  0.00638014,\n","          0.07713117, -0.0555497 ]]], dtype=float32)>"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHwCMwX6Di4J","executionInfo":{"status":"ok","timestamp":1636996861011,"user_tz":-60,"elapsed":37,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"44931875-57be-462d-fcca-d0a490ec3ba9"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n","array([[ 1.11576924e-02,  7.85430986e-03,  1.47640351e-02,\n","         1.07137319e-02,  3.64588830e-03,  1.67278796e-02,\n","         1.34425284e-03, -2.37888610e-03, -4.53043841e-02,\n","         3.18491012e-02,  1.95410363e-02,  1.14593888e-02,\n","         1.75643116e-02,  1.36310114e-02, -2.52847336e-02,\n","         2.52709687e-02, -1.18106306e-02, -8.87616407e-05,\n","         5.62975556e-03,  1.87446177e-02,  1.26475617e-02,\n","        -1.11991875e-02,  1.76293682e-03,  1.63977174e-03,\n","        -3.16058937e-03, -3.12474612e-02, -6.21723616e-03,\n","         6.01544650e-03,  4.35616225e-02, -2.77330466e-02,\n","         8.30608141e-03, -3.16027477e-02, -3.33204158e-02,\n","         3.90100740e-02, -5.50506823e-02,  4.36665788e-02,\n","        -3.52847390e-02, -1.49069668e-03, -9.91806202e-03,\n","        -4.39796485e-02, -3.00285988e-03,  4.04703580e-02,\n","         5.69241755e-02,  1.07780751e-02,  3.27506550e-02,\n","        -1.17954444e-02,  3.48997936e-02,  4.54235673e-02,\n","         3.46800350e-02, -1.01468414e-02, -2.19463883e-03,\n","         1.71453375e-02,  1.72622260e-02, -2.04144306e-02,\n","        -2.86175590e-03,  1.41735720e-02,  3.85188498e-02,\n","        -4.04113717e-02,  1.46956537e-02,  2.47726124e-02,\n","         6.68419152e-02,  8.17550719e-02,  2.88983602e-02,\n","        -5.81532391e-03, -3.50903310e-02,  3.27334180e-02,\n","        -2.17938274e-02,  7.41877407e-03,  2.08363228e-04,\n","         3.52962911e-02, -5.02163619e-02,  3.44684832e-02,\n","        -1.77331443e-03, -2.44398657e-02,  1.83889573e-03,\n","        -2.10562814e-03, -3.32124270e-02,  5.47173098e-02,\n","        -7.46122673e-02, -2.44816337e-02,  2.80682724e-02,\n","        -1.36383409e-02, -6.03674874e-02,  1.82685368e-02,\n","         1.08315041e-02,  2.53913272e-03,  2.82846577e-02,\n","         2.46416964e-02, -2.06950381e-02, -1.28740482e-02,\n","         2.58225016e-03, -1.92146357e-02,  2.74054762e-02,\n","         6.78047165e-03, -1.75133825e-03,  2.72310972e-02,\n","        -5.60040167e-03, -3.78058366e-02, -4.56359945e-02,\n","        -1.30275162e-02,  6.69119284e-02, -2.07106080e-02,\n","         3.29457689e-04, -2.13719229e-03,  1.97705086e-02,\n","        -2.68854946e-02, -2.85990536e-03,  1.83026846e-02,\n","        -1.05844419e-02,  2.32839175e-02,  2.30918862e-02,\n","        -1.16093280e-02,  2.09390130e-02,  7.31010363e-02,\n","         7.17132026e-03, -5.30096777e-02,  3.25558111e-02,\n","        -1.99336875e-02, -2.19371617e-02,  3.53238471e-02,\n","         3.91235054e-02,  1.28732407e-02,  9.16307140e-03,\n","        -9.96130425e-03,  3.33261378e-02, -2.66422033e-02,\n","        -2.58348174e-02,  3.96922417e-03,  4.26587835e-02,\n","         1.33152381e-02, -2.25821286e-02, -3.33150104e-03,\n","         3.41331922e-02,  4.28661928e-02, -1.39371660e-02,\n","        -1.95772182e-02, -6.55611604e-03,  2.27603167e-02,\n","         1.44937439e-02, -4.95234281e-02, -8.88027600e-04,\n","         3.65253538e-02, -4.23080102e-03,  1.55270621e-02,\n","        -1.96985025e-02,  1.17864311e-02,  2.50290092e-02,\n","        -4.02977876e-02, -1.89295691e-02,  1.34218503e-02,\n","        -3.26310880e-02,  2.95592044e-02, -1.68051687e-03,\n","        -1.48041993e-02,  2.03545857e-02,  3.37891541e-02,\n","         3.45946737e-02,  6.45851791e-02,  5.54657541e-02,\n","         2.00906508e-02,  3.88898887e-02,  5.10818288e-02,\n","        -2.69313361e-02,  2.00204514e-02,  2.92444956e-02,\n","         5.85977174e-02,  4.44275253e-02,  2.01823860e-02,\n","        -1.44512206e-02,  9.13147721e-03,  2.84043010e-02,\n","        -3.24683040e-02, -1.51330745e-02, -2.75351852e-02,\n","         2.21144539e-04,  1.18779615e-02,  1.43353511e-02,\n","        -1.39789479e-02,  8.40993598e-03,  9.77264065e-03,\n","        -1.09777832e-02,  2.81627700e-02,  1.32696088e-02,\n","         2.38553584e-02, -5.78375440e-03,  2.14757677e-03,\n","         2.97660492e-02,  2.08657458e-02, -7.22592548e-02,\n","         7.54972780e-03,  2.07735393e-02,  3.17171775e-02,\n","         1.50334947e-02, -1.35403580e-03, -3.40307131e-02,\n","         1.61658060e-02,  1.97951347e-02,  3.38566601e-02,\n","         2.26268638e-03, -1.96233150e-02,  5.89934848e-02,\n","        -2.01371945e-02, -3.90739515e-02, -2.15641446e-02,\n","         1.78161934e-02,  6.16456047e-02, -3.04355342e-02,\n","        -1.01928068e-02,  6.83094114e-02, -4.29121703e-02,\n","         5.60172051e-02, -1.29510155e-02,  9.56106558e-03,\n","        -5.46065532e-03,  1.49577465e-02,  1.68756545e-02,\n","         2.68484596e-02, -5.44731952e-02, -8.78459308e-03,\n","        -2.21012514e-02,  3.50673683e-02, -3.01737874e-03,\n","         2.25279462e-02,  2.62557566e-02, -2.19880156e-02,\n","         2.78455466e-02,  1.86560731e-02, -3.08100469e-02,\n","        -1.24076093e-02, -2.83751287e-03, -1.39470045e-02,\n","        -9.97778680e-03,  1.31150922e-02,  1.75651293e-02,\n","        -3.66678685e-02, -7.58219976e-03, -8.14166218e-02,\n","         1.35031734e-02, -4.33000065e-02, -5.36965877e-02,\n","        -1.24416151e-03, -6.20820327e-03,  1.81382243e-02,\n","        -2.22506393e-02,  5.40098138e-02,  3.28300372e-02,\n","        -2.56835986e-02,  2.41997000e-02,  1.63204651e-02,\n","        -1.45199997e-02,  9.29934066e-03, -5.09660318e-03,\n","         4.84175235e-02,  6.38013938e-03,  7.71311745e-02,\n","        -5.55496998e-02]], dtype=float32)>"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"Y-4Vni1Lek2f"},"source":["### Attention layer\n","\n","5. Create a `Bahdanau_attention_maker` class that lets you instanciate an attention layer that you will include in your decoder model. You may follow the instructions from this schema: \n","\n","![bahdanau](https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Deep+Learning/attention/Attention-encoder-decoder.drawio.png)\n","\n","And get inspiration (as much as you want) from the lecture's demo!"]},{"cell_type":"code","metadata":{"id":"Uq2nP5l_76LS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNUh8ANHfmP0"},"source":["6. Create an instance of the class called `attention_layer`."]},{"cell_type":"code","metadata":{"id":"KetnxQvd8bF0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZREk1XRfxzf"},"source":["7. Try out the `__call__` method on the `encoder_output`, and `encoder_state`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M4CORsJZD8m0","executionInfo":{"status":"ok","timestamp":1636996861013,"user_tz":-60,"elapsed":16,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"f618c45c-1f25-4eec-8212-e95014144c8f"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n"," array([[ 0.00601813,  0.00965912,  0.00083819,  0.0061054 ,  0.00741023,\n","          0.01029313,  0.01178071, -0.00325181, -0.03398979,  0.0228165 ,\n","          0.01502588,  0.00566412,  0.00525974,  0.00832514, -0.02218461,\n","          0.02579355, -0.01195296, -0.00294421, -0.00418221,  0.0133222 ,\n","          0.00819678, -0.00276295, -0.00685048, -0.00093522, -0.00660039,\n","         -0.01658776,  0.01074449,  0.01261417,  0.03227958, -0.01585295,\n","         -0.00188884, -0.01266814, -0.01212784,  0.02138761, -0.04008794,\n","          0.02293937, -0.01680868, -0.00520804, -0.00773489, -0.02959757,\n","         -0.01014823,  0.01726578,  0.03042765,  0.00458782,  0.02141881,\n","         -0.01147711,  0.02137397,  0.02372793,  0.02370477, -0.00983301,\n","         -0.00022486,  0.00519349,  0.00588073, -0.00999522, -0.00343707,\n","          0.01357444,  0.0222452 , -0.0225017 ,  0.01612515,  0.01630028,\n","          0.03240031,  0.04815333,  0.01252025, -0.00452948, -0.01315825,\n","          0.02074493, -0.00790152, -0.00717134,  0.0091586 ,  0.01574357,\n","         -0.02892154,  0.02702979, -0.00308935, -0.01582841,  0.00505671,\n","          0.00059313, -0.02405761,  0.03434255, -0.05081457, -0.01056708,\n","          0.02193979, -0.01368783, -0.02600022,  0.01173044,  0.00093904,\n","          0.00266143,  0.01007185,  0.01086613, -0.01940764, -0.00983234,\n","         -0.01143659, -0.00553253,  0.0198642 , -0.00343198,  0.00752841,\n","          0.0212313 , -0.01196357, -0.03005029, -0.02819345, -0.02361193,\n","          0.04209189, -0.02314679,  0.00584263,  0.00299146,  0.02183428,\n","         -0.02339369,  0.00096425,  0.01253968, -0.00709302,  0.00931077,\n","          0.00968174, -0.00103249,  0.01564814,  0.04166966,  0.00766981,\n","         -0.0407977 ,  0.00970643, -0.01084691, -0.01022166,  0.01720146,\n","          0.02136214,  0.00985264, -0.00241002, -0.01280839,  0.00916533,\n","         -0.01675962, -0.00360337, -0.00451947,  0.02455998, -0.0009068 ,\n","         -0.01763811,  0.00183136,  0.02608827,  0.02733473, -0.0158555 ,\n","         -0.01062223, -0.00600118,  0.01970402,  0.00481716, -0.03129836,\n","         -0.00707578,  0.02958761, -0.0054457 ,  0.02009272, -0.02068434,\n","          0.00361391,  0.01528193, -0.0256059 , -0.00798835,  0.00903744,\n","         -0.02127278,  0.01257785,  0.00157022, -0.00116231,  0.01087427,\n","          0.01755278,  0.01690084,  0.04417796,  0.04184927,  0.01079607,\n","          0.02388361,  0.0382755 , -0.0192136 ,  0.00992419,  0.02112957,\n","          0.03277587,  0.02946214,  0.0108204 , -0.01106619,  0.00472855,\n","          0.01156554, -0.02034682, -0.01060122, -0.01183932, -0.00302513,\n","          0.00534956,  0.01399262, -0.00157409,  0.00635226,  0.01266009,\n","          0.00272643,  0.00350576,  0.00774768,  0.01143405, -0.00625067,\n","          0.006657  ,  0.02138373,  0.00971934, -0.0437242 ,  0.01507954,\n","          0.01309135,  0.00789802,  0.00931053, -0.00810309, -0.02238726,\n","          0.00519034,  0.01721693,  0.02277517,  0.00777355, -0.00751196,\n","          0.03704073, -0.01030604, -0.0155938 , -0.01476832,  0.00881804,\n","          0.04548967, -0.02603988, -0.0031762 ,  0.04807117, -0.02064649,\n","          0.0357161 , -0.01367307,  0.000688  , -0.00011703,  0.01680751,\n","          0.00714586,  0.01557699, -0.03542333, -0.00487473, -0.00161721,\n","          0.01126893, -0.00588666,  0.01157947,  0.01605294, -0.01334963,\n","          0.01207571,  0.00277616, -0.01876066, -0.01032351,  0.0054581 ,\n","         -0.01658582, -0.0150857 ,  0.01445416,  0.00181205, -0.01930014,\n","         -0.01426476, -0.04924973,  0.00803269, -0.0244206 , -0.03103577,\n","          0.00719968, -0.0037109 ,  0.01007892, -0.01972632,  0.03754777,\n","          0.02275205, -0.01069855,  0.00984887,  0.01076948, -0.01255914,\n","          0.00184979, -0.01073251,  0.02569724,  0.00090984,  0.0439404 ,\n","         -0.04304252]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 10, 1), dtype=float32, numpy=\n"," array([[[0.09965546],\n","         [0.09701071],\n","         [0.09984754],\n","         [0.09995408],\n","         [0.1001896 ],\n","         [0.10040961],\n","         [0.10058235],\n","         [0.10070755],\n","         [0.10079346],\n","         [0.10084961]]], dtype=float32)>)"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"eectqIKPgGdM"},"source":["### Decoder\n","\n","8. Set up a `decoder_maker` class that will let you create decoder models according to the demo and the following schema: \n","\n","![bahdanau](https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Deep+Learning/attention/Attention-encoder-decoder.drawio.png)"]},{"cell_type":"code","metadata":{"id":"Ruo6sIk98eLs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pBkX8N8zgj9F"},"source":["9. Create an instance of the class called...... `decoder` !"]},{"cell_type":"code","metadata":{"id":"jPW5WmrY8hyT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KW5RF79Pg-AY"},"source":["10. Try out the decoder on some teacher forcing data and the encoder outputs."]},{"cell_type":"code","metadata":{"id":"OrRgi6Q-ELHb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zA00Pv8EEaQJ","executionInfo":{"status":"ok","timestamp":1636996861208,"user_tz":-60,"elapsed":201,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"f25fed40-f50f-4a50-9cec-3fd2b46c7fcd"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(1, 1258), dtype=float32, numpy=\n"," array([[0.0007928 , 0.0007945 , 0.00079508, ..., 0.00079173, 0.00080088,\n","         0.00080663]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n"," array([[ 0.0054262 , -0.00104087, -0.01437734,  0.0007463 , -0.00516099,\n","         -0.00511794, -0.02686284,  0.02661309,  0.0079101 , -0.00073266,\n","          0.00072341,  0.00591044, -0.00800723, -0.01275809,  0.02003843,\n","         -0.00484268,  0.00313709,  0.00535898,  0.01624558, -0.02003136,\n","         -0.02237273, -0.01843331, -0.01609821,  0.00016066,  0.00276765,\n","         -0.00078844,  0.01207494,  0.01926803, -0.01058505,  0.01601942,\n","          0.00838074,  0.01534662, -0.00091441, -0.00283618,  0.0260405 ,\n","         -0.00851085, -0.01272523,  0.00645019, -0.02285067,  0.00688291,\n","         -0.00792905, -0.00216173, -0.01906213,  0.01370876, -0.00043282,\n","         -0.00464217,  0.0030584 ,  0.009551  , -0.01040792, -0.01235285,\n","         -0.01967028,  0.01735196, -0.01187575,  0.00143144, -0.00967435,\n","          0.0055482 , -0.00232422, -0.01735605, -0.02062414,  0.00150056,\n","          0.00195196,  0.01853536,  0.02538961, -0.00312725, -0.0121216 ,\n","          0.00230487,  0.01527215,  0.02550088, -0.00546687,  0.0170251 ,\n","          0.00199472,  0.02478495, -0.00265258,  0.00585963,  0.00954732,\n","          0.00145537, -0.0103327 ,  0.01096416, -0.00889686,  0.00187653,\n","          0.00717943,  0.01509177, -0.02329896,  0.01842747, -0.0112708 ,\n","         -0.01942161, -0.00373992,  0.01035008,  0.02819517, -0.03356774,\n","         -0.02317039, -0.02689943,  0.026964  ,  0.02579332, -0.01202641,\n","          0.00074404, -0.01799774, -0.03748633, -0.00684021, -0.02259947,\n","          0.02105188, -0.02333407, -0.00834422,  0.00741489, -0.02887459,\n","         -0.00428525,  0.01528427,  0.02617988, -0.00231807, -0.00606469,\n","         -0.03728792,  0.02255907, -0.0061888 ,  0.00158925,  0.00084056,\n","         -0.00842811,  0.0140052 , -0.01533941, -0.01805434, -0.02072551,\n","          0.027758  ,  0.00059137, -0.02949847,  0.02810773, -0.00559356,\n","         -0.00611569, -0.00063584,  0.0079628 , -0.03923811, -0.02139425,\n","         -0.02133578,  0.0066628 ,  0.00262301,  0.00716154,  0.02037518,\n","         -0.01096549, -0.01726679,  0.03031906, -0.0272177 ,  0.00689821,\n","         -0.01045329,  0.01709365, -0.01749223,  0.00399638, -0.0091267 ,\n","          0.01101067, -0.00274485,  0.00646327, -0.02292542, -0.00717578,\n","          0.00455836, -0.00261149,  0.00410897, -0.00868881, -0.03877889,\n","          0.00330593,  0.01194378, -0.00297521, -0.00227981,  0.0083942 ,\n","          0.00544533, -0.03475341,  0.0092515 , -0.00339696,  0.00581517,\n","          0.01883457, -0.00694712, -0.02413802,  0.00293239,  0.00200684,\n","          0.00088294,  0.00448768,  0.00150457, -0.01823462,  0.00130689,\n","         -0.0057553 , -0.01078139, -0.00202113,  0.00067757, -0.01081783,\n","         -0.00623608,  0.01681464, -0.0186994 ,  0.00674382, -0.00229963,\n","          0.01941417, -0.00206451,  0.00828718,  0.00194679,  0.01110958,\n","         -0.01406587,  0.00713421, -0.0015938 ,  0.00742845, -0.00065056,\n","         -0.02653295, -0.01952962,  0.02835621, -0.00412777,  0.00379564,\n","          0.00258839, -0.00193847, -0.01138537,  0.00680073,  0.00450739,\n","         -0.02091591,  0.04868564,  0.00779228, -0.00108732,  0.00584601,\n","          0.01205788, -0.00050835, -0.00762911, -0.00142423, -0.0157417 ,\n","         -0.00317637, -0.00497259,  0.01359244,  0.01776739,  0.0289322 ,\n","         -0.0136748 , -0.00120239,  0.03936272, -0.01701744,  0.00167454,\n","          0.00239677,  0.00478851,  0.01386454, -0.03564278, -0.00605286,\n","         -0.00139096,  0.00930685, -0.00513023, -0.00849863,  0.02876588,\n","         -0.01547365,  0.00346182, -0.01495636, -0.00871618,  0.00381817,\n","         -0.00728785, -0.01925783, -0.01482297,  0.02692884,  0.00085095,\n","         -0.01793097, -0.03248708, -0.0020236 , -0.00658838,  0.0146982 ,\n","          0.02400606,  0.01099509,  0.019386  ,  0.00057842,  0.01391871,\n","         -0.0032294 ]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 10, 1), dtype=float32, numpy=\n"," array([[[0.10223556],\n","         [0.10087116],\n","         [0.10264554],\n","         [0.10025136],\n","         [0.09931984],\n","         [0.09898305],\n","         [0.0988918 ],\n","         [0.09889653],\n","         [0.09893246],\n","         [0.09897271]]], dtype=float32)>)"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"r2h8jDTT8wHq"},"source":["### Loss\n","\n","11. Look at the following loss function, what is the purpose of it, what will it change about the way the model learns?"]},{"cell_type":"code","metadata":{"id":"3ldPbErh8j1x"},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9zMoWHITh1Pk"},"source":["12. Set up a checkpoint for the optimizer, the encoder, and the decoder."]},{"cell_type":"code","metadata":{"id":"RBFavAT78w25"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8V4m-De84om"},"source":["## Training \n","\n","1. Define a `train_step` function that will take as arguments `inp` which represents a batch of input sequences, and `targ` which represents an input of target sequences.\n","\n","This function will:\n","* Initiate `loss` to zero\n","* Track all operations with `tf.GradientTape() as tape`\n","* Use the encoder on `inp` to compute its outputs\n","* Set `dec_state` as the encoder state\n","* Set `dec_input` as the first sequence element of the target batch `targ` (careful with the shapes)\n","* Start a loop that will go through each subsequent elements of the target sequence, and will do:\n","  * Apply the decoder on the encoder outputs and `dec_input`, this will create the prediction's probability vector, and update the decoder state\n","  * Calculate  the loss based on the next element of `targ`, and the prediction probability vector and add it to `loss`\n","  * Set the new decoder input as the next element of `targ`\n","* Create `batch_loss` as equal to the average value of the loss over the target sequence.\n","* Create a `variables` object containing both the encoder's and the decoder's training variables.\n","* Compute the gradient and update the training variables.\n","* Return `batch_loss`\n"]},{"cell_type":"code","metadata":{"id":"bUAgTA4-8zsX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dnp_T-Yukiag"},"source":["2. Code the training loop.\n","It needs to loop across the number of epochs you wish to train for, use the train step, print out the train loss every now and then, and the val loss at the end of each epoch (optional)"]},{"cell_type":"code","metadata":{"id":"XgoAc_Sd85tt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636997363577,"user_tz":-60,"elapsed":502374,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"feddf990-7b8b-444f-c73d-f20bc16b96c2"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 3.7139\n","Epoch 1 Batch 10 Loss 3.0329\n","Epoch 1 Batch 20 Loss 3.0076\n","Epoch 1 Loss 94.7033\n","Time taken for 1 epoch 4.985331773757935 sec\n","\n"," val loss : tf.Tensor(3.7366273, shape=(), dtype=float32) \n","\n","Epoch 2 Batch 0 Loss 2.7408\n","Epoch 2 Batch 10 Loss 2.7151\n","Epoch 2 Batch 20 Loss 2.7714\n","Epoch 2 Loss 82.6822\n","Time taken for 1 epoch 4.810648202896118 sec\n","\n"," val loss : tf.Tensor(3.8331704, shape=(), dtype=float32) \n","\n","Epoch 3 Batch 0 Loss 2.6021\n","Epoch 3 Batch 10 Loss 2.5657\n","Epoch 3 Batch 20 Loss 2.6074\n","Epoch 3 Loss 77.7419\n","Time taken for 1 epoch 4.990307807922363 sec\n","\n"," val loss : tf.Tensor(3.9707232, shape=(), dtype=float32) \n","\n","Epoch 4 Batch 0 Loss 2.4774\n","Epoch 4 Batch 10 Loss 2.4839\n","Epoch 4 Batch 20 Loss 2.3897\n","Epoch 4 Loss 73.2277\n","Time taken for 1 epoch 4.866182804107666 sec\n","\n"," val loss : tf.Tensor(4.07519, shape=(), dtype=float32) \n","\n","Epoch 5 Batch 0 Loss 2.3309\n","Epoch 5 Batch 10 Loss 2.2713\n","Epoch 5 Batch 20 Loss 2.3188\n","Epoch 5 Loss 68.7288\n","Time taken for 1 epoch 5.3698811531066895 sec\n","\n"," val loss : tf.Tensor(3.9696465, shape=(), dtype=float32) \n","\n","Epoch 6 Batch 0 Loss 2.1556\n","Epoch 6 Batch 10 Loss 2.1435\n","Epoch 6 Batch 20 Loss 2.0640\n","Epoch 6 Loss 63.2900\n","Time taken for 1 epoch 4.881079435348511 sec\n","\n"," val loss : tf.Tensor(3.880742, shape=(), dtype=float32) \n","\n","Epoch 7 Batch 0 Loss 2.0360\n","Epoch 7 Batch 10 Loss 1.9244\n","Epoch 7 Batch 20 Loss 1.8688\n","Epoch 7 Loss 57.4210\n","Time taken for 1 epoch 4.8533570766448975 sec\n","\n"," val loss : tf.Tensor(3.824672, shape=(), dtype=float32) \n","\n","Epoch 8 Batch 0 Loss 1.7450\n","Epoch 8 Batch 10 Loss 1.8153\n","Epoch 8 Batch 20 Loss 1.6530\n","Epoch 8 Loss 51.8692\n","Time taken for 1 epoch 4.841083765029907 sec\n","\n"," val loss : tf.Tensor(3.7771661, shape=(), dtype=float32) \n","\n","Epoch 9 Batch 0 Loss 1.5224\n","Epoch 9 Batch 10 Loss 1.5720\n","Epoch 9 Batch 20 Loss 1.5495\n","Epoch 9 Loss 46.4598\n","Time taken for 1 epoch 4.898526430130005 sec\n","\n"," val loss : tf.Tensor(3.582653, shape=(), dtype=float32) \n","\n","Epoch 10 Batch 0 Loss 1.4165\n","Epoch 10 Batch 10 Loss 1.3452\n","Epoch 10 Batch 20 Loss 1.3868\n","Epoch 10 Loss 41.8707\n","Time taken for 1 epoch 4.821177244186401 sec\n","\n"," val loss : tf.Tensor(3.521257, shape=(), dtype=float32) \n","\n","Epoch 11 Batch 0 Loss 1.2442\n","Epoch 11 Batch 10 Loss 1.2433\n","Epoch 11 Batch 20 Loss 1.2120\n","Epoch 11 Loss 37.5734\n","Time taken for 1 epoch 4.91525936126709 sec\n","\n"," val loss : tf.Tensor(3.4422586, shape=(), dtype=float32) \n","\n","Epoch 12 Batch 0 Loss 1.1194\n","Epoch 12 Batch 10 Loss 1.0912\n","Epoch 12 Batch 20 Loss 1.0882\n","Epoch 12 Loss 33.6681\n","Time taken for 1 epoch 4.797847032546997 sec\n","\n"," val loss : tf.Tensor(3.3601081, shape=(), dtype=float32) \n","\n","Epoch 13 Batch 0 Loss 1.0402\n","Epoch 13 Batch 10 Loss 0.9987\n","Epoch 13 Batch 20 Loss 1.0025\n","Epoch 13 Loss 30.1823\n","Time taken for 1 epoch 4.756273031234741 sec\n","\n"," val loss : tf.Tensor(3.3112109, shape=(), dtype=float32) \n","\n","Epoch 14 Batch 0 Loss 0.8720\n","Epoch 14 Batch 10 Loss 0.9744\n","Epoch 14 Batch 20 Loss 0.9345\n","Epoch 14 Loss 27.0289\n","Time taken for 1 epoch 5.423038721084595 sec\n","\n"," val loss : tf.Tensor(3.3093114, shape=(), dtype=float32) \n","\n","Epoch 15 Batch 0 Loss 0.7312\n","Epoch 15 Batch 10 Loss 0.8191\n","Epoch 15 Batch 20 Loss 0.8096\n","Epoch 15 Loss 24.2835\n","Time taken for 1 epoch 4.82619833946228 sec\n","\n"," val loss : tf.Tensor(3.2323532, shape=(), dtype=float32) \n","\n","Epoch 16 Batch 0 Loss 0.6989\n","Epoch 16 Batch 10 Loss 0.7115\n","Epoch 16 Batch 20 Loss 0.7213\n","Epoch 16 Loss 21.4298\n","Time taken for 1 epoch 4.875579595565796 sec\n","\n"," val loss : tf.Tensor(3.2124894, shape=(), dtype=float32) \n","\n","Epoch 17 Batch 0 Loss 0.6178\n","Epoch 17 Batch 10 Loss 0.5965\n","Epoch 17 Batch 20 Loss 0.6420\n","Epoch 17 Loss 19.2283\n","Time taken for 1 epoch 4.835637331008911 sec\n","\n"," val loss : tf.Tensor(3.145035, shape=(), dtype=float32) \n","\n","Epoch 18 Batch 0 Loss 0.5856\n","Epoch 18 Batch 10 Loss 0.5191\n","Epoch 18 Batch 20 Loss 0.6260\n","Epoch 18 Loss 17.0160\n","Time taken for 1 epoch 4.9434404373168945 sec\n","\n"," val loss : tf.Tensor(3.2009013, shape=(), dtype=float32) \n","\n","Epoch 19 Batch 0 Loss 0.5038\n","Epoch 19 Batch 10 Loss 0.4754\n","Epoch 19 Batch 20 Loss 0.4947\n","Epoch 19 Loss 15.1569\n","Time taken for 1 epoch 4.854745864868164 sec\n","\n"," val loss : tf.Tensor(3.1075025, shape=(), dtype=float32) \n","\n","Epoch 20 Batch 0 Loss 0.4558\n","Epoch 20 Batch 10 Loss 0.4089\n","Epoch 20 Batch 20 Loss 0.4839\n","Epoch 20 Loss 13.1708\n","Time taken for 1 epoch 4.85149359703064 sec\n","\n"," val loss : tf.Tensor(3.0915153, shape=(), dtype=float32) \n","\n","Epoch 21 Batch 0 Loss 0.3730\n","Epoch 21 Batch 10 Loss 0.4018\n","Epoch 21 Batch 20 Loss 0.3951\n","Epoch 21 Loss 11.7049\n","Time taken for 1 epoch 4.945007801055908 sec\n","\n"," val loss : tf.Tensor(3.1009712, shape=(), dtype=float32) \n","\n","Epoch 22 Batch 0 Loss 0.3485\n","Epoch 22 Batch 10 Loss 0.3174\n","Epoch 22 Batch 20 Loss 0.4057\n","Epoch 22 Loss 10.3766\n","Time taken for 1 epoch 4.8485634326934814 sec\n","\n"," val loss : tf.Tensor(3.1594477, shape=(), dtype=float32) \n","\n","Epoch 23 Batch 0 Loss 0.3497\n","Epoch 23 Batch 10 Loss 0.3134\n","Epoch 23 Batch 20 Loss 0.3189\n","Epoch 23 Loss 9.2337\n","Time taken for 1 epoch 5.037564039230347 sec\n","\n"," val loss : tf.Tensor(3.0983336, shape=(), dtype=float32) \n","\n","Epoch 24 Batch 0 Loss 0.2515\n","Epoch 24 Batch 10 Loss 0.2628\n","Epoch 24 Batch 20 Loss 0.3228\n","Epoch 24 Loss 8.2312\n","Time taken for 1 epoch 4.890857934951782 sec\n","\n"," val loss : tf.Tensor(3.1023016, shape=(), dtype=float32) \n","\n","Epoch 25 Batch 0 Loss 0.2476\n","Epoch 25 Batch 10 Loss 0.2694\n","Epoch 25 Batch 20 Loss 0.2729\n","Epoch 25 Loss 7.4280\n","Time taken for 1 epoch 4.992100238800049 sec\n","\n"," val loss : tf.Tensor(3.1062722, shape=(), dtype=float32) \n","\n","Epoch 26 Batch 0 Loss 0.1956\n","Epoch 26 Batch 10 Loss 0.1886\n","Epoch 26 Batch 20 Loss 0.2198\n","Epoch 26 Loss 6.6300\n","Time taken for 1 epoch 4.859872102737427 sec\n","\n"," val loss : tf.Tensor(3.0836067, shape=(), dtype=float32) \n","\n","Epoch 27 Batch 0 Loss 0.1788\n","Epoch 27 Batch 10 Loss 0.1912\n","Epoch 27 Batch 20 Loss 0.2045\n","Epoch 27 Loss 6.0051\n","Time taken for 1 epoch 4.872943162918091 sec\n","\n"," val loss : tf.Tensor(3.118429, shape=(), dtype=float32) \n","\n","Epoch 28 Batch 0 Loss 0.1387\n","Epoch 28 Batch 10 Loss 0.1690\n","Epoch 28 Batch 20 Loss 0.1749\n","Epoch 28 Loss 5.3249\n","Time taken for 1 epoch 4.8986170291900635 sec\n","\n"," val loss : tf.Tensor(3.0679116, shape=(), dtype=float32) \n","\n","Epoch 29 Batch 0 Loss 0.1462\n","Epoch 29 Batch 10 Loss 0.1878\n","Epoch 29 Batch 20 Loss 0.1751\n","Epoch 29 Loss 4.8604\n","Time taken for 1 epoch 4.819720506668091 sec\n","\n"," val loss : tf.Tensor(3.0955381, shape=(), dtype=float32) \n","\n","Epoch 30 Batch 0 Loss 0.1386\n","Epoch 30 Batch 10 Loss 0.1299\n","Epoch 30 Batch 20 Loss 0.1492\n","Epoch 30 Loss 4.3425\n","Time taken for 1 epoch 4.958749532699585 sec\n","\n"," val loss : tf.Tensor(3.126362, shape=(), dtype=float32) \n","\n","Epoch 31 Batch 0 Loss 0.1165\n","Epoch 31 Batch 10 Loss 0.1165\n","Epoch 31 Batch 20 Loss 0.1280\n","Epoch 31 Loss 3.9683\n","Time taken for 1 epoch 4.851534366607666 sec\n","\n"," val loss : tf.Tensor(3.0564013, shape=(), dtype=float32) \n","\n","Epoch 32 Batch 0 Loss 0.1058\n","Epoch 32 Batch 10 Loss 0.1114\n","Epoch 32 Batch 20 Loss 0.1276\n","Epoch 32 Loss 3.6480\n","Time taken for 1 epoch 4.968942165374756 sec\n","\n"," val loss : tf.Tensor(3.1242456, shape=(), dtype=float32) \n","\n","Epoch 33 Batch 0 Loss 0.1072\n","Epoch 33 Batch 10 Loss 0.1016\n","Epoch 33 Batch 20 Loss 0.1155\n","Epoch 33 Loss 3.4057\n","Time taken for 1 epoch 4.899837970733643 sec\n","\n"," val loss : tf.Tensor(3.132495, shape=(), dtype=float32) \n","\n","Epoch 34 Batch 0 Loss 0.0895\n","Epoch 34 Batch 10 Loss 0.1296\n","Epoch 34 Batch 20 Loss 0.0900\n","Epoch 34 Loss 3.1501\n","Time taken for 1 epoch 4.943503379821777 sec\n","\n"," val loss : tf.Tensor(3.1258461, shape=(), dtype=float32) \n","\n","Epoch 35 Batch 0 Loss 0.0798\n","Epoch 35 Batch 10 Loss 0.1014\n","Epoch 35 Batch 20 Loss 0.1184\n","Epoch 35 Loss 2.9807\n","Time taken for 1 epoch 4.97598123550415 sec\n","\n"," val loss : tf.Tensor(3.1458206, shape=(), dtype=float32) \n","\n","Epoch 36 Batch 0 Loss 0.0887\n","Epoch 36 Batch 10 Loss 0.0911\n","Epoch 36 Batch 20 Loss 0.0954\n","Epoch 36 Loss 2.8860\n","Time taken for 1 epoch 4.841508150100708 sec\n","\n"," val loss : tf.Tensor(3.1650007, shape=(), dtype=float32) \n","\n","Epoch 37 Batch 0 Loss 0.0987\n","Epoch 37 Batch 10 Loss 0.0699\n","Epoch 37 Batch 20 Loss 0.0907\n","Epoch 37 Loss 2.6711\n","Time taken for 1 epoch 4.8371217250823975 sec\n","\n"," val loss : tf.Tensor(3.1532333, shape=(), dtype=float32) \n","\n","Epoch 38 Batch 0 Loss 0.0856\n","Epoch 38 Batch 10 Loss 0.0778\n","Epoch 38 Batch 20 Loss 0.0819\n","Epoch 38 Loss 2.5638\n","Time taken for 1 epoch 4.962472200393677 sec\n","\n"," val loss : tf.Tensor(3.108272, shape=(), dtype=float32) \n","\n","Epoch 39 Batch 0 Loss 0.0604\n","Epoch 39 Batch 10 Loss 0.0802\n","Epoch 39 Batch 20 Loss 0.0931\n","Epoch 39 Loss 2.4070\n","Time taken for 1 epoch 4.826804161071777 sec\n","\n"," val loss : tf.Tensor(3.1233456, shape=(), dtype=float32) \n","\n","Epoch 40 Batch 0 Loss 0.0723\n","Epoch 40 Batch 10 Loss 0.0744\n","Epoch 40 Batch 20 Loss 0.0676\n","Epoch 40 Loss 2.2908\n","Time taken for 1 epoch 4.83244514465332 sec\n","\n"," val loss : tf.Tensor(3.1643336, shape=(), dtype=float32) \n","\n","Epoch 41 Batch 0 Loss 0.0583\n","Epoch 41 Batch 10 Loss 0.0631\n","Epoch 41 Batch 20 Loss 0.0725\n","Epoch 41 Loss 2.1592\n","Time taken for 1 epoch 4.993801593780518 sec\n","\n"," val loss : tf.Tensor(3.2153444, shape=(), dtype=float32) \n","\n","Epoch 42 Batch 0 Loss 0.0685\n","Epoch 42 Batch 10 Loss 0.0667\n","Epoch 42 Batch 20 Loss 0.0709\n","Epoch 42 Loss 2.0834\n","Time taken for 1 epoch 4.8531858921051025 sec\n","\n"," val loss : tf.Tensor(3.1303089, shape=(), dtype=float32) \n","\n","Epoch 43 Batch 0 Loss 0.0620\n","Epoch 43 Batch 10 Loss 0.0457\n","Epoch 43 Batch 20 Loss 0.0820\n","Epoch 43 Loss 2.0329\n","Time taken for 1 epoch 5.189390659332275 sec\n","\n"," val loss : tf.Tensor(3.154311, shape=(), dtype=float32) \n","\n","Epoch 44 Batch 0 Loss 0.0559\n","Epoch 44 Batch 10 Loss 0.0793\n","Epoch 44 Batch 20 Loss 0.0625\n","Epoch 44 Loss 1.9647\n","Time taken for 1 epoch 4.819674253463745 sec\n","\n"," val loss : tf.Tensor(3.1686726, shape=(), dtype=float32) \n","\n","Epoch 45 Batch 0 Loss 0.0493\n","Epoch 45 Batch 10 Loss 0.0466\n","Epoch 45 Batch 20 Loss 0.0670\n","Epoch 45 Loss 1.8756\n","Time taken for 1 epoch 4.928944110870361 sec\n","\n"," val loss : tf.Tensor(3.1380405, shape=(), dtype=float32) \n","\n","Epoch 46 Batch 0 Loss 0.0468\n","Epoch 46 Batch 10 Loss 0.0463\n","Epoch 46 Batch 20 Loss 0.0647\n","Epoch 46 Loss 1.7909\n","Time taken for 1 epoch 4.787635564804077 sec\n","\n"," val loss : tf.Tensor(3.172824, shape=(), dtype=float32) \n","\n","Epoch 47 Batch 0 Loss 0.0555\n","Epoch 47 Batch 10 Loss 0.0653\n","Epoch 47 Batch 20 Loss 0.0671\n","Epoch 47 Loss 1.7633\n","Time taken for 1 epoch 4.882593870162964 sec\n","\n"," val loss : tf.Tensor(3.2538228, shape=(), dtype=float32) \n","\n","Epoch 48 Batch 0 Loss 0.0582\n","Epoch 48 Batch 10 Loss 0.0527\n","Epoch 48 Batch 20 Loss 0.0499\n","Epoch 48 Loss 1.6960\n","Time taken for 1 epoch 4.7943434715271 sec\n","\n"," val loss : tf.Tensor(3.12507, shape=(), dtype=float32) \n","\n","Epoch 49 Batch 0 Loss 0.0468\n","Epoch 49 Batch 10 Loss 0.0518\n","Epoch 49 Batch 20 Loss 0.0523\n","Epoch 49 Loss 1.6495\n","Time taken for 1 epoch 4.941641569137573 sec\n","\n"," val loss : tf.Tensor(3.208263, shape=(), dtype=float32) \n","\n","Epoch 50 Batch 0 Loss 0.0436\n","Epoch 50 Batch 10 Loss 0.0350\n","Epoch 50 Batch 20 Loss 0.0659\n","Epoch 50 Loss 1.6464\n","Time taken for 1 epoch 4.78167200088501 sec\n","\n"," val loss : tf.Tensor(3.2084506, shape=(), dtype=float32) \n","\n","Epoch 51 Batch 0 Loss 0.0473\n","Epoch 51 Batch 10 Loss 0.0476\n","Epoch 51 Batch 20 Loss 0.0593\n","Epoch 51 Loss 1.5752\n","Time taken for 1 epoch 4.963531017303467 sec\n","\n"," val loss : tf.Tensor(3.1465628, shape=(), dtype=float32) \n","\n","Epoch 52 Batch 0 Loss 0.0565\n","Epoch 52 Batch 10 Loss 0.0490\n","Epoch 52 Batch 20 Loss 0.0522\n","Epoch 52 Loss 1.5417\n","Time taken for 1 epoch 4.826049089431763 sec\n","\n"," val loss : tf.Tensor(3.1324492, shape=(), dtype=float32) \n","\n","Epoch 53 Batch 0 Loss 0.0422\n","Epoch 53 Batch 10 Loss 0.0371\n","Epoch 53 Batch 20 Loss 0.0510\n","Epoch 53 Loss 1.4888\n","Time taken for 1 epoch 4.888978481292725 sec\n","\n"," val loss : tf.Tensor(3.1614985, shape=(), dtype=float32) \n","\n","Epoch 54 Batch 0 Loss 0.0391\n","Epoch 54 Batch 10 Loss 0.0421\n","Epoch 54 Batch 20 Loss 0.0350\n","Epoch 54 Loss 1.4999\n","Time taken for 1 epoch 4.863503456115723 sec\n","\n"," val loss : tf.Tensor(3.1673536, shape=(), dtype=float32) \n","\n","Epoch 55 Batch 0 Loss 0.0279\n","Epoch 55 Batch 10 Loss 0.0438\n","Epoch 55 Batch 20 Loss 0.0465\n","Epoch 55 Loss 1.4669\n","Time taken for 1 epoch 4.909207582473755 sec\n","\n"," val loss : tf.Tensor(3.2227736, shape=(), dtype=float32) \n","\n","Epoch 56 Batch 0 Loss 0.0391\n","Epoch 56 Batch 10 Loss 0.0469\n","Epoch 56 Batch 20 Loss 0.0403\n","Epoch 56 Loss 1.4108\n","Time taken for 1 epoch 5.273891925811768 sec\n","\n"," val loss : tf.Tensor(3.1850283, shape=(), dtype=float32) \n","\n","Epoch 57 Batch 0 Loss 0.0353\n","Epoch 57 Batch 10 Loss 0.0482\n","Epoch 57 Batch 20 Loss 0.0348\n","Epoch 57 Loss 1.4212\n","Time taken for 1 epoch 4.938407897949219 sec\n","\n"," val loss : tf.Tensor(3.2096808, shape=(), dtype=float32) \n","\n","Epoch 58 Batch 0 Loss 0.0435\n","Epoch 58 Batch 10 Loss 0.0379\n","Epoch 58 Batch 20 Loss 0.0303\n","Epoch 58 Loss 1.3951\n","Time taken for 1 epoch 4.817895174026489 sec\n","\n"," val loss : tf.Tensor(3.1786103, shape=(), dtype=float32) \n","\n","Epoch 59 Batch 0 Loss 0.0419\n","Epoch 59 Batch 10 Loss 0.0445\n","Epoch 59 Batch 20 Loss 0.0393\n","Epoch 59 Loss 1.3936\n","Time taken for 1 epoch 4.851594686508179 sec\n","\n"," val loss : tf.Tensor(3.1855001, shape=(), dtype=float32) \n","\n","Epoch 60 Batch 0 Loss 0.0456\n","Epoch 60 Batch 10 Loss 0.0525\n","Epoch 60 Batch 20 Loss 0.0784\n","Epoch 60 Loss 1.3619\n","Time taken for 1 epoch 5.027291297912598 sec\n","\n"," val loss : tf.Tensor(3.2163072, shape=(), dtype=float32) \n","\n","Epoch 61 Batch 0 Loss 0.0304\n","Epoch 61 Batch 10 Loss 0.0533\n","Epoch 61 Batch 20 Loss 0.0582\n","Epoch 61 Loss 1.3272\n","Time taken for 1 epoch 4.745848894119263 sec\n","\n"," val loss : tf.Tensor(3.178921, shape=(), dtype=float32) \n","\n","Epoch 62 Batch 0 Loss 0.0341\n","Epoch 62 Batch 10 Loss 0.0363\n","Epoch 62 Batch 20 Loss 0.0499\n","Epoch 62 Loss 1.3377\n","Time taken for 1 epoch 4.9086387157440186 sec\n","\n"," val loss : tf.Tensor(3.2516809, shape=(), dtype=float32) \n","\n","Epoch 63 Batch 0 Loss 0.0326\n","Epoch 63 Batch 10 Loss 0.0422\n","Epoch 63 Batch 20 Loss 0.0439\n","Epoch 63 Loss 1.3150\n","Time taken for 1 epoch 4.7214882373809814 sec\n","\n"," val loss : tf.Tensor(3.2134967, shape=(), dtype=float32) \n","\n","Epoch 64 Batch 0 Loss 0.0339\n","Epoch 64 Batch 10 Loss 0.0544\n","Epoch 64 Batch 20 Loss 0.0382\n","Epoch 64 Loss 1.2698\n","Time taken for 1 epoch 4.91681981086731 sec\n","\n"," val loss : tf.Tensor(3.2109485, shape=(), dtype=float32) \n","\n","Epoch 65 Batch 0 Loss 0.0383\n","Epoch 65 Batch 10 Loss 0.0403\n","Epoch 65 Batch 20 Loss 0.0453\n","Epoch 65 Loss 1.2841\n","Time taken for 1 epoch 4.785826683044434 sec\n","\n"," val loss : tf.Tensor(3.2831993, shape=(), dtype=float32) \n","\n","Epoch 66 Batch 0 Loss 0.0329\n","Epoch 66 Batch 10 Loss 0.0557\n","Epoch 66 Batch 20 Loss 0.0591\n","Epoch 66 Loss 1.4316\n","Time taken for 1 epoch 4.934556245803833 sec\n","\n"," val loss : tf.Tensor(3.233037, shape=(), dtype=float32) \n","\n","Epoch 67 Batch 0 Loss 0.0333\n","Epoch 67 Batch 10 Loss 0.0492\n","Epoch 67 Batch 20 Loss 0.0602\n","Epoch 67 Loss 1.5672\n","Time taken for 1 epoch 4.834491014480591 sec\n","\n"," val loss : tf.Tensor(3.3689902, shape=(), dtype=float32) \n","\n","Epoch 68 Batch 0 Loss 0.0351\n","Epoch 68 Batch 10 Loss 0.0431\n","Epoch 68 Batch 20 Loss 0.0524\n","Epoch 68 Loss 1.5654\n","Time taken for 1 epoch 4.9719016551971436 sec\n","\n"," val loss : tf.Tensor(3.2381153, shape=(), dtype=float32) \n","\n","Epoch 69 Batch 0 Loss 0.0427\n","Epoch 69 Batch 10 Loss 0.0521\n","Epoch 69 Batch 20 Loss 0.0446\n","Epoch 69 Loss 1.4438\n","Time taken for 1 epoch 4.7959840297698975 sec\n","\n"," val loss : tf.Tensor(3.3135915, shape=(), dtype=float32) \n","\n","Epoch 70 Batch 0 Loss 0.0521\n","Epoch 70 Batch 10 Loss 0.0332\n","Epoch 70 Batch 20 Loss 0.0464\n","Epoch 70 Loss 1.4295\n","Time taken for 1 epoch 4.869116306304932 sec\n","\n"," val loss : tf.Tensor(3.2265635, shape=(), dtype=float32) \n","\n","Epoch 71 Batch 0 Loss 0.0391\n","Epoch 71 Batch 10 Loss 0.0521\n","Epoch 71 Batch 20 Loss 0.0350\n","Epoch 71 Loss 1.2866\n","Time taken for 1 epoch 4.878424882888794 sec\n","\n"," val loss : tf.Tensor(3.2752314, shape=(), dtype=float32) \n","\n","Epoch 72 Batch 0 Loss 0.0432\n","Epoch 72 Batch 10 Loss 0.0342\n","Epoch 72 Batch 20 Loss 0.0471\n","Epoch 72 Loss 1.2342\n","Time taken for 1 epoch 4.879260540008545 sec\n","\n"," val loss : tf.Tensor(3.305807, shape=(), dtype=float32) \n","\n","Epoch 73 Batch 0 Loss 0.0350\n","Epoch 73 Batch 10 Loss 0.0365\n","Epoch 73 Batch 20 Loss 0.0529\n","Epoch 73 Loss 1.1769\n","Time taken for 1 epoch 4.9158430099487305 sec\n","\n"," val loss : tf.Tensor(3.2502992, shape=(), dtype=float32) \n","\n","Epoch 74 Batch 0 Loss 0.0280\n","Epoch 74 Batch 10 Loss 0.0462\n","Epoch 74 Batch 20 Loss 0.0258\n","Epoch 74 Loss 1.1677\n","Time taken for 1 epoch 4.919823884963989 sec\n","\n"," val loss : tf.Tensor(3.3028805, shape=(), dtype=float32) \n","\n","Epoch 75 Batch 0 Loss 0.0232\n","Epoch 75 Batch 10 Loss 0.0303\n","Epoch 75 Batch 20 Loss 0.0288\n","Epoch 75 Loss 1.1253\n","Time taken for 1 epoch 4.760087490081787 sec\n","\n"," val loss : tf.Tensor(3.257171, shape=(), dtype=float32) \n","\n","Epoch 76 Batch 0 Loss 0.0223\n","Epoch 76 Batch 10 Loss 0.0356\n","Epoch 76 Batch 20 Loss 0.0384\n","Epoch 76 Loss 1.0869\n","Time taken for 1 epoch 4.962471961975098 sec\n","\n"," val loss : tf.Tensor(3.269803, shape=(), dtype=float32) \n","\n","Epoch 77 Batch 0 Loss 0.0244\n","Epoch 77 Batch 10 Loss 0.0316\n","Epoch 77 Batch 20 Loss 0.0495\n","Epoch 77 Loss 1.0959\n","Time taken for 1 epoch 4.847684860229492 sec\n","\n"," val loss : tf.Tensor(3.2615092, shape=(), dtype=float32) \n","\n","Epoch 78 Batch 0 Loss 0.0353\n","Epoch 78 Batch 10 Loss 0.0215\n","Epoch 78 Batch 20 Loss 0.0399\n","Epoch 78 Loss 1.0915\n","Time taken for 1 epoch 4.933850526809692 sec\n","\n"," val loss : tf.Tensor(3.2918222, shape=(), dtype=float32) \n","\n","Epoch 79 Batch 0 Loss 0.0272\n","Epoch 79 Batch 10 Loss 0.0428\n","Epoch 79 Batch 20 Loss 0.0334\n","Epoch 79 Loss 1.0939\n","Time taken for 1 epoch 4.776376962661743 sec\n","\n"," val loss : tf.Tensor(3.261068, shape=(), dtype=float32) \n","\n","Epoch 80 Batch 0 Loss 0.0247\n","Epoch 80 Batch 10 Loss 0.0313\n","Epoch 80 Batch 20 Loss 0.0389\n","Epoch 80 Loss 1.0694\n","Time taken for 1 epoch 5.001269340515137 sec\n","\n"," val loss : tf.Tensor(3.2954843, shape=(), dtype=float32) \n","\n","Epoch 81 Batch 0 Loss 0.0209\n","Epoch 81 Batch 10 Loss 0.0322\n","Epoch 81 Batch 20 Loss 0.0405\n","Epoch 81 Loss 1.0796\n","Time taken for 1 epoch 4.902752876281738 sec\n","\n"," val loss : tf.Tensor(3.3434384, shape=(), dtype=float32) \n","\n","Epoch 82 Batch 0 Loss 0.0284\n","Epoch 82 Batch 10 Loss 0.0354\n","Epoch 82 Batch 20 Loss 0.0307\n","Epoch 82 Loss 1.0787\n","Time taken for 1 epoch 5.069967985153198 sec\n","\n"," val loss : tf.Tensor(3.2993722, shape=(), dtype=float32) \n","\n","Epoch 83 Batch 0 Loss 0.0286\n","Epoch 83 Batch 10 Loss 0.0502\n","Epoch 83 Batch 20 Loss 0.0342\n","Epoch 83 Loss 1.0236\n","Time taken for 1 epoch 4.800041437149048 sec\n","\n"," val loss : tf.Tensor(3.256566, shape=(), dtype=float32) \n","\n","Epoch 84 Batch 0 Loss 0.0244\n","Epoch 84 Batch 10 Loss 0.0333\n","Epoch 84 Batch 20 Loss 0.0328\n","Epoch 84 Loss 1.0401\n","Time taken for 1 epoch 4.798405885696411 sec\n","\n"," val loss : tf.Tensor(3.3031712, shape=(), dtype=float32) \n","\n","Epoch 85 Batch 0 Loss 0.0261\n","Epoch 85 Batch 10 Loss 0.0249\n","Epoch 85 Batch 20 Loss 0.0291\n","Epoch 85 Loss 1.0255\n","Time taken for 1 epoch 4.939400911331177 sec\n","\n"," val loss : tf.Tensor(3.2679965, shape=(), dtype=float32) \n","\n","Epoch 86 Batch 0 Loss 0.0367\n","Epoch 86 Batch 10 Loss 0.0308\n","Epoch 86 Batch 20 Loss 0.0346\n","Epoch 86 Loss 0.9949\n","Time taken for 1 epoch 4.994939088821411 sec\n","\n"," val loss : tf.Tensor(3.3017976, shape=(), dtype=float32) \n","\n","Epoch 87 Batch 0 Loss 0.0260\n","Epoch 87 Batch 10 Loss 0.0310\n","Epoch 87 Batch 20 Loss 0.0455\n","Epoch 87 Loss 1.0123\n","Time taken for 1 epoch 4.7895667552948 sec\n","\n"," val loss : tf.Tensor(3.287047, shape=(), dtype=float32) \n","\n","Epoch 88 Batch 0 Loss 0.0175\n","Epoch 88 Batch 10 Loss 0.0355\n","Epoch 88 Batch 20 Loss 0.0490\n","Epoch 88 Loss 0.9891\n","Time taken for 1 epoch 5.02311110496521 sec\n","\n"," val loss : tf.Tensor(3.30244, shape=(), dtype=float32) \n","\n","Epoch 89 Batch 0 Loss 0.0289\n","Epoch 89 Batch 10 Loss 0.0265\n","Epoch 89 Batch 20 Loss 0.0435\n","Epoch 89 Loss 0.9749\n","Time taken for 1 epoch 4.8152313232421875 sec\n","\n"," val loss : tf.Tensor(3.306836, shape=(), dtype=float32) \n","\n","Epoch 90 Batch 0 Loss 0.0326\n","Epoch 90 Batch 10 Loss 0.0368\n","Epoch 90 Batch 20 Loss 0.0344\n","Epoch 90 Loss 0.9863\n","Time taken for 1 epoch 4.849490404129028 sec\n","\n"," val loss : tf.Tensor(3.303451, shape=(), dtype=float32) \n","\n","Epoch 91 Batch 0 Loss 0.0309\n","Epoch 91 Batch 10 Loss 0.0223\n","Epoch 91 Batch 20 Loss 0.0267\n","Epoch 91 Loss 0.9903\n","Time taken for 1 epoch 4.785642147064209 sec\n","\n"," val loss : tf.Tensor(3.3631992, shape=(), dtype=float32) \n","\n","Epoch 92 Batch 0 Loss 0.0269\n","Epoch 92 Batch 10 Loss 0.0208\n","Epoch 92 Batch 20 Loss 0.0373\n","Epoch 92 Loss 0.9696\n","Time taken for 1 epoch 4.91841721534729 sec\n","\n"," val loss : tf.Tensor(3.3391898, shape=(), dtype=float32) \n","\n","Epoch 93 Batch 0 Loss 0.0271\n","Epoch 93 Batch 10 Loss 0.0355\n","Epoch 93 Batch 20 Loss 0.0306\n","Epoch 93 Loss 0.9819\n","Time taken for 1 epoch 4.852178573608398 sec\n","\n"," val loss : tf.Tensor(3.3434017, shape=(), dtype=float32) \n","\n","Epoch 94 Batch 0 Loss 0.0300\n","Epoch 94 Batch 10 Loss 0.0339\n","Epoch 94 Batch 20 Loss 0.0289\n","Epoch 94 Loss 0.9400\n","Time taken for 1 epoch 4.961540937423706 sec\n","\n"," val loss : tf.Tensor(3.346846, shape=(), dtype=float32) \n","\n","Epoch 95 Batch 0 Loss 0.0359\n","Epoch 95 Batch 10 Loss 0.0237\n","Epoch 95 Batch 20 Loss 0.0296\n","Epoch 95 Loss 0.9535\n","Time taken for 1 epoch 4.859182834625244 sec\n","\n"," val loss : tf.Tensor(3.3437402, shape=(), dtype=float32) \n","\n","Epoch 96 Batch 0 Loss 0.0326\n","Epoch 96 Batch 10 Loss 0.0260\n","Epoch 96 Batch 20 Loss 0.0224\n","Epoch 96 Loss 0.9565\n","Time taken for 1 epoch 4.844298362731934 sec\n","\n"," val loss : tf.Tensor(3.3376324, shape=(), dtype=float32) \n","\n","Epoch 97 Batch 0 Loss 0.0249\n","Epoch 97 Batch 10 Loss 0.0227\n","Epoch 97 Batch 20 Loss 0.0290\n","Epoch 97 Loss 0.9554\n","Time taken for 1 epoch 4.869659423828125 sec\n","\n"," val loss : tf.Tensor(3.3729308, shape=(), dtype=float32) \n","\n","Epoch 98 Batch 0 Loss 0.0204\n","Epoch 98 Batch 10 Loss 0.0334\n","Epoch 98 Batch 20 Loss 0.0201\n","Epoch 98 Loss 0.9082\n","Time taken for 1 epoch 5.289070129394531 sec\n","\n"," val loss : tf.Tensor(3.3594966, shape=(), dtype=float32) \n","\n","Epoch 99 Batch 0 Loss 0.0207\n","Epoch 99 Batch 10 Loss 0.0212\n","Epoch 99 Batch 20 Loss 0.0322\n","Epoch 99 Loss 0.9306\n","Time taken for 1 epoch 4.829520225524902 sec\n","\n"," val loss : tf.Tensor(3.373816, shape=(), dtype=float32) \n","\n","Epoch 100 Batch 0 Loss 0.0284\n","Epoch 100 Batch 10 Loss 0.0260\n","Epoch 100 Batch 20 Loss 0.0444\n","Epoch 100 Loss 0.9097\n","Time taken for 1 epoch 4.973791599273682 sec\n","\n"," val loss : tf.Tensor(3.3820672, shape=(), dtype=float32) \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"rnnEFJ4elsda"},"source":["3. What do you think of the training process, did it work well on the train set?  On the validation set?"]},{"cell_type":"code","metadata":{"id":"WLK_47Bg_iU2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L1zjfpzkl2dl"},"source":["4. Use `X_val` to compute all the predictions for the validation set and convert them  back to text. Compare them with the actual target values, what do you think? What about the results on the training set?"]},{"cell_type":"code","metadata":{"id":"q_2bS3Cm_aDM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636997413027,"user_tz":-60,"elapsed":211,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhACbICY03s2M31yuvkFW6Ft-VOK5GyGu_Tz68=s64","userId":"11930294859591867631"}},"outputId":"8dc3c439-4a98-4724-9701-ade48daa3d61"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["pred: off it wasn't me\n","true: go away\n","\n","\n","pred: find a job a\n","true: get a job\n","\n","\n","pred: i won win win\n","true: did i win\n","\n","\n","pred: now drink up tom\n","true: now drink up\n","\n","\n","pred: stop that out of\n","true: come off it\n","\n","\n","pred: a nap a nap\n","true: i have proof\n","\n","\n","pred: where is he is\n","true: where is it\n","\n","\n","pred: i was busy got\n","true: i was busy\n","\n","\n","pred: i've tried it out\n","true: i tried\n","\n","\n","pred: i'm starved all set\n","true: i'm through\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"WN2jMgLxmKPu"},"source":["5. Now that everything works well, it's time to increase our number of samples and start another training, did the results improve?"]}]}