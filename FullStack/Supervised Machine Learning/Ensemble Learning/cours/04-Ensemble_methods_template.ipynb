{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a1373e-d82b-4ba3-876e-abd05669ef18",
   "metadata": {},
   "source": [
    "# Ensemble methods with scikit-learn and XGBoost üßë‚Äçü§ù‚Äçüßëüßë‚Äçü§ù‚Äçüßë\n",
    "\n",
    "Let's train ensemble models on our toy-dataset.\n",
    "\n",
    "## What will you learn in this course? üßêüßê\n",
    "This lecture is a follow-along demo that will guide you through the steps of building ensemble models, choosing the base estimators, optimizing the hyperparameters and evaluating the performances on actual data.\n",
    "\n",
    "* Preprocessing\n",
    "    * Training pipeline\n",
    "    * Test pipeline\n",
    "* Bagging\n",
    "    * Bagging with logistic regression as base estimator\n",
    "    * Bagging with decision tree as base estimator\n",
    "* Boosting\n",
    "    * AdaBoost\n",
    "    * scikit-learn's GradientBoosting\n",
    "    * XGBoost\n",
    "* Voting\n",
    "* Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77cfddf4-0a6a-44b6-b592-48b6ed1eeea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# import ensemble methods\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# import base estimators\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) # to avoid deprecation warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# setting Jedha color palette as default\n",
    "pio.templates[\"jedha\"] = go.layout.Template(\n",
    "    layout_colorway=[\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    ")\n",
    "pio.templates.default = \"jedha\"\n",
    "pio.renderers.default = \"svg\" # to be replaced by \"iframe\" if working on JULIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1bc617-7b42-48dc-bb83-006142c51330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = pd.read_csv(\"src/Data.csv\")\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6572043b-8c19-4959-9175-d500460fea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 10\n",
      "\n",
      "Display of dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>69000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age  Salary Purchased\n",
       "0   France  44.0   72000        No\n",
       "1    Spain  27.0   48000       Yes\n",
       "2  Germany  30.0   54000        No\n",
       "3    Spain  38.0   61000        No\n",
       "4  Germany  40.0   69000       Yes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basics statistics: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>France</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>38.777778</td>\n",
       "      <td>64300.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.693793</td>\n",
       "      <td>11681.419244</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>48000.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>55000.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>64000.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>71250.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>83000.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country        Age        Salary Purchased\n",
       "count       10   9.000000     10.000000        10\n",
       "unique       3        NaN           NaN         2\n",
       "top     France        NaN           NaN        No\n",
       "freq         4        NaN           NaN         5\n",
       "mean       NaN  38.777778  64300.000000       NaN\n",
       "std        NaN   7.693793  11681.419244       NaN\n",
       "min        NaN  27.000000  48000.000000       NaN\n",
       "25%        NaN  35.000000  55000.000000       NaN\n",
       "50%        NaN  38.000000  64000.000000       NaN\n",
       "75%        NaN  44.000000  71250.000000       NaN\n",
       "max        NaN  50.000000  83000.000000       NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of missing values: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Country       0.0\n",
       "Age          10.0\n",
       "Salary        0.0\n",
       "Purchased     0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic stats\n",
    "print(\"Number of rows : {}\".format(dataset.shape[0]))\n",
    "print()\n",
    "\n",
    "print(\"Display of dataset: \")\n",
    "display(dataset.head())\n",
    "print()\n",
    "\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = dataset.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n",
    "\n",
    "print(\"Percentage of missing values: \")\n",
    "display(100*dataset.isnull().sum()/dataset.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca47316e-3f7c-4222-8e4f-e3a061d5ee54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40cc490c-01cf-4042-96f9-92662f0a6e0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "Zn2MYjTjrfBJ",
    "outputId": "720c974a-fdfd-4544-c188-229008b33351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating labels from features...\n",
      "...Done.\n",
      "\n",
      "Y : \n",
      "0     No\n",
      "1    Yes\n",
      "2     No\n",
      "3     No\n",
      "4    Yes\n",
      "Name: Purchased, dtype: object\n",
      "\n",
      "X :\n",
      "   Country   Age  Salary\n",
      "0   France  44.0   72000\n",
      "1    Spain  27.0   48000\n",
      "2  Germany  30.0   54000\n",
      "3    Spain  38.0   61000\n",
      "4  Germany  40.0   69000\n"
     ]
    }
   ],
   "source": [
    "# Separate target variable Y from features X\n",
    "print(\"Separating labels from features...\")\n",
    "target_variable = \"Purchased\"\n",
    "\n",
    "X = dataset.drop(target_variable, axis = 1)\n",
    "Y = dataset.loc[:,target_variable]\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print('Y : ')\n",
    "print(Y.head())\n",
    "print()\n",
    "print('X :')\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597f1c42-0346-40b0-8352-6793a39190d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found numeric features  ['Age', 'Salary']\n",
      "Found categorical features  ['Country']\n"
     ]
    }
   ],
   "source": [
    "# Automatically detect names of numeric/categorical columns\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "for i,t in X.dtypes.iteritems():\n",
    "    if ('float' in str(t)) or ('int' in str(t)) :\n",
    "        numeric_features.append(i)\n",
    "    else :\n",
    "        categorical_features.append(i)\n",
    "\n",
    "print('Found numeric features ', numeric_features)\n",
    "print('Found categorical features ', categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8445b5d9-a5eb-4dbd-a877-b5b27070b746",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Ra8LgBS5s83i",
    "outputId": "6dd938b3-ca4b-4ddb-c55e-f020aed4eca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing into train and test sets...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "print(\"Dividing into train and test sets...\")\n",
    "# WARNING : don't forget stratify=Y for classification problems\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify = Y)\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0721517e-f60e-4153-9823-110d335f731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # missing values will be replaced by columns' mean\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecf7b6dc-386b-4b7b-a2c4-c1c4d44cb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for categorical features\n",
    "categorical_transformer = OneHotEncoder(drop='first') # no missing values in categorical data, so we only need the OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a53252d3-2af7-49cb-8f07-f8277d46413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ColumnTransformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e212e063-6d16-4566-b12d-5fa76698ccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing preprocessings on train set...\n",
      "   Country   Age  Salary\n",
      "0   France  44.0   72000\n",
      "4  Germany  40.0   69000\n",
      "6    Spain   NaN   52000\n",
      "9   France  37.0   67000\n",
      "3    Spain  38.0   61000\n",
      "...Done.\n",
      "[[ 1.61706195e+00  1.46885753e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 8.22715727e-01  1.09777773e+00  1.00000000e+00  0.00000000e+00]\n",
      " [-1.41104234e-15 -1.00500778e+00  0.00000000e+00  1.00000000e+00]\n",
      " [ 2.26956063e-01  8.50391200e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.25542617e-01  1.08231607e-01  0.00000000e+00  1.00000000e+00]]\n",
      "\n",
      "Encoding labels...\n",
      "0     No\n",
      "4    Yes\n",
      "6     No\n",
      "9    Yes\n",
      "3     No\n",
      "Name: Purchased, dtype: object\n",
      "...Done\n",
      "[0 1 0 1 0]\n",
      "Performing preprocessings on test set...\n",
      "   Country   Age  Salary\n",
      "8  Germany  50.0   83000\n",
      "7   France  48.0   79000\n",
      "...Done.\n",
      "[[2.80858127 2.82948345 1.         0.        ]\n",
      " [2.41140816 2.33471038 0.         0.        ]]\n",
      "\n",
      "Encoding labels...\n",
      "8     No\n",
      "7    Yes\n",
      "Name: Purchased, dtype: object\n",
      "...Done\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessings on train set\n",
    "print(\"Performing preprocessings on train set...\")\n",
    "print(X_train.head())\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "print('...Done.')\n",
    "print(X_train[0:5]) # MUST use this syntax because X_train is a numpy array and not a pandas DataFrame anymore\n",
    "print()\n",
    "# Label encoding\n",
    "print(\"Encoding labels...\")\n",
    "print(Y_train.head())\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "print(\"...Done\")\n",
    "print(Y_train[0:5])\n",
    "\n",
    "# Preprocessings on test set\n",
    "print(\"Performing preprocessings on test set...\")\n",
    "print(X_test.head()) \n",
    "X_test = preprocessor.transform(X_test) # Don't fit again !! The test set is used for validating decisions\n",
    "# we made based on the training set, therefore we can only apply transformations that were parametered using the training set.\n",
    "# Otherwise this creates what is called a leak from the test set which will introduce a bias in all your results.\n",
    "print('...Done.')\n",
    "print(X_test[0:5,:]) # MUST use this syntax because X_test is a numpy array and not a pandas DataFrame anymore\n",
    "print()\n",
    "# Label encoding\n",
    "print(\"Encoding labels...\")\n",
    "print(Y_test[0:5])\n",
    "Y_test = encoder.transform(Y_test)\n",
    "print(\"...Done\")\n",
    "print(Y_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce24b5a-a5bd-4f12-a7a5-4c239fef5f0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bagging\n",
    "### Bagging with logistic regression as base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53cbec0b-df0f-4144-8b49-fc7544787f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "{'base_estimator__C': [0.01, 0.05, 0.1, 0.5], 'n_estimators': [5, 10, 20, 30]}\n",
      "...Done.\n",
      "Best hyperparameters :  {'base_estimator__C': 0.05, 'n_estimators': 5}\n",
      "Best validation accuracy :  0.5\n",
      "\n",
      "Accuracy on training set :  0.5\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "logistic_regression = LogisticRegression(max_iter = 1000) # max_iter changed because of convergence warning\n",
    "model = BaggingClassifier(logistic_regression)\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'base_estimator__C': [0.01, 0.05, 0.1, 0.5,], # base_estimator__ prefix because C is a parameter from LogisticRegression! \n",
    "    'n_estimators': [5, 10, 20, 30] # n_estimators is a hyperparameter of the ensemble method\n",
    "}\n",
    "print(params)\n",
    "gridsearch = GridSearchCV(model, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)\n",
    "print()\n",
    "print(\"Accuracy on training set : \", gridsearch.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", gridsearch.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56f1eb-f17b-4489-bd4e-01d923d04113",
   "metadata": {},
   "source": [
    "### Bagging with decision tree as base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28cb660a-50a1-45d7-a0f4-699d3d8d71db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "{'base_estimator__max_depth': [1, 2, 3], 'base_estimator__min_samples_leaf': [1, 2, 3], 'base_estimator__min_samples_split': [2, 3, 4], 'n_estimators': [2, 4, 6, 8, 10]}\n",
      "...Done.\n",
      "Best hyperparameters :  {'base_estimator__max_depth': 3, 'base_estimator__min_samples_leaf': 2, 'base_estimator__min_samples_split': 3, 'n_estimators': 2}\n",
      "Best validation accuracy :  0.611111111111111\n",
      "\n",
      "Accuracy on training set :  0.5\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "model = BaggingClassifier(decision_tree)\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'base_estimator__max_depth': [1, 2, 3],\n",
    "    'base_estimator__min_samples_leaf': [1, 2, 3],\n",
    "    'base_estimator__min_samples_split': [2, 3, 4],\n",
    "    'n_estimators': [2, 4, 6, 8, 10]\n",
    "}\n",
    "print(params)\n",
    "gridsearch = GridSearchCV(model, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)\n",
    "print()\n",
    "print(\"Accuracy on training set : \", gridsearch.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", gridsearch.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eabc75-dd0b-4919-b485-89cea18bdfef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Boosting\n",
    "### Adaboost\n",
    "#### Adaboost with logistic regression as base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e18623fe-37a7-4ceb-913a-9bc78d056c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "{'base_estimator__C': [0.01, 0.05, 0.1, 0.5], 'n_estimators': [5, 10, 20, 30]}\n",
      "...Done.\n",
      "Best hyperparameters :  {'base_estimator__C': 0.01, 'n_estimators': 5}\n",
      "Best validation accuracy :  0.38888888888888884\n",
      "\n",
      "Accuracy on training set :  0.625\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "logistic_regression = LogisticRegression(max_iter = 1000) # max_iter changed because of convergence warning\n",
    "model = AdaBoostClassifier(logistic_regression)\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'base_estimator__C': [0.01, 0.05, 0.1, 0.5,], # base_estimator__ prefix because C is a parameter from LogisticRegression! \n",
    "    'n_estimators': [5, 10, 20, 30] # n_estimators is a hyperparameter of the ensemble method\n",
    "}\n",
    "print(params)\n",
    "gridsearch = GridSearchCV(model, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)\n",
    "print()\n",
    "print(\"Accuracy on training set : \", gridsearch.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", gridsearch.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca61f5f-0e1e-4c6c-8534-8a1b550b8f6b",
   "metadata": {},
   "source": [
    "#### Adaboost with decision tree as base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1d35929-9b62-4b7d-9a73-9a981f8b4084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "{'base_estimator__max_depth': [1, 2, 3], 'base_estimator__min_samples_leaf': [1, 2, 3], 'base_estimator__min_samples_split': [2, 3, 4], 'n_estimators': [2, 4, 6, 8, 10]}\n",
      "...Done.\n",
      "Best hyperparameters :  {'base_estimator__max_depth': 1, 'base_estimator__min_samples_leaf': 1, 'base_estimator__min_samples_split': 2, 'n_estimators': 2}\n",
      "Best validation accuracy :  0.38888888888888884\n",
      "\n",
      "Accuracy on training set :  0.75\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "model = AdaBoostClassifier(decision_tree)\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'base_estimator__max_depth': [1, 2, 3],\n",
    "    'base_estimator__min_samples_leaf': [1, 2, 3],\n",
    "    'base_estimator__min_samples_split': [2, 3, 4],\n",
    "    'n_estimators': [2, 4, 6, 8, 10]\n",
    "}\n",
    "print(params)\n",
    "gridsearch = GridSearchCV(model, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)\n",
    "print()\n",
    "print(\"Accuracy on training set : \", gridsearch.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", gridsearch.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05824d1e-7a5d-42a0-9893-60b5d589e2c7",
   "metadata": {},
   "source": [
    "### Scikit-learn's GradientBoosting\n",
    "#### Boosting with logistic regression as base estimator: forbidden ‚õîÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f68266-f613-4665-9261-0f7ed6b3482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrid search...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m logistic_regression \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;66;03m# max_iter changed because of convergence warning\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGradientBoostingClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogistic_regression\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "logistic_regression = LogisticRegression(max_iter = 1000) # max_iter changed because of convergence warning\n",
    "model = GradientBoostingClassifier(logistic_regression) # this will fail: only decision trees are accepted as base estimators in GradientBoosting !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1942ba-578e-4704-8cce-8e7a6d3455ab",
   "metadata": {},
   "source": [
    "#### Boosting with decision tree as base estimator\n",
    "Actually, only trees are implemented in boosting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc1baed3-7994-407c-b3fb-fe9ed6b79993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "{'max_depth': [1, 2, 3], 'min_samples_leaf': [1, 2, 3], 'min_samples_split': [2, 3, 4], 'n_estimators': [2, 4, 6, 8, 10]}\n",
      "...Done.\n",
      "Best hyperparameters :  {'max_depth': 1, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 2}\n",
      "Best validation accuracy :  0.38888888888888884\n",
      "\n",
      "Accuracy on training set :  0.75\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'max_depth': [1, 2, 3], # no base_estimator_ prefix because these are all arguments of GradientBoostingClassifier\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'n_estimators': [2, 4, 6, 8, 10]\n",
    "}\n",
    "print(params)\n",
    "gridsearch = GridSearchCV(model, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)\n",
    "print()\n",
    "print(\"Accuracy on training set : \", gridsearch.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", gridsearch.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d178f5-5941-4108-80cc-8ca8199677bf",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "XGBoost is a different library, but it provides a [scikit-learn API](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn) that allows to train a model as if it had been built from a scikit-learn class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c457ce94-a138-48fc-99b0-7a653ffb3ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "{'max_depth': [2, 4, 6], 'min_child_weight': [1, 2, 3], 'n_estimators': [2, 4, 6, 8]}\n",
      "...Done.\n",
      "Best hyperparameters :  {'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 2}\n",
      "Best validation accuracy :  0.5\n",
      "\n",
      "Accuracy on training set :  0.5\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "xgboost = XGBClassifier()\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'max_depth': [2, 4, 6], # exactly the same role as in scikit-learn\n",
    "    'min_child_weight': [1, 2, 3], # effect is more or less similar to min_samples_leaf and min_samples_split\n",
    "    'n_estimators': [2, 4, 6, 8,] # exactly the same role as in scikit-learn\n",
    "}\n",
    "print(params)\n",
    "gridsearch = GridSearchCV(xgboost, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)\n",
    "print()\n",
    "print(\"Accuracy on training set : \", gridsearch.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", gridsearch.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf96ae6-f322-44f3-815d-0c6b4609e598",
   "metadata": {},
   "source": [
    "## Voting\n",
    "Contrary to bagging and boosting, the voting classifier is meant to mix different base estimators. Let's see an example with three base estimators:\n",
    "* logistic regression\n",
    "* decision tree\n",
    "* SVM with rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b7bfad0-aa7f-4c28-b201-40b107779656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "...Done.\n",
      "Best hyperparameters :  {'C': 10.0}\n",
      "Best validation accuracy :  0.611111111111111\n",
      "\n",
      "Accuracy on training set :  0.75\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'C': [0.1, 1.0, 10.0]\n",
    "}\n",
    "logreg_opt = GridSearchCV(logreg, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "logreg_opt.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", logreg_opt.best_params_)\n",
    "print(\"Best validation accuracy : \", logreg_opt.best_score_)\n",
    "print()\n",
    "print(\"Accuracy on training set : \", logreg_opt.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", logreg_opt.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b588f473-74ec-41ef-94eb-d0cbebd5db20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "...Done.\n",
      "Best hyperparameters :  {'max_depth': 1, 'min_samples_leaf': 2, 'min_samples_split': 3}\n",
      "Best validation accuracy :  0.5\n",
      "\n",
      "Accuracy on training set :  0.625\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'max_depth': [1, 2, 3], \n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 3, 4]\n",
    "}\n",
    "dt_opt = GridSearchCV(dt, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "dt_opt.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", dt_opt.best_params_)\n",
    "print(\"Best validation accuracy : \", dt_opt.best_score_)\n",
    "print()\n",
    "print(\"Accuracy on training set : \", dt_opt.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", dt_opt.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e211c40-7340-4b45-97e8-dffff25788a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "...Done.\n",
      "Best hyperparameters :  {'C': 0.1, 'gamma': 10.0}\n",
      "Best validation accuracy :  0.5555555555555555\n",
      "\n",
      "Accuracy on training set :  1.0\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "svm = SVC(kernel = 'rbf', probability = True)\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'gamma': [0.1, 1.0, 10.0]\n",
    "}\n",
    "svm_opt = GridSearchCV(svm, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "svm_opt.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", svm_opt.best_params_)\n",
    "print(\"Best validation accuracy : \", svm_opt.best_score_)\n",
    "print()\n",
    "print(\"Accuracy on training set : \", svm_opt.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", svm_opt.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f506f35-e480-49d1-b4fb-f94834ab9cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set :  0.875\n",
      "Accuracy on test set :  0.0\n"
     ]
    }
   ],
   "source": [
    "# Voting\n",
    "voting = VotingClassifier(estimators=[(\"logistic\", logreg), (\"tree\", dt), (\"svm\", svm)], voting='soft') # soft: use probabilities for voting\n",
    "voting.fit(X_train, Y_train)\n",
    "print(\"Accuracy on training set : \", voting.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", voting.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401d767-eefa-446c-ac28-5b01f1175dab",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "As voting, the stacking concerns different base estimators, but this time they will be used to estimate independent probabilities that will be plugged as input of a final estimator. The default final estimator is LogisticRegression, but it can be changed using the `final_estimator` parameter.\n",
    "\n",
    "In the examples below, the models `logreg`, `tree` and `svm` have already been optimized through a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a5f0441-ee64-401a-9724-50f1231009e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stacking classifier...\n",
      "...Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logistic</th>\n",
       "      <th>tree</th>\n",
       "      <th>svm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.441257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.490859</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.127465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.379211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.862778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.572142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.114630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.406044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.564332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.605352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.753250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.540797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logistic  tree       svm\n",
       "0  0.441257   0.0  0.633595\n",
       "1  0.490859   1.0  0.127465\n",
       "2  0.379211   0.0  0.862778\n",
       "3  0.572142   1.0  0.114630\n",
       "4  0.406044   0.0  0.735506\n",
       "5  0.564332   1.0  0.250769\n",
       "6  0.605352   0.0  0.753250\n",
       "7  0.540797   1.0  0.332807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set :  0.0\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# Default: LogisticRegression will be used as final estimator\n",
    "print('Training stacking classifier...')\n",
    "stacking = StackingClassifier(estimators = [(\"logistic\", logreg), (\"tree\", dt), (\"svm\", svm)], cv = 3)\n",
    "preds = stacking.fit_transform(X_train, Y_train)\n",
    "predictions = pd.DataFrame(preds, columns=stacking.named_estimators_.keys())\n",
    "print('...Done.')\n",
    "display(predictions)\n",
    "print(\"Accuracy on training set : \", stacking.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", stacking.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d35065-bc0c-44e7-a6e1-e3b11e00c542",
   "metadata": {},
   "source": [
    ":::tip Check for correlations\n",
    "As the predictions used in stacking are supposed to be independent, it's a good practice to check the correlation matrix of the outputs from the different estimators. If some predictions have a strong correlation, it's better to re-train the stacking model by dropping one of the estimators\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c53262cb-2cec-4e41-bf4e-f0178ffaecda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"700\" height=\"500\" style=\"\" viewBox=\"0 0 700 500\"><rect x=\"0\" y=\"0\" width=\"700\" height=\"500\" style=\"fill: rgb(255, 255, 255); fill-opacity: 1;\"/><defs id=\"defs-34b12a\"><g class=\"clips\"><clipPath id=\"clip34b12axyplot\" class=\"plotclip\"><rect width=\"540\" height=\"320\"/></clipPath><clipPath class=\"axesclip\" id=\"clip34b12ax\"><rect x=\"80\" y=\"0\" width=\"540\" height=\"500\"/></clipPath><clipPath class=\"axesclip\" id=\"clip34b12ay\"><rect x=\"0\" y=\"100\" width=\"700\" height=\"320\"/></clipPath><clipPath class=\"axesclip\" id=\"clip34b12axy\"><rect x=\"80\" y=\"100\" width=\"540\" height=\"320\"/></clipPath></g><g class=\"gradients\"/><g class=\"patterns\"/></defs><g class=\"bglayer\"/><g class=\"layer-below\"><g class=\"imagelayer\"/><g class=\"shapelayer\"/></g><g class=\"cartesianlayer\"><g class=\"subplot xy\"><g class=\"layer-subplot\"><g class=\"shapelayer\"/><g class=\"imagelayer\"/></g><g class=\"gridlayer\"><g class=\"x\"><path class=\"xgrid crisp\" transform=\"translate(170,0)\" d=\"M0,100v320\" style=\"stroke: rgb(0, 0, 0); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(350,0)\" d=\"M0,100v320\" style=\"stroke: rgb(0, 0, 0); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(530,0)\" d=\"M0,100v320\" style=\"stroke: rgb(0, 0, 0); stroke-opacity: 1; stroke-width: 1px;\"/></g><g class=\"y\"><path class=\"ygrid crisp\" transform=\"translate(0,366.67)\" d=\"M80,0h540\" style=\"stroke: rgb(238, 238, 238); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,260)\" d=\"M80,0h540\" style=\"stroke: rgb(238, 238, 238); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,153.32999999999998)\" d=\"M80,0h540\" style=\"stroke: rgb(238, 238, 238); stroke-opacity: 1; stroke-width: 1px;\"/></g></g><g class=\"zerolinelayer\"/><path class=\"xlines-below\"/><path class=\"ylines-below\"/><g class=\"overlines-below\"/><g class=\"xaxislayer-below\"/><g class=\"yaxislayer-below\"/><g class=\"overaxes-below\"/><g class=\"plot\" transform=\"translate(80,100)\" clip-path=\"url(#clip34b12axyplot)\"><g class=\"heatmaplayer mlayer\"><g class=\"hm\"><image xmlns=\"http://www.w3.org/2000/svg\" preserveAspectRatio=\"none\" height=\"320\" width=\"540\" x=\"0\" y=\"0\" xlink:href=\"data:image/png;base64,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\" style=\"opacity: 1;\"/></g></g></g><g class=\"overplot\"/><path class=\"xlines-above crisp\" d=\"M0,0\" style=\"fill: none;\"/><path class=\"ylines-above crisp\" d=\"M0,0\" style=\"fill: none;\"/><g class=\"overlines-above\"/><g class=\"xaxislayer-above\"><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"96.6\" transform=\"translate(170,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">logistic</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"96.6\" transform=\"translate(350,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">tree</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"96.6\" transform=\"translate(530,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">svm</text></g></g><g class=\"yaxislayer-above\"><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" transform=\"translate(0,366.67)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">logistic  </text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" transform=\"translate(0,260)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">tree  </text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" transform=\"translate(0,153.32999999999998)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">svm  </text></g></g><g class=\"overaxes-above\"/></g></g><g class=\"polarlayer\"/><g class=\"smithlayer\"/><g class=\"ternarylayer\"/><g class=\"geolayer\"/><g class=\"funnelarealayer\"/><g class=\"pielayer\"/><g class=\"iciclelayer\"/><g class=\"treemaplayer\"/><g class=\"sunburstlayer\"/><g class=\"glimages\"/><defs id=\"topdefs-34b12a\"><g class=\"clips\"/></defs><g class=\"layer-above\"><g class=\"imagelayer\"/><g class=\"shapelayer\"/></g><g class=\"infolayer\"><g class=\"g-gtitle\"/><g class=\"g-xtitle\"/><g class=\"g-ytitle\"/><g class=\"annotation\" data-index=\"0\" style=\"opacity: 1;\"><g class=\"annotation-text-g\" transform=\"rotate(0,170,366.67)\"><g class=\"cursor-pointer\" transform=\"translate(160,358)\"><rect class=\"bg\" x=\"0.5\" y=\"0.5\" width=\"20\" height=\"17\" style=\"stroke-width: 1px; stroke: rgb(0, 0, 0); stroke-opacity: 0; fill: rgb(0, 0, 0); fill-opacity: 0;\"/><text class=\"annotation-text\" text-anchor=\"middle\" x=\"10.34375\" y=\"13\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(0, 0, 0); fill-opacity: 1; white-space: pre;\">1.0</text></g></g></g><g class=\"annotation\" data-index=\"1\" style=\"opacity: 1;\"><g class=\"annotation-text-g\" transform=\"rotate(0,350,366.67)\"><g class=\"cursor-pointer\" transform=\"translate(336,358)\"><rect class=\"bg\" x=\"0.5\" y=\"0.5\" width=\"27\" height=\"17\" style=\"stroke-width: 1px; stroke: rgb(0, 0, 0); stroke-opacity: 0; fill: rgb(0, 0, 0); fill-opacity: 0;\"/><text class=\"annotation-text\" text-anchor=\"middle\" x=\"13.6875\" y=\"13\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(0, 0, 0); fill-opacity: 1; white-space: pre;\">0.54</text></g></g></g><g class=\"annotation\" data-index=\"2\" style=\"opacity: 1;\"><g class=\"annotation-text-g\" transform=\"rotate(0,530,366.67)\"><g class=\"cursor-pointer\" transform=\"translate(514,358)\"><rect class=\"bg\" x=\"0.5\" y=\"0.5\" width=\"31\" height=\"17\" style=\"stroke-width: 1px; stroke: rgb(0, 0, 0); stroke-opacity: 0; fill: rgb(0, 0, 0); fill-opacity: 0;\"/><text class=\"annotation-text\" text-anchor=\"middle\" x=\"15.6875\" y=\"13\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(255, 255, 255); fill-opacity: 1; white-space: pre;\">-0.53</text></g></g></g><g class=\"annotation\" data-index=\"3\" style=\"opacity: 1;\"><g class=\"annotation-text-g\" transform=\"rotate(0,170,260)\"><g class=\"cursor-pointer\" transform=\"translate(156,251)\"><rect class=\"bg\" x=\"0.5\" y=\"0.5\" width=\"27\" height=\"17\" style=\"stroke-width: 1px; stroke: rgb(0, 0, 0); stroke-opacity: 0; fill: rgb(0, 0, 0); fill-opacity: 0;\"/><text class=\"annotation-text\" text-anchor=\"middle\" x=\"13.6875\" y=\"13\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(0, 0, 0); fill-opacity: 1; white-space: pre;\">0.54</text></g></g></g><g class=\"annotation\" data-index=\"4\" style=\"opacity: 1;\"><g class=\"annotation-text-g\" transform=\"rotate(0,350,260)\"><g class=\"cursor-pointer\" transform=\"translate(340,251)\"><rect class=\"bg\" x=\"0.5\" y=\"0.5\" width=\"20\" height=\"17\" style=\"stroke-width: 1px; stroke: rgb(0, 0, 0); stroke-opacity: 0; fill: rgb(0, 0, 0); fill-opacity: 0;\"/><text class=\"annotation-text\" text-anchor=\"middle\" x=\"10.34375\" y=\"13\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(0, 0, 0); fill-opacity: 1; white-space: pre;\">1.0</text></g></g></g><g class=\"annotation\" data-index=\"5\" style=\"opacity: 1;\"><g class=\"annotation-text-g\" transform=\"rotate(0,530,260)\"><g class=\"cursor-pointer\" transform=\"translate(514,251)\"><rect class=\"bg\" x=\"0.5\" y=\"0.5\" width=\"31\" height=\"17\" style=\"stroke-width: 1px; stroke: rgb(0, 0, 0); stroke-opacity: 0; fill: rgb(0, 0, 0); fill-opacity: 0;\"/><text class=\"annotation-text\" text-anchor=\"middle\" x=\"15.6875\" y=\"13\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(255, 255, 255); fill-opacity: 1; white-space: pre;\">-0.95</text></g></g></g><g class=\"annotation\" data-index=\"6\" style=\"opacity: 1;\"><g class=\"annotation-text-g\" transform=\"rotate(0,170,153.32999999999998)\"><g class=\"cursor-pointer\" transform=\"translate(154,144)\"><rect class=\"bg\" x=\"0.5\" y=\"0.5\" width=\"31\" height=\"17\" style=\"stroke-width: 1px; stroke: rgb(0, 0, 0); stroke-opacity: 0; fill: rgb(0, 0, 0); fill-opacity: 0;\"/><text class=\"annotation-text\" text-anchor=\"middle\" x=\"15.6875\" y=\"13\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(255, 255, 255); fill-opacity: 1; white-space: pre;\">-0.53</text></g></g></g><g class=\"annotation\" data-index=\"7\" style=\"opacity: 1;\"><g class=\"annotation-text-g\" transform=\"rotate(0,350,153.32999999999998)\"><g class=\"cursor-pointer\" transform=\"translate(334,144)\"><rect class=\"bg\" x=\"0.5\" y=\"0.5\" width=\"31\" height=\"17\" style=\"stroke-width: 1px; stroke: rgb(0, 0, 0); stroke-opacity: 0; fill: rgb(0, 0, 0); fill-opacity: 0;\"/><text class=\"annotation-text\" text-anchor=\"middle\" x=\"15.6875\" y=\"13\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(255, 255, 255); fill-opacity: 1; white-space: pre;\">-0.95</text></g></g></g><g class=\"annotation\" data-index=\"8\" style=\"opacity: 1;\"><g class=\"annotation-text-g\" transform=\"rotate(0,530,153.32999999999998)\"><g class=\"cursor-pointer\" transform=\"translate(520,144)\"><rect class=\"bg\" x=\"0.5\" y=\"0.5\" width=\"20\" height=\"17\" style=\"stroke-width: 1px; stroke: rgb(0, 0, 0); stroke-opacity: 0; fill: rgb(0, 0, 0); fill-opacity: 0;\"/><text class=\"annotation-text\" text-anchor=\"middle\" x=\"10.34375\" y=\"13\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(0, 0, 0); fill-opacity: 1; white-space: pre;\">1.0</text></g></g></g></g></svg>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the predictions are independent\n",
    "# Correlation matrix\n",
    "corr_matrix = predictions.corr().round(2)\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_annotated_heatmap(corr_matrix.values,\n",
    "                                  x = corr_matrix.columns.tolist(),\n",
    "                                  y = corr_matrix.index.tolist())\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffc1f78a-dfdc-404f-8d62-4f075f884f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stacking classifier...\n",
      "...Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logistic</th>\n",
       "      <th>svm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.441257</td>\n",
       "      <td>0.688604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.490859</td>\n",
       "      <td>0.195893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.379211</td>\n",
       "      <td>0.876702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.572142</td>\n",
       "      <td>0.179399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.406044</td>\n",
       "      <td>0.773772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.564332</td>\n",
       "      <td>0.341632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.605352</td>\n",
       "      <td>0.788086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.540797</td>\n",
       "      <td>0.419795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logistic       svm\n",
       "0  0.441257  0.688604\n",
       "1  0.490859  0.195893\n",
       "2  0.379211  0.876702\n",
       "3  0.572142  0.179399\n",
       "4  0.406044  0.773772\n",
       "5  0.564332  0.341632\n",
       "6  0.605352  0.788086\n",
       "7  0.540797  0.419795"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set :  0.0\n",
      "Accuracy on test set :  0.5\n"
     ]
    }
   ],
   "source": [
    "# Re-train by dropping the tree estimator because it's quite correlated with the logistic regression\n",
    "print('Training stacking classifier...')\n",
    "stacking = StackingClassifier(estimators = [(\"logistic\", logreg), (\"svm\", svm)], cv = 3)\n",
    "preds = stacking.fit_transform(X_train, Y_train)\n",
    "predictions = pd.DataFrame(preds, columns=stacking.named_estimators_.keys())\n",
    "print('...Done.')\n",
    "display(predictions)\n",
    "print(\"Accuracy on training set : \", stacking.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", stacking.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9bc93f8-25ae-4c85-8b1f-816059efa059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stacking classifier...\n",
      "...Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logistic</th>\n",
       "      <th>svm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.441257</td>\n",
       "      <td>0.651634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.490859</td>\n",
       "      <td>0.083022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.379211</td>\n",
       "      <td>0.902867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.572142</td>\n",
       "      <td>0.072386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.406044</td>\n",
       "      <td>0.772935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.564332</td>\n",
       "      <td>0.200781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.605352</td>\n",
       "      <td>0.792466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.540797</td>\n",
       "      <td>0.287195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   logistic       svm\n",
       "0  0.441257  0.651634\n",
       "1  0.490859  0.083022\n",
       "2  0.379211  0.902867\n",
       "3  0.572142  0.072386\n",
       "4  0.406044  0.772935\n",
       "5  0.564332  0.200781\n",
       "6  0.605352  0.792466\n",
       "7  0.540797  0.287195"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set :  0.25\n",
      "Accuracy on test set :  1.0\n"
     ]
    }
   ],
   "source": [
    "# DecisionTree as final estimator\n",
    "print('Training stacking classifier...')\n",
    "stacking = StackingClassifier(estimators = [(\"logistic\", logreg), (\"svm\", svm)], final_estimator = DecisionTreeClassifier(), cv = 3)\n",
    "preds = stacking.fit_transform(X_train, Y_train)\n",
    "predictions = pd.DataFrame(preds, columns=stacking.named_estimators_.keys())\n",
    "print('...Done.')\n",
    "display(predictions)\n",
    "print(\"Accuracy on training set : \", stacking.score(X_train, Y_train))\n",
    "print(\"Accuracy on test set : \", stacking.score(X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
