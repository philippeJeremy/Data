{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Yelp\n",
    "\n",
    "The aim of this exercise is to allow a user to make an automatic search on <a href=\"https://www.yelp.fr/\" target=\"_blank\">Yelp</a> and store the results in a `.json` file. You will be guided through the different steps: making a form request with search keywords, parsing the search results, crawling all the result pages and storing the results into a file.\n",
    "\n",
    "⚠ **As scrapy is not made to launch several crawler processes in the same script, you will have to restart your notebook's kernel before completing each question!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.6.1-py2.py3-none-any.whl (264 kB)\n",
      "     |████████████████████████████████| 264 kB 10.6 MB/s            \n",
      "\u001b[?25hCollecting w3lib>=1.17.0\n",
      "  Using cached w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Using cached cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Using cached parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.9/site-packages (from scrapy) (36.0.1)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Using cached service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Using cached zope.interface-5.4.0-cp39-cp39-manylinux2010_x86_64.whl (255 kB)\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.2.0-py3-none-any.whl (87 kB)\n",
      "     |████████████████████████████████| 87 kB 5.2 MB/s             \n",
      "\u001b[?25hCollecting lxml>=3.5.0\n",
      "  Using cached lxml-4.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Using cached PyDispatcher-2.0.5-py3-none-any.whl\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Using cached itemadapter-0.4.0-py3-none-any.whl (10 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Using cached itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /opt/conda/lib/python3.9/site-packages (from scrapy) (22.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from scrapy) (59.8.0)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Using cached queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Using cached Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-22.2.0-py3-none-any.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 30.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: six>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (21.4.0)\n",
      "Collecting pyasn1-modules\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pyasn1\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Using cached constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Using cached incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (4.0.1)\n",
      "Collecting Automat>=0.8.0\n",
      "  Using cached Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting filelock>=3.0.8\n",
      "  Downloading filelock-3.6.0-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.9/site-packages (from tldextract->scrapy) (3.3)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from tldextract->scrapy) (2.27.1)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->scrapy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.10)\n",
      "Installing collected packages: w3lib, pyasn1, lxml, cssselect, zope.interface, requests-file, pyasn1-modules, parsel, jmespath, itemadapter, incremental, hyperlink, filelock, constantly, Automat, Twisted, tldextract, service-identity, queuelib, PyDispatcher, protego, itemloaders, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-22.2.0 constantly-15.1.0 cssselect-1.1.0 filelock-3.6.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.4.0 itemloaders-1.0.4 jmespath-1.0.0 lxml-4.8.0 parsel-1.6.0 protego-0.2.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.6.1 service-identity-21.1.0 tldextract-3.2.0 w3lib-1.22.0 zope.interface-5.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a class `YelpSpider(scrapy.Spider)` with `start_urls = ['https://www.yelp.fr/']`. In this class, define a `parse(self, response)` method that automatically fills Yelp's homepage form with : \"restaurant japonais\" as search keywords and \"Paris\" as search location. Then, define another method `after_search(self, response)` that parses the first page of results, and yields the name and url of each search result. Finally, declare a `CrawlerProcess` that will store the results in a file named `\"restaurant_japonais-paris.json\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-24 14:53:49 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2022-02-24 14:53:49 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.1.0, Python 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.1, Platform Linux-5.4.144+-x86_64-with-glibc2.31\n",
      "2022-02-24 14:53:49 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-02-24 14:53:49 [scrapy.extensions.telnet] INFO: Telnet Password: 3353827ae5fc5891\n",
      "2022-02-24 14:53:49 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-02-24 14:53:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-02-24 14:53:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-02-24 14:53:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-02-24 14:53:49 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-02-24 14:53:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-02-24 14:53:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-02-24 14:53:51 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-02-24 14:53:51 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: 02-Optional_Scraping_Yelp/restaurant_japonais-paris.json\n",
      "2022-02-24 14:53:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 914,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 84137,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 2.829941,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 2, 24, 14, 53, 51, 983387),\n",
      " 'httpcompression/response_bytes': 469717,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 52854784,\n",
      " 'memusage/startup': 52854784,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2022, 2, 24, 14, 53, 49, 153446)}\n",
      "2022-02-24 14:53:51 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!python 02-Optional_Scraping_Yelp/yelp1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Once you've managed to get the first page's results in `restaurant_japonais-paris.json`, complete the `after_search(self,response)` method to crawl the different result pages, such that all the search results will be stored in the file `\"restaurant_japonais-paris.json\"`. Restart your notebook's kernel, execute the new `CrawlerProcess` and check that all the search results (and not only the first page) are now stored in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-24 16:19:11 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2022-02-24 16:19:11 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.1.0, Python 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.1, Platform Linux-5.4.144+-x86_64-with-glibc2.31\n",
      "2022-02-24 16:19:11 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-02-24 16:19:11 [scrapy.extensions.telnet] INFO: Telnet Password: d9e8b220bb7d5214\n",
      "2022-02-24 16:19:11 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-02-24 16:19:11 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-02-24 16:19:11 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-02-24 16:19:11 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-02-24 16:19:11 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-02-24 16:19:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-02-24 16:19:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-02-24 16:19:59 [root] INFO: No next page. Terminating crawling process.\n",
      "2022-02-24 16:19:59 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-02-24 16:19:59 [scrapy.extensions.feedexport] INFO: Stored json feed (240 items) in: 02-Optional_Scraping_Yelp/restaurant_japonais-paris.json\n",
      "2022-02-24 16:19:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 11446,\n",
      " 'downloader/request_count': 26,\n",
      " 'downloader/request_method_count/GET': 26,\n",
      " 'downloader/response_bytes': 1351794,\n",
      " 'downloader/response_count': 26,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 47.589962,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 2, 24, 16, 19, 59, 229439),\n",
      " 'httpcompression/response_bytes': 7984575,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 240,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 54964224,\n",
      " 'memusage/startup': 54964224,\n",
      " 'request_depth_max': 24,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 26,\n",
      " 'scheduler/dequeued/memory': 26,\n",
      " 'scheduler/enqueued': 26,\n",
      " 'scheduler/enqueued/memory': 26,\n",
      " 'start_time': datetime.datetime(2022, 2, 24, 16, 19, 11, 639477)}\n",
      "2022-02-24 16:19:59 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!python 02-Optional_Scraping_Yelp/yelp2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you've just made the proof of concept of making an automated search on Yelp with Scrapy! Now, let's improve the script such that it will allow the user to make any search at any location 😎\n",
    "\n",
    "3. Use python's `input()` function to ask the user which keywords and location he would like to use, and save them into two variables: `search_keywords` and `search_location`. Then, change the `parse(self, response)` method such that it fills Yelp's form with user-defined keywords and location. Finally, change the `CrawlerProcess` such that it stores the results in a file named with the following format : `search_keywords-location.json`. \n",
    "\n",
    "Try your search engine with different keywords and locations ✌️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in the terminal to be able to interract\n",
    "```shell\n",
    "python 02-Optional_scraping_yelp/yelp3.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a script that will use the json file you just created to create a list of urls you wish to scrape and then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/FULL_STACK_12_WEEK_PROGRAM/M04-Data_Collection_and_Management/D02-Web_Scraping/03-Instructors/01-Solutions'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = open(\"02-Optional_Scraping_Yelp/restaurant_japonais-paris.json\")\n",
    "file = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.yelp.fr//biz/sanukiya-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/sushi-yaki-paris-4?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/onigiriya-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/aki-paris-2?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/okuda-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/y-izakaya-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/ippudo-paris-2?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/la-maison-du-sak%C3%A9-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/teppanyaki-ginza-onodera-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/ginza-paris-5?osq=restaurant+japonais']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_urls = [\"https://www.yelp.fr/\" + element[\"url\"] for element in file]\n",
    "list_urls[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Scrape the list of urls and gather the following data about each restaurant (or place):\n",
    "    * name\n",
    "    * stars\n",
    "    * number of votes\n",
    "    * address\n",
    "    * opening hours\n",
    "    * phone\n",
    "    * amenities\n",
    "    * reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-18 17:39:16 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-03-18 17:39:16 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.1, Platform Linux-5.4.144+-x86_64-with-glibc2.31\n",
      "2022-03-18 17:39:16 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True, 'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-03-18 17:39:16 [scrapy.extensions.telnet] INFO: Telnet Password: e155f1a4f63028fc\n",
      "2022-03-18 17:39:16 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2022-03-18 17:39:16 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-03-18 17:39:16 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-03-18 17:39:16 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-03-18 17:39:16 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-03-18 17:39:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-03-18 17:39:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-03-18 17:40:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-03-18 17:41:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-03-18 17:42:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-03-18 17:42:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yelp.fr//biz/sanukiya-paris?osq=restaurant+japonais> (failed 3 times): 503 Service Unavailable\n",
      "2022-03-18 17:42:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://www.yelp.fr//biz/sanukiya-paris?osq=restaurant+japonais>: HTTP status code is not handled or not allowed\n",
      "2022-03-18 17:42:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yelp.fr//biz/sushi-yaki-paris-4?osq=restaurant+japonais> (failed 3 times): 503 Service Unavailable\n",
      "2022-03-18 17:42:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://www.yelp.fr//biz/sushi-yaki-paris-4?osq=restaurant+japonais>: HTTP status code is not handled or not allowed\n",
      "2022-03-18 17:42:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yelp.fr//biz/onigiriya-paris?osq=restaurant+japonais> (failed 3 times): 503 Service Unavailable\n",
      "2022-03-18 17:42:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://www.yelp.fr//biz/onigiriya-paris?osq=restaurant+japonais>: HTTP status code is not handled or not allowed\n",
      "2022-03-18 17:42:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yelp.fr//biz/aki-paris-2?osq=restaurant+japonais> (failed 3 times): 503 Service Unavailable\n",
      "2022-03-18 17:42:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://www.yelp.fr//biz/aki-paris-2?osq=restaurant+japonais>: HTTP status code is not handled or not allowed\n",
      "^C\n",
      "2022-03-18 17:42:55 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force \n",
      "2022-03-18 17:42:55 [scrapy.core.engine] INFO: Closing spider (shutdown)\n"
     ]
    }
   ],
   "source": [
    "!python 02-Optional_Scraping_Yelp/yelp4.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
