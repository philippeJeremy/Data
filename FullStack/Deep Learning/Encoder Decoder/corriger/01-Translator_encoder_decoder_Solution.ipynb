{"cells":[{"cell_type":"markdown","metadata":{"id":"5NFuUYKHABlD"},"source":["# Translations with Ecoder Decoder\n","\n","We'll see that with LSTMs and the Encoder Decoder framework, we can do some pretty powerful things like: *translators* ! Let's see how we can create a French > English translator with TensorFlow \n","\n","### Tips \n","\n","Don't take the whole dataset at the beginning for your experiments, just take 5000 or even 3000 sentences. This will allow you to iterate faster and avoid bugs simply related to your need for computing power.\n","\n","Let's get started!"]},{"cell_type":"markdown","metadata":{"id":"em7GqFRv4nRZ"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1633714477653,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"2qUhyNPnhBtk","outputId":"0d6746ce-cae0-41da-8a06-2ae991989e22"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.6.0'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# Import necessaries librairies\n","import pandas as pd\n","import numpy as np \n","import sklearn\n","import tensorflow_datasets as tfds\n","import tensorflow as tf \n","tf.__version__"]},{"cell_type":"markdown","metadata":{"id":"3UoJ_qncuYKk"},"source":["## Importing data \n","\n","1. Load the data using the following url https://go.aws/38ECHUB you can read this using `pd.read_csv` with the `\"\\t\"` delimiter and `header=None`"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":1335,"status":"ok","timestamp":1633714479320,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"U2-Sd6lq_8ax","outputId":"984135a3-b0ea-40f1-b756-6ca385d3ad13"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0           1\n","0   Go.        Va !\n","1   Hi.     Salut !\n","2  Run!     Cours !\n","3  Run!    Courez !\n","4  Wow!  Ça alors !"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# Loading document txt function\n","df = pd.read_csv(\"https://go.aws/38ECHUB\", delimiter=\"\\t\", header=None)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"sJZCMavOoX3E"},"source":["2. Create an object `doc` containing the first 5000 rows from the file."]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1633714479323,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"YrcFSfBZMuQ7"},"outputs":[],"source":["# Let's just take a sample of 5000 sentences to avoid slowness. \n","doc = df.iloc[:5000,:]"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1633714479324,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"pqkCYT475rCv","outputId":"4b95ae03-3389-4b10-f468-070b970f29bf"},"outputs":[{"data":{"text/plain":["5000"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["len(doc)"]},{"cell_type":"markdown","metadata":{"id":"eu-0BjwvoiGY"},"source":["3. In your opinion, are we going to need to lemmatize and remove stop words for a translation problem?"]},{"cell_type":"markdown","metadata":{"id":"ZUxnMkUOwIKb"},"source":["No for translation problems we are not looking to extract the general meaning but to translate as precisely as possible, therefore we need the preprocessed text to be as close to the original as possible!"]},{"cell_type":"markdown","metadata":{"id":"_ha3hfzswnVd"},"source":["4. Add the word `<start>` to the beginning of each target sentence in order to create a new column named `padded_en`"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1633714479325,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"5AReCt5Zw5TF","outputId":"3abd6792-de20-4114-8292-64987a2e56c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>padded_en</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","      <td>&lt;start&gt; Go.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","      <td>&lt;start&gt; Hi.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","      <td>&lt;start&gt; Run!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","      <td>&lt;start&gt; Run!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","      <td>&lt;start&gt; Wow!</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0           1     padded_en\n","0   Go.        Va !   <start> Go.\n","1   Hi.     Salut !   <start> Hi.\n","2  Run!     Cours !  <start> Run!\n","3  Run!    Courez !  <start> Run!\n","4  Wow!  Ça alors !  <start> Wow!"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["doc[\"padded_en\"] = doc.iloc[:,0].apply(lambda x: \"<start> \"+x)\n","doc.head()"]},{"cell_type":"markdown","metadata":{"id":"0V_yusBAot6c"},"source":["5. Create two objects : `tokenizer_fr` and `tokenizer_en` that will be instances of the `tf.keras.preprocessing.text.Tokenizer` class. \n","\n","Be careful! Since we added a special token containing special characters, make sure you setup the tokenizers right so this token is well interpreted! (use the `filters` argument for example)."]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1633714479327,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"f5yAwku3eBro"},"outputs":[],"source":["tokenizer_fr = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')"]},{"cell_type":"markdown","metadata":{"id":"oNRgvH_co7Va"},"source":["6. Fit the tokenizers on the french, and **padded** english sentences respectively."]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1633714479329,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"OVxpAyw5eGAH"},"outputs":[],"source":["tokenizer_fr.fit_on_texts(doc.iloc[:,1])\n","tokenizer_en.fit_on_texts(doc[\"padded_en\"])"]},{"cell_type":"markdown","metadata":{"id":"CUojFPPwpCpo"},"source":["7. Create three new columns in your Dataframe for the encoded french, english, and padded english sentences."]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":518},"executionInfo":{"elapsed":508,"status":"ok","timestamp":1633714479816,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"X2ueH5W8eTBZ","outputId":"9265158b-f53f-49ec-a255-793ca2841a11"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>padded_en</th>\n","      <th>fr_indices</th>\n","      <th>en_indices</th>\n","      <th>padded_en_indices</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","      <td>&lt;start&gt; Go.</td>\n","      <td>[36]</td>\n","      <td>[11]</td>\n","      <td>[1, 11]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","      <td>&lt;start&gt; Hi.</td>\n","      <td>[404]</td>\n","      <td>[616]</td>\n","      <td>[1, 616]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>[1212]</td>\n","      <td>[111]</td>\n","      <td>[1, 111]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>[1213]</td>\n","      <td>[111]</td>\n","      <td>[1, 111]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","      <td>&lt;start&gt; Wow!</td>\n","      <td>[22, 1214]</td>\n","      <td>[872]</td>\n","      <td>[1, 872]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0           1     padded_en  fr_indices en_indices padded_en_indices\n","0   Go.        Va !   <start> Go.        [36]       [11]           [1, 11]\n","1   Hi.     Salut !   <start> Hi.       [404]      [616]          [1, 616]\n","2  Run!     Cours !  <start> Run!      [1212]      [111]          [1, 111]\n","3  Run!    Courez !  <start> Run!      [1213]      [111]          [1, 111]\n","4  Wow!  Ça alors !  <start> Wow!  [22, 1214]      [872]          [1, 872]"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["doc[\"fr_indices\"] = tokenizer_fr.texts_to_sequences(doc.iloc[:,1])\n","doc[\"en_indices\"] = tokenizer_en.texts_to_sequences(doc.iloc[:,0])\n","doc[\"padded_en_indices\"] = tokenizer_en.texts_to_sequences(doc[\"padded_en\"])\n","\n","doc.head()"]},{"cell_type":"markdown","metadata":{"id":"qFVz8F99yr7P"},"source":["8. We learned from the tutorial that the padded target sequences need to have the same length as the target sequences, so we will remove the last element of each padded target sequence (this will help us enforce teacher forcing)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1633714479817,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"9oYtlrlqzVBi","outputId":"dde0298e-cccc-4bda-fbb7-2b00290579bd"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>padded_en</th>\n","      <th>fr_indices</th>\n","      <th>en_indices</th>\n","      <th>padded_en_indices</th>\n","      <th>padded_en_indices_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","      <td>&lt;start&gt; Go.</td>\n","      <td>[36]</td>\n","      <td>[11]</td>\n","      <td>[1, 11]</td>\n","      <td>[1]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","      <td>&lt;start&gt; Hi.</td>\n","      <td>[404]</td>\n","      <td>[616]</td>\n","      <td>[1, 616]</td>\n","      <td>[1]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>[1212]</td>\n","      <td>[111]</td>\n","      <td>[1, 111]</td>\n","      <td>[1]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>[1213]</td>\n","      <td>[111]</td>\n","      <td>[1, 111]</td>\n","      <td>[1]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","      <td>&lt;start&gt; Wow!</td>\n","      <td>[22, 1214]</td>\n","      <td>[872]</td>\n","      <td>[1, 872]</td>\n","      <td>[1]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0           1  ... padded_en_indices padded_en_indices_clean\n","0   Go.        Va !  ...           [1, 11]                     [1]\n","1   Hi.     Salut !  ...          [1, 616]                     [1]\n","2  Run!     Cours !  ...          [1, 111]                     [1]\n","3  Run!    Courez !  ...          [1, 111]                     [1]\n","4  Wow!  Ça alors !  ...          [1, 872]                     [1]\n","\n","[5 rows x 7 columns]"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["doc[\"padded_en_indices_clean\"] = doc[\"padded_en_indices\"].apply(lambda x: x[:-1])\n","doc.head()"]},{"cell_type":"markdown","metadata":{"id":"mSU_oz8WpShr"},"source":["9. It's rather difficult to work with sequences with variable length, use zero-padding to normalize the length of all the sequences in each category."]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1633714479819,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"UpNwsqpjO0TM"},"outputs":[],"source":["# Use of Keras to create token sequences of the same length\n","padded_fr_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"fr_indices\"], padding=\"post\")\n","padded_en_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"en_indices\"], padding=\"post\")\n","teacher_forcing_en = tf.keras.preprocessing.sequence.pad_sequences(doc[\"padded_en_indices_clean\"], padding=\"post\")"]},{"cell_type":"markdown","metadata":{"id":"BOje1lkApr4Q"},"source":["10. What are the shapes of the arrays you just created for the french, padded english, and english sentences?"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1633714479821,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"38el3B3z3btO","outputId":"31d254a0-43a5-4b05-a822-ba2e5c786475"},"outputs":[{"data":{"text/plain":["(5000, 10)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["# Visualization of the shape of one of the tensors\n","padded_fr_indices.shape"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1633714479822,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"2qy5q9XSqQCB","outputId":"253c2a3b-8e80-4138-cc43-0bb7ab553de7"},"outputs":[{"data":{"text/plain":["(5000, 4)"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["padded_en_indices.shape"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1633714479823,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"u9DUbG8D0AgG","outputId":"bfcdfc36-e6ea-4670-e5ec-15fb51859cb6"},"outputs":[{"data":{"text/plain":["(5000, 4)"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["teacher_forcing_en.shape"]},{"cell_type":"markdown","metadata":{"id":"T0OTamAZ6gX4"},"source":["11. Use `sklearn` `train_test_split` function to divide your sample into train and validation sets."]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1633714479824,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"5xTFcEtSLamK"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","en_train, en_val, fr_train, fr_val, teacher_train, teacher_val =  train_test_split(padded_en_indices,\n","                                                                                   padded_fr_indices,\n","                                                                                   teacher_forcing_en,\n","                                                                                   test_size=0.3)"]},{"cell_type":"markdown","metadata":{"id":"2A63yoDAsZJ0"},"source":["## MODEL\n","\n","Now it's time to code the model, thankfully you can largely base yourself off the code provided during the demo!"]},{"cell_type":"markdown","metadata":{"id":"krxpB96-64Rx"},"source":["1. Create the following variables:\n","* `n_embed` the number of dimensions you want for the embeddings output spaces\n","* `n_lstm` the number of units you want for the lstm layers\n","* `fr_len` the length of a french sentence\n","* `en_len` the length of an english or teacher forcing sentence\n","* `vocab_size_fr` the number of tokens in the french vocabulary\n","* `vocab_size_en` the number of tokens in the english vocabulary (based of the padded sequences so the `<start>` is included!"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1633714479825,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"LO6v2m4q1Z9V"},"outputs":[],"source":["# let's start by defining the number of units needed for the embedding and\n","# the lstm layers\n","\n","n_embed = 128\n","n_lstm = 64\n","fr_len = padded_fr_indices.shape[1]\n","en_len = padded_en_indices.shape[1]\n","vocab_size_fr = len(tokenizer_fr.word_index)\n","vocab_size_en = len(tokenizer_en.word_index)"]},{"cell_type":"markdown","metadata":{"id":"MJPChtoE71DK"},"source":["2. Set up the encoder\n","\n","This will work in the same way as the demo, just make sure the input dimension of the embedding is equal to the number of words in the french vocabulary +1 (for the zero-padding)"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1633714479827,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"BPGvJYwx1bqB"},"outputs":[],"source":["encoder_input = tf.keras.Input(shape=(fr_len))\n","encoder_embed = tf.keras.layers.Embedding(input_dim=vocab_size_fr+1, output_dim=n_embed)\n","encoder_lstm = tf.keras.layers.LSTM(n_lstm, return_state=True)\n","\n","encoder_embed_ouput = encoder_embed(encoder_input)\n","encoder_output = encoder_lstm(encoder_embed_ouput)\n","\n","encoder = tf.keras.Model(inputs = encoder_input, outputs = encoder_output)"]},{"cell_type":"markdown","metadata":{"id":"dblxRs9G8PQC"},"source":["3. Try the encoder on the french train data (using the call method)"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1633714479828,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"Tc4TGKC02BFB","outputId":"6dbf83ea-098d-4334-ee89-6eb2707115b6"},"outputs":[{"data":{"text/plain":["[<tf.Tensor: shape=(3500, 64), dtype=float32, numpy=\n"," array([[ 0.03516009,  0.03100508,  0.02265517, ..., -0.01395426,\n","         -0.02611356,  0.009006  ],\n","        [ 0.03598815,  0.03333484,  0.01612609, ..., -0.01795677,\n","         -0.02568691,  0.00788247],\n","        [ 0.0381716 ,  0.03174238,  0.01842551, ..., -0.01462295,\n","         -0.02422647,  0.00832345],\n","        ...,\n","        [ 0.03569918,  0.03146626,  0.02163094, ..., -0.01790284,\n","         -0.02431938,  0.00937005],\n","        [ 0.037827  ,  0.03526489,  0.01708072, ..., -0.01626929,\n","         -0.02635708,  0.01015202],\n","        [ 0.03321327,  0.02693242,  0.0135023 , ..., -0.01000259,\n","         -0.02379798,  0.01082663]], dtype=float32)>,\n"," <tf.Tensor: shape=(3500, 64), dtype=float32, numpy=\n"," array([[ 0.03516009,  0.03100508,  0.02265517, ..., -0.01395426,\n","         -0.02611356,  0.009006  ],\n","        [ 0.03598815,  0.03333484,  0.01612609, ..., -0.01795677,\n","         -0.02568691,  0.00788247],\n","        [ 0.0381716 ,  0.03174238,  0.01842551, ..., -0.01462295,\n","         -0.02422647,  0.00832345],\n","        ...,\n","        [ 0.03569918,  0.03146626,  0.02163094, ..., -0.01790284,\n","         -0.02431938,  0.00937005],\n","        [ 0.037827  ,  0.03526489,  0.01708072, ..., -0.01626929,\n","         -0.02635708,  0.01015202],\n","        [ 0.03321327,  0.02693242,  0.0135023 , ..., -0.01000259,\n","         -0.02379798,  0.01082663]], dtype=float32)>,\n"," <tf.Tensor: shape=(3500, 64), dtype=float32, numpy=\n"," array([[ 0.07132903,  0.06252082,  0.04594512, ..., -0.02778306,\n","         -0.05129484,  0.01805663],\n","        [ 0.07302698,  0.06718089,  0.03269703, ..., -0.03574736,\n","         -0.0505277 ,  0.01579245],\n","        [ 0.07750386,  0.06398077,  0.03737562, ..., -0.02909358,\n","         -0.04764846,  0.01668139],\n","        ...,\n","        [ 0.07240549,  0.0634395 ,  0.04386986, ..., -0.0356677 ,\n","         -0.04776749,  0.01878225],\n","        [ 0.07677907,  0.07117263,  0.03464082, ..., -0.03239217,\n","         -0.05181399,  0.02034902],\n","        [ 0.06741559,  0.05421781,  0.02736798, ..., -0.01989932,\n","         -0.04694292,  0.02171513]], dtype=float32)>]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["encoder(fr_train)"]},{"cell_type":"markdown","metadata":{"id":"HZLS83o28Whq"},"source":["4. Set up the decoder\n","\n","This will work in the same way as the demo, just make sure the input dimension of the embedding is equal to the number of words in the french vocabulary +1 (for the zero-padding). The same goes for the last Dense layer!"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":387,"status":"ok","timestamp":1633714480190,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"zqgF-UNG2h5X"},"outputs":[],"source":["decoder_input = tf.keras.Input(shape=(en_len))\n","decoder_embed = tf.keras.layers.Embedding(input_dim=vocab_size_en+1, \n","                                          output_dim=n_embed)\n","decoder_lstm = tf.keras.layers.LSTM(n_lstm, return_sequences=True, return_state=True)\n","decoder_pred = tf.keras.layers.Dense(vocab_size_en+1, activation=\"softmax\")\n","\n","decoder_embed_output = decoder_embed(decoder_input) # teacher forcing happens here\n","# the decoder input is actually the padded target we created earlier, remember\n","# if target is: [91, 47, 89, 21, 62]\n","# the decoder input will be: [0, 91, 47, 89, 21]\n","decoder_lstm_output, _, _ = decoder_lstm(decoder_embed_output, initial_state=encoder_output[1:])\n","# in the step described above the decoder receives the encoder state as its\n","# initial state.\n","decoder_output = decoder_pred(decoder_lstm_output)\n","# then the dense layer will convert the vector representation for each element\n","# in the sequence into a probability distribution across all possible tokens\n","# in the vocabulary!\n","\n","decoder = tf.keras.Model(inputs = [encoder_input,decoder_input], outputs = decoder_output)\n","# all we need to do is put the model together using the input output framework!"]},{"cell_type":"markdown","metadata":{"id":"8d_9ixBl8miy"},"source":["5. Try the decoder on the french train data and the teacher forcing data"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1633714480190,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"4HkqPbEk2qk6","outputId":"0ddd04c4-d839-4299-9884-b6c7dc725906"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(3500, 4, 1258), dtype=float32, numpy=\n","array([[[0.00079917, 0.00078856, 0.00079867, ..., 0.00080011,\n","         0.00079256, 0.00079229],\n","        [0.00079764, 0.00079093, 0.00079901, ..., 0.00079932,\n","         0.00078957, 0.0007946 ],\n","        [0.00079621, 0.00079152, 0.00079583, ..., 0.00079952,\n","         0.00078921, 0.00079494],\n","        [0.00079622, 0.00079408, 0.00079392, ..., 0.00079977,\n","         0.00079026, 0.00079613]],\n","\n","       [[0.00079946, 0.00078939, 0.00079881, ..., 0.00080009,\n","         0.00079287, 0.00079212],\n","        [0.00079881, 0.00078832, 0.00079819, ..., 0.00079679,\n","         0.00079118, 0.00078969],\n","        [0.00079534, 0.00078864, 0.00079493, ..., 0.00079706,\n","         0.00079157, 0.00078996],\n","        [0.0007948 , 0.00079164, 0.00079286, ..., 0.00079765,\n","         0.00079189, 0.00079264]],\n","\n","       [[0.00079904, 0.00078939, 0.00079909, ..., 0.00079993,\n","         0.00079231, 0.00079234],\n","        [0.0007985 , 0.00078829, 0.00079849, ..., 0.00079665,\n","         0.00079078, 0.00078986],\n","        [0.00079875, 0.00079259, 0.00079924, ..., 0.00079699,\n","         0.0007923 , 0.00079135],\n","        [0.00079763, 0.00079447, 0.00079675, ..., 0.00079758,\n","         0.00079283, 0.00079343]],\n","\n","       ...,\n","\n","       [[0.00079916, 0.00078883, 0.00079918, ..., 0.00080005,\n","         0.0007927 , 0.00079221],\n","        [0.00080047, 0.00078975, 0.00079841, ..., 0.00079917,\n","         0.00079389, 0.00079321],\n","        [0.00079752, 0.00079132, 0.0007953 , ..., 0.00080062,\n","         0.00079228, 0.00079401],\n","        [0.00079789, 0.00079367, 0.00079417, ..., 0.00080055,\n","         0.00079242, 0.00079486]],\n","\n","       [[0.00079949, 0.00078923, 0.00079853, ..., 0.00080021,\n","         0.00079316, 0.00079215],\n","        [0.0007979 , 0.0007895 , 0.0008    , ..., 0.00079876,\n","         0.00079324, 0.0007897 ],\n","        [0.00079698, 0.00079185, 0.0007975 , ..., 0.00079911,\n","         0.00079332, 0.00079178],\n","        [0.0007964 , 0.00079385, 0.0007955 , ..., 0.0007994 ,\n","         0.00079335, 0.00079369]],\n","\n","       [[0.00079869, 0.00079052, 0.00079885, ..., 0.00079941,\n","         0.00079275, 0.00079233],\n","        [0.00079754, 0.00079024, 0.00079731, ..., 0.00079928,\n","         0.00079195, 0.00079294],\n","        [0.00079684, 0.00079258, 0.00079525, ..., 0.00079955,\n","         0.00079243, 0.00079415],\n","        [0.00079644, 0.00079446, 0.00079363, ..., 0.00079981,\n","         0.00079273, 0.00079544]]], dtype=float32)>"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["decoder([fr_train,teacher_train])"]},{"cell_type":"markdown","metadata":{"id":"cR06rACR8uD6"},"source":["6. Set up the inference decoder\n","\n","The code here will be identical to the one from the demo except if you changed some naming conventions!"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":504,"status":"ok","timestamp":1633714480691,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"hp9wT_7e3HWO"},"outputs":[],"source":["decoder_state_input_h = tf.keras.Input(shape=(n_lstm,))\n","decoder_state_input_c = tf.keras.Input(shape=(n_lstm,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","# at the first step of the inference, these input will be respectively the\n","# hidden state and C state of the encoder model\n","# for following steps, they will become the hidden and C state from the decoder\n","# itself since the input sequence is unknown we will have to predict step by step\n","# using a loop\n","\n","decoder_input_inf = tf.keras.Input(shape=(1))\n","decoder_embed_output = decoder_embed(decoder_input_inf)\n","# the decoder input here is of shape 1 because we will feed the elements in the \n","# sequence one by one\n","\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_embed_output, initial_state=decoder_states_inputs)\n","# the lstm layer works in the same way, the output from the embedding is used\n","# and the decoder state is used as described above\n","\n","decoder_states = [state_h, state_c]\n","# we store the lstm states in a specific object as we'll have to use them as \n","# initial state for the next inference step\n","\n","decoder_outputs = decoder_pred(decoder_outputs)\n","# the lstm output is then converted to a probability distribution over the\n","# target vocabulary\n","\n","decoder_inf = tf.keras.Model(inputs = [decoder_input_inf, decoder_states_inputs], \n","                     outputs = [decoder_outputs, decoder_states])\n","# Finally we wrap up the model building by setting up the inputs and outputs"]},{"cell_type":"markdown","metadata":{"id":"5MVJbjVE8_HU"},"source":["7. Compile the decoder (the training version) using the appropriate loss and metric functions."]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1633714480692,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"aQwqIpRn3Xjg"},"outputs":[],"source":["decoder.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",")"]},{"cell_type":"markdown","metadata":{"id":"CdKFFI3i9JaL"},"source":["8. Train the decoder for 50 epochs, this should take 10 minutes. Is there overfitting ?"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78015,"status":"ok","timestamp":1633714558697,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"26EGWOcz3vgG","outputId":"54803b9a-1287-4a98-e849-99dfc209c605"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","110/110 [==============================] - 6s 21ms/step - loss: 4.9265 - sparse_categorical_accuracy: 0.3499 - val_loss: 4.0519 - val_sparse_categorical_accuracy: 0.3503\n","Epoch 2/50\n","110/110 [==============================] - 1s 13ms/step - loss: 3.7258 - sparse_categorical_accuracy: 0.3727 - val_loss: 3.6789 - val_sparse_categorical_accuracy: 0.3935\n","Epoch 3/50\n","110/110 [==============================] - 1s 14ms/step - loss: 3.5260 - sparse_categorical_accuracy: 0.3989 - val_loss: 3.5757 - val_sparse_categorical_accuracy: 0.3983\n","Epoch 4/50\n","110/110 [==============================] - 1s 13ms/step - loss: 3.4243 - sparse_categorical_accuracy: 0.4013 - val_loss: 3.5028 - val_sparse_categorical_accuracy: 0.4040\n","Epoch 5/50\n","110/110 [==============================] - 1s 13ms/step - loss: 3.3338 - sparse_categorical_accuracy: 0.4061 - val_loss: 3.4290 - val_sparse_categorical_accuracy: 0.4087\n","Epoch 6/50\n","110/110 [==============================] - 1s 13ms/step - loss: 3.2429 - sparse_categorical_accuracy: 0.4225 - val_loss: 3.3594 - val_sparse_categorical_accuracy: 0.4250\n","Epoch 7/50\n","110/110 [==============================] - 1s 14ms/step - loss: 3.1464 - sparse_categorical_accuracy: 0.4366 - val_loss: 3.2610 - val_sparse_categorical_accuracy: 0.4357\n","Epoch 8/50\n","110/110 [==============================] - 2s 14ms/step - loss: 3.0236 - sparse_categorical_accuracy: 0.4503 - val_loss: 3.1686 - val_sparse_categorical_accuracy: 0.4470\n","Epoch 9/50\n","110/110 [==============================] - 1s 13ms/step - loss: 2.9135 - sparse_categorical_accuracy: 0.4626 - val_loss: 3.0985 - val_sparse_categorical_accuracy: 0.4635\n","Epoch 10/50\n","110/110 [==============================] - 1s 13ms/step - loss: 2.8141 - sparse_categorical_accuracy: 0.4769 - val_loss: 3.0317 - val_sparse_categorical_accuracy: 0.4642\n","Epoch 11/50\n","110/110 [==============================] - 1s 13ms/step - loss: 2.7264 - sparse_categorical_accuracy: 0.4850 - val_loss: 2.9831 - val_sparse_categorical_accuracy: 0.4712\n","Epoch 12/50\n","110/110 [==============================] - 1s 13ms/step - loss: 2.6447 - sparse_categorical_accuracy: 0.4944 - val_loss: 2.9388 - val_sparse_categorical_accuracy: 0.4787\n","Epoch 13/50\n","110/110 [==============================] - 1s 13ms/step - loss: 2.5647 - sparse_categorical_accuracy: 0.5021 - val_loss: 2.8910 - val_sparse_categorical_accuracy: 0.4833\n","Epoch 14/50\n","110/110 [==============================] - 1s 13ms/step - loss: 2.4882 - sparse_categorical_accuracy: 0.5084 - val_loss: 2.8436 - val_sparse_categorical_accuracy: 0.4903\n","Epoch 15/50\n","110/110 [==============================] - 1s 13ms/step - loss: 2.4082 - sparse_categorical_accuracy: 0.5176 - val_loss: 2.7993 - val_sparse_categorical_accuracy: 0.4960\n","Epoch 16/50\n","110/110 [==============================] - 1s 13ms/step - loss: 2.3329 - sparse_categorical_accuracy: 0.5279 - val_loss: 2.7527 - val_sparse_categorical_accuracy: 0.5070\n","Epoch 17/50\n","110/110 [==============================] - 1s 13ms/step - loss: 2.2448 - sparse_categorical_accuracy: 0.5499 - val_loss: 2.7075 - val_sparse_categorical_accuracy: 0.5200\n","Epoch 18/50\n","110/110 [==============================] - 1s 14ms/step - loss: 2.1615 - sparse_categorical_accuracy: 0.5719 - val_loss: 2.6611 - val_sparse_categorical_accuracy: 0.5340\n","Epoch 19/50\n","110/110 [==============================] - 1s 13ms/step - loss: 2.0780 - sparse_categorical_accuracy: 0.5891 - val_loss: 2.6210 - val_sparse_categorical_accuracy: 0.5448\n","Epoch 20/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.9999 - sparse_categorical_accuracy: 0.6040 - val_loss: 2.5806 - val_sparse_categorical_accuracy: 0.5532\n","Epoch 21/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.9241 - sparse_categorical_accuracy: 0.6137 - val_loss: 2.5430 - val_sparse_categorical_accuracy: 0.5548\n","Epoch 22/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.8551 - sparse_categorical_accuracy: 0.6222 - val_loss: 2.5165 - val_sparse_categorical_accuracy: 0.5583\n","Epoch 23/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.7886 - sparse_categorical_accuracy: 0.6302 - val_loss: 2.4912 - val_sparse_categorical_accuracy: 0.5642\n","Epoch 24/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.7270 - sparse_categorical_accuracy: 0.6411 - val_loss: 2.4654 - val_sparse_categorical_accuracy: 0.5702\n","Epoch 25/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.6685 - sparse_categorical_accuracy: 0.6494 - val_loss: 2.4680 - val_sparse_categorical_accuracy: 0.5562\n","Epoch 26/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.6156 - sparse_categorical_accuracy: 0.6594 - val_loss: 2.4349 - val_sparse_categorical_accuracy: 0.5765\n","Epoch 27/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.5614 - sparse_categorical_accuracy: 0.6674 - val_loss: 2.4074 - val_sparse_categorical_accuracy: 0.5820\n","Epoch 28/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.5141 - sparse_categorical_accuracy: 0.6741 - val_loss: 2.3879 - val_sparse_categorical_accuracy: 0.5843\n","Epoch 29/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.4614 - sparse_categorical_accuracy: 0.6831 - val_loss: 2.3882 - val_sparse_categorical_accuracy: 0.5875\n","Epoch 30/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.4147 - sparse_categorical_accuracy: 0.6932 - val_loss: 2.3746 - val_sparse_categorical_accuracy: 0.5902\n","Epoch 31/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.3700 - sparse_categorical_accuracy: 0.7011 - val_loss: 2.3630 - val_sparse_categorical_accuracy: 0.5923\n","Epoch 32/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.3312 - sparse_categorical_accuracy: 0.7091 - val_loss: 2.3701 - val_sparse_categorical_accuracy: 0.5928\n","Epoch 33/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.2882 - sparse_categorical_accuracy: 0.7158 - val_loss: 2.3514 - val_sparse_categorical_accuracy: 0.5958\n","Epoch 34/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.2472 - sparse_categorical_accuracy: 0.7247 - val_loss: 2.3315 - val_sparse_categorical_accuracy: 0.6027\n","Epoch 35/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.2127 - sparse_categorical_accuracy: 0.7299 - val_loss: 2.3306 - val_sparse_categorical_accuracy: 0.6058\n","Epoch 36/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.1728 - sparse_categorical_accuracy: 0.7364 - val_loss: 2.3203 - val_sparse_categorical_accuracy: 0.6037\n","Epoch 37/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.1332 - sparse_categorical_accuracy: 0.7440 - val_loss: 2.3203 - val_sparse_categorical_accuracy: 0.6108\n","Epoch 38/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.0969 - sparse_categorical_accuracy: 0.7526 - val_loss: 2.3176 - val_sparse_categorical_accuracy: 0.6152\n","Epoch 39/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.0629 - sparse_categorical_accuracy: 0.7601 - val_loss: 2.3172 - val_sparse_categorical_accuracy: 0.6137\n","Epoch 40/50\n","110/110 [==============================] - 1s 13ms/step - loss: 1.0321 - sparse_categorical_accuracy: 0.7669 - val_loss: 2.3214 - val_sparse_categorical_accuracy: 0.6143\n","Epoch 41/50\n","110/110 [==============================] - 1s 14ms/step - loss: 0.9964 - sparse_categorical_accuracy: 0.7710 - val_loss: 2.3125 - val_sparse_categorical_accuracy: 0.6187\n","Epoch 42/50\n","110/110 [==============================] - 1s 13ms/step - loss: 0.9650 - sparse_categorical_accuracy: 0.7800 - val_loss: 2.3168 - val_sparse_categorical_accuracy: 0.6200\n","Epoch 43/50\n","110/110 [==============================] - 1s 13ms/step - loss: 0.9375 - sparse_categorical_accuracy: 0.7879 - val_loss: 2.3013 - val_sparse_categorical_accuracy: 0.6262\n","Epoch 44/50\n","110/110 [==============================] - 1s 14ms/step - loss: 0.9090 - sparse_categorical_accuracy: 0.7916 - val_loss: 2.2956 - val_sparse_categorical_accuracy: 0.6210\n","Epoch 45/50\n","110/110 [==============================] - 1s 13ms/step - loss: 0.8789 - sparse_categorical_accuracy: 0.7989 - val_loss: 2.3065 - val_sparse_categorical_accuracy: 0.6203\n","Epoch 46/50\n","110/110 [==============================] - 1s 13ms/step - loss: 0.8550 - sparse_categorical_accuracy: 0.8046 - val_loss: 2.2972 - val_sparse_categorical_accuracy: 0.6278\n","Epoch 47/50\n","110/110 [==============================] - 1s 13ms/step - loss: 0.8275 - sparse_categorical_accuracy: 0.8104 - val_loss: 2.2988 - val_sparse_categorical_accuracy: 0.6307\n","Epoch 48/50\n","110/110 [==============================] - 1s 13ms/step - loss: 0.7998 - sparse_categorical_accuracy: 0.8193 - val_loss: 2.3126 - val_sparse_categorical_accuracy: 0.6285\n","Epoch 49/50\n","110/110 [==============================] - 1s 13ms/step - loss: 0.7734 - sparse_categorical_accuracy: 0.8229 - val_loss: 2.3042 - val_sparse_categorical_accuracy: 0.6310\n","Epoch 50/50\n","110/110 [==============================] - 1s 13ms/step - loss: 0.7523 - sparse_categorical_accuracy: 0.8277 - val_loss: 2.3125 - val_sparse_categorical_accuracy: 0.6293\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7fd0cf49ff90>"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["decoder.fit(x=[fr_train, teacher_train], y=en_train,epochs=50, validation_data=([fr_val, teacher_val], en_val))"]},{"cell_type":"markdown","metadata":{"id":"-w3kOO6g9YO4"},"source":["9. Adapt the code from the demo to make some predictions on the validation data.\n","\n","Be careful, in the demo the starting index for the teacher forcing sequences was 0, what index is the starting point of the teacher forcing sequences now?\n","\n","Set up the first decoder input with the right dimension too!"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1633714558698,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"mO87iO6a5dlc","outputId":"3433439e-c825-44d9-ecce-c3ce3cfaa4a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["pred: [ 2 18 54 54]\n","true: [26 29  5  0]\n","\n","\n","pred: [ 41 207 207  32]\n","true: [41 32  0  0]\n","\n","\n","pred: [8 6 8 7]\n","true: [ 28  78 192   0]\n","\n","\n","pred: [  2  25 238   5]\n","true: [  2 284   5   0]\n","\n","\n","pred: [  8   6 291 120]\n","true: [  8   6  34 574]\n","\n","\n","pred: [  3  46 106 101]\n","true: [  3  46 233   0]\n","\n","\n","pred: [ 15 130 136  39]\n","true: [13  5 89  0]\n","\n","\n","pred: [  3 128  77  32]\n","true: [  2 706   4   0]\n","\n","\n","pred: [ 25   5 117   7]\n","true: [ 25   5 516   0]\n","\n","\n","pred: [  2  33 193  77]\n","true: [  2  33 193   0]\n","\n","\n"]}],"source":["enc_input = fr_val\n","#classic encoder input\n","\n","dec_input = tf.ones(shape=(len(fr_val),1))\n","# the first decoder input is the special token 0\n","\n","enc_out, state_h_inf, state_c_inf = encoder(enc_input)\n","# we compute once and for all the encoder output and the encoder\n","# h state and c state\n","\n","dec_state = [state_h_inf, state_c_inf]\n","# The encoder h state and c state will serve as initial states for the\n","# decoder\n","\n","pred = []  # we'll store the predictions in here\n","\n","# we loop over the expected length of the target, but actually the loop can run\n","# for as many steps as we wish, which is the advantage of the encoder decoder\n","# architecture\n","for i in range(en_len):\n","  dec_out, dec_state = decoder_inf([dec_input, dec_state])\n","  # the decoder state is updated and we get the first prediction probability \n","  # vector\n","  decoded_out = tf.argmax(dec_out, axis=-1)\n","  # we decode the softmax vector into and index\n","  pred.append(decoded_out) # update the prediction list\n","  dec_input = decoded_out # the previous pred will be used as the new input\n","\n","pred = tf.concat(pred, axis=-1).numpy()\n","for i in range(10):\n","  print(\"pred:\", pred[i,:])\n","  print(\"true:\", en_val[i,:])\n","  print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"e_Rno_pP97hp"},"source":["10. Use the tokenizer to convert the target and predicted sequences back to text, what do you think of the translations?"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1633714558698,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"tSxu203W3gDC","outputId":"8d9fcf88-f230-4d2a-a1fd-7f82e30a29f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["true: i'll get you\n","pred i was good good\n","\n","\n","true: tom's here\n","pred tom's mad mad here\n","\n","\n","true: he's so young\n","pred he is he a\n","\n","\n","true: i called you\n","pred i can read you\n","\n","\n","true: he is no fool\n","pred he is kind too\n","\n","\n","true: i'm not mean\n","pred i'm not sure done\n","\n","\n","true: are you ready\n","pred be careful well in\n","\n","\n","true: i oppose it\n","pred i'm sorry busy here\n","\n","\n","true: can you pitch\n","pred can you swim a\n","\n","\n","true: i got fined\n","pred i got fined busy\n","\n","\n"]}],"source":["y_sample = tokenizer_en.sequences_to_texts(en_val)[:10]\n","pred_sample = tokenizer_en.sequences_to_texts(pred)[:10]\n","\n","for i, j in zip(y_sample,pred_sample):\n","  print(\"true:\", i)\n","  print(\"pred\", j)\n","  print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"VviPc59W-pJI"},"source":["11. Now that you reached the end of the exercise, go back to the beginning and increase the number of sentences your model will train on, this should significantly improve the quality of the predictions!"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"01-Translator_encoder_decoder.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"23e0aedb6d47e040503db7fcf09a0dff3cea72eb7d5d4c2596a602b1504b448c"}}},"nbformat":4,"nbformat_minor":0}
