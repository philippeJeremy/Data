{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra & Probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "* Doing matrix calculations\n",
    "* Understanding probabilities and how they are computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algebra\n",
    "\n",
    "\n",
    "### Vector\n",
    "\n",
    "For the rest of the course, it is important to understand what a scalar number is. For that, let's come back to what is a vector.\n",
    "\n",
    "\n",
    "#### Definition\n",
    "\n",
    "A vector is nothing more than a list of numbers. The size of this list can vary. For example, we can have :\n",
    "\n",
    "### $\\vec{u} = \\begin{bmatrix}5\\\\2\\end{bmatrix}$\n",
    "\n",
    "\n",
    "This is a two dimensions vector, represented by $\\vec{u}$ and can be represented like that: \n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1upNvSWhsE04GiPbxt_udfb2wsFn1VKC_)\n",
    "\n",
    "\n",
    "#### Operations on vectors\n",
    "\n",
    "We can add two vectors between them. Let $\\vec{u}$ and $\\vec{v}$ be two vectors with these coordinates:\n",
    "\n",
    "### $\\vec{u} = \\begin{bmatrix}5\\\\2\\end{bmatrix}$\n",
    "\n",
    "### $\\vec{v} = \\begin{bmatrix}3\\\\4\\end{bmatrix}$\n",
    "\n",
    "\n",
    "### $\\vec{u} + \\vec{v} = \\begin{bmatrix}5\\\\2\\end{bmatrix} + \\begin{bmatrix} 3\\\\4 \\end{bmatrix} = \\begin{bmatrix}8\\\\6\\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "We can also multiply a vector by a number. Now let's take $\\vec{w}$  with these coordinates :\n",
    "\n",
    "$\\vec{w} = \\begin{bmatrix}3\\\\2\\end{bmatrix}$ \n",
    "\n",
    "$4 * \\begin{bmatrix}3\\\\2\\end{bmatrix}=\\begin{bmatrix} 4 & * & 3\\\\ 4 & * & 2 \\end{bmatrix}$\n",
    "$= \\begin{bmatrix}12\\\\8\\end{bmatrix}$\n",
    "\n",
    "\n",
    "#### Vectors collinearity\n",
    "\n",
    "Let's take three vectors $\\vec{u}$, $\\vec{v}$, $\\vec{w}$ wich values are respectively :\n",
    "\n",
    "## $\\vec{u} = \\begin{bmatrix}4\\\\2\\end{bmatrix}$ \n",
    "\n",
    "## $\\vec{v} = \\begin{bmatrix}2\\\\6\\end{bmatrix}$\n",
    "\n",
    "## $\\vec{w} = \\begin{bmatrix}9\\\\7\\end{bmatrix}$\n",
    "\n",
    "\n",
    "We can express:\n",
    "\n",
    "## $\\vec{w} = 2\\vec{u} + \\frac{1}{2}\\vec{v}$\n",
    "\n",
    "\n",
    "\n",
    "That is to say that there is a linear relationship that allows to express $\\vec{w}$ as a function of $\\vec{u}$ and $\\vec{v}.\n",
    "\n",
    "In general, when you have two two-dimensional scalars, you can reach any point in a two-dimensional plane, EXCEPT if the vectors are collinear. That is to say that one can express $\\vec{u}$ as a function of $\\vec{v}$ or vice versa.\n",
    "\n",
    "Let's take an example with two other vectors $\\vec{i}$ and $\\vec{j}$, respectively :\n",
    "\n",
    "$\\vec{i} = \\begin{bmatrix}2\\\\4\\end{bmatrix}$ \n",
    "\n",
    "$\\vec{j} = \\begin{bmatrix}1\\\\2\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Here, we can see:\n",
    "\n",
    "# $\\vec{i} = 2\\vec{j}$\n",
    "\n",
    "\n",
    "In fact, we have a collinearity relationship between the two vectors, which is going to be problematic because if these two vectors represented explanatory variables in a Machine Learning algorithm, using both variables at the same time would increase the errors made in the predictions (you'll learn more about that during module M05 about supervised machine learning). In practice, when we detect some collinearity between two or more vectors, we usually drop one of the vectors to avoid problems and to simplify the set of variables to be used in the analysis.\n",
    "\n",
    "\n",
    "### Matrices\n",
    "\n",
    "\n",
    "#### Definition\n",
    "\n",
    "We talked about vectors above. Well, a matrix only represents vectors with multiple columns. Here's an example:\n",
    "\n",
    "### $X = \\begin{bmatrix} 3 & 0 & 2\\\\ 2 & 1 & -1 \\\\ 1 & 0 & -2\\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "#### Matricial product\n",
    "\n",
    "Very often we have to do matrix multiplications to be able to move in the space of the vectors defined by the variables of our dataset. This is why it is good to know how matrix multiplications are made.\n",
    "\n",
    "Let's take two matrices $X$ and $Y$ taking respectively the following values\n",
    "\n",
    "\n",
    "### $X = \\begin{bmatrix} 3 & 1 & 2\\\\ -1 & 3 & 2 \\\\ 3 & 1 & 4\\end{bmatrix}$\n",
    "\n",
    "### $Y = \\begin{bmatrix} 2 & 1 & 0\\\\ -1 & 3 & 2 \\\\ 1 & 0 & 1\\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "The product of the two matrices is computed as follows:\n",
    "\n",
    "### $XY = \\begin{bmatrix} 3 & 1 & 2\\\\ -1 & 3 & 2 \\\\ 3 & 1 & 4\\end{bmatrix} \\begin{bmatrix} 2 & 1 & 0\\\\ -1 & 3 & 2 \\\\ 1 & 0 & 1\\end{bmatrix} = \\begin{bmatrix} 6-1+2 & 3+3+0 & 0+2+2\\\\ -2-3+2 & -1+9+0 & 0+6+2 \\\\ 6-1+4 & 3+3+0 & 0+2+4\\end{bmatrix} = \\begin{bmatrix} 7 & 6 & 4 \\\\ -3 & 8 & 8 \\\\ 9 & 6 & 6 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "More generally, matrix multiplication is done as follows:\n",
    "\n",
    "## $\\begin{bmatrix} a & b & c\\\\ d & e & f \\\\ g & h & i\\end{bmatrix}$ $\\begin{bmatrix} a' & b' & c'\\\\ d' & e' & f' \\\\ g' & h' & i'\\end{bmatrix}$ $= \\begin{bmatrix} aa'+ bd'+ cg' & ab'+be'+ch' & ac'+bf'+ci'\\\\ da'+ed'+fg' & db'+ee'+fh' & dc'+ef'+fi' \\\\ ga'+hd'+ig' & gb'+he'+ih' & gc'+hf'+ii'\\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "Take a minute to analyze how the multiplication is done on the first column of the matrix and you will easily understand the rest of the columns.\n",
    "\n",
    "\n",
    "#### Matrix inversion\n",
    "\n",
    "Knowing how to invert matrices will allow us to solve linear equations with several unknowns. In particular, this will be the basis for linear regressions in Machine Learning.\n",
    "\n",
    "Consider the following equations:\n",
    "\n",
    "\n",
    "## $2x + y + 4z = -1$\n",
    "\n",
    "\n",
    "## $x + 2y - z = 0$\n",
    "\n",
    "\n",
    "## $3x + y + 2z = 2$\n",
    "\n",
    "\n",
    "\n",
    "This could be represented as follows:\n",
    "\n",
    "### $\\begin{bmatrix} 2&1&4\\\\1&2&-1 \\\\3&1&2 \\end{bmatrix}$$\\begin{bmatrix} x\\\\ y\\\\ z\\end{bmatrix} = \\begin{bmatrix} -1\\\\ 0\\\\ 2\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Let's assign the first matrix to $A$ the second matrix to the vector $/vec{u}$ and the third to $\\vec{v}$ such as :\n",
    "\n",
    "### $A = \\begin{bmatrix} 2&1&4\\\\ 1&2&-1¬†\\\\ 3&1&2 \\end{bmatrix}$\n",
    "\n",
    "### $\\vec{u} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$\n",
    "\n",
    "### $\\vec{v} = \\begin{bmatrix} -1 \\\\ 0 \\\\ 2 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "As we had seen above, we can write: \n",
    "\n",
    "## $A\\vec{u} = \\vec{v}$\n",
    "\n",
    "To solve the equation, we would like to compute the invert of A such that:\n",
    "\n",
    "## $\\vec{u} = \\frac{1}{A}\\vec{v}$\n",
    "\n",
    "\n",
    "\n",
    "When we are using matrices, we are not writing $\\frac{1}{A}$ but $A^{-1}$.\n",
    "\n",
    "Here the result:\n",
    "\n",
    "### $\\vec{u} = A^{-1}\\vec{v}$\n",
    "\n",
    "This $A^{-1}$ represents the **inverse matrix** of $A$\n",
    "\n",
    "\n",
    "We will not see in this course how to calculate the inverse of a matrix but it is important to understand this simple concept of solving equations which will be used later on to talk about Machine Learning.\n",
    "\n",
    "\n",
    "## Probabilities ##\n",
    "\n",
    "Let's move on to the probabilities. For some machine learning algorithms, we will need some theoretical knowledge of probability.\n",
    "\n",
    "\n",
    "### Definition\n",
    "\n",
    "A probability is simply the \"percentage chance\" that an event will occur. This is not a rigorous definition but an intuitive one. A probability is therefore a number between 0 and 1.\n",
    "\n",
    "For example, if we flip a coin, the probability of getting \"tail\" is 50% or 0.5. Let the event \"get tails\" be called A. Which will be modeled by:\n",
    "\n",
    "### $P(A) = \\frac{1}{2}$\n",
    "\n",
    "\n",
    "### Union and intersection of events\n",
    "\n",
    "Let A and B be two events. There are two ways of \"combining\" these two events : union and intersection\n",
    "\n",
    "#### Intersection of events\n",
    "\n",
    "The probability $P(A \\cap B)$, where $A \\cap B$ is the intersection of A and B, corresponds to the probability that event A **and** B occurred. \n",
    "\n",
    "Example :\n",
    "* We flip two coins. Let A be the event \"getting Head on first coin\" and B \"getting Head on second coin\". Then, the event \"getting two Heads\" is the intersection of A and B, and $P(\"getting two Heads\") = P(A \\cap B)$\n",
    "\n",
    "#### Union of events\n",
    "\n",
    "The probability $P(A \\cup B)$, where $A \\cup B$ is the union of A and B, corresponds to the probability that event A **or** B occurred. \n",
    "*Here, \"or\" refers to the logical \"or\", which means that the union $A \\cup B$ combines actually three events : \"A happened but B didn't happen\", \"B happened but A didn't happen\", \"both A and B happened\". It's interesting to notice that the latter refers to the intersection of A and B.*\n",
    "\n",
    "Example :\n",
    "* We throw a dice. Let A, B, C be the events : \"getting 2\", \"getting 4\", \"getting 6\". Then, the event \"getting an even number\" is the union of A, B and C, and $P(\"getting an even number\") = P(A \\cup B \\cup C)$\n",
    "\n",
    "\n",
    "#### Fundamental relationship between probability of union and probability of intersection\n",
    "The following formula is always true :\n",
    "\n",
    "### $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$\n",
    "\n",
    "### Conditional probability\n",
    "The conditional probability of A given B $P_B(A)$ is the probability of a, given that B has happened. The conditional probability can be computed from the probability of the intersection of events :\n",
    "\n",
    "### $P_B(A) = \\frac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "\n",
    "### Mutual exclusivity\n",
    "**Mutually exclusive events** (sometimes also called **disjoint events**) refers to events that can't occur at the same time. Let A and B be two disjoint events, then :\n",
    "\n",
    "### $P(A \\cap B) = 0$\n",
    "\n",
    "#### Fundamental properties related to mutual exclusivity\n",
    "* Let A and B be two disjoint events. Then $P(A \\cup B) = P(A) + P(B)$\n",
    "* Let e be all the possible events in an experiment. If all the events e are mutually exclusive, then $\\sum e = 1$\n",
    "\n",
    "#### Example\n",
    "We flip a coin. There are only to possible outcomes : A \"getting Heads\" and B \"getting Tails\". A and B are mutually exclusive events, so we can write $P(A \\cup B) = P(A) + P(B) = 1$. \n",
    "\n",
    "### Independent events\n",
    "\n",
    "We say that two events A and B are **independent** if the probability $P(A)$ doesn't change whether B has occured or not (and conversely, $P(B)$ is the same if A has occured or not). In this case :\n",
    "\n",
    "### $P(A \\cap B) = P(A) \\times P(B)$\n",
    "\n",
    "Example\n",
    "* We flip two coins. The events A \"getting Heads on first coin\" and B \"getting Heads on second coin\" are independent, then the probability of the intersection \"getting 2 Heads\" is : $P(\"getting 2 Heads\") = P(A) \\times P(B)$\n",
    "\n",
    "\n",
    "\n",
    "### Let's practice probabilities !\n",
    "\n",
    "Imagine we're in a class of students. In this class we have 10 students divided as follows:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td>Student\n",
    "   </td>\n",
    "   <td>Strong school subjects\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>1\n",
    "   </td>\n",
    "   <td>French, Maths\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>2\n",
    "   </td>\n",
    "   <td>Maths, French\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>3\n",
    "   </td>\n",
    "   <td>English, French\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>4\n",
    "   </td>\n",
    "   <td>Maths, English\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>5\n",
    "   </td>\n",
    "   <td>Spanish, French\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>6\n",
    "   </td>\n",
    "   <td>Physics, Maths\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>7\n",
    "   </td>\n",
    "   <td>Sports, English\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>8\n",
    "   </td>\n",
    "   <td>English, French\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>9\n",
    "   </td>\n",
    "   <td>French, Sports\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>10\n",
    "   </td>\n",
    "   <td>French, Maths\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "We can take the $A$ event: \"The student is strong at French\", and $B$ event \"The student is strong in maths\"\n",
    "\n",
    "We can say that:\n",
    "\n",
    "## $P(A) = \\frac{7}{10}$\n",
    "\n",
    "## $P(B) = \\frac{5}{10}$\n",
    "\n",
    "\n",
    "\n",
    "And we can also say:\n",
    "\n",
    "## $P(A \\cap B) = \\frac{3}{10}$\n",
    "\n",
    "\n",
    "Now, imagine if we knew all the students who were good at French. In your opinion, how many of these students who are strong in French are strong in Maths? To answer that, we can make a subgroup:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td>Student\n",
    "   </td>\n",
    "   <td>Strong school subjects\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>1\n",
    "   </td>\n",
    "   <td>French, Maths\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>2\n",
    "   </td>\n",
    "   <td>Maths, French\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>3\n",
    "   </td>\n",
    "   <td>English, French\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>5\n",
    "   </td>\n",
    "   <td>Spanish, French\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>8\n",
    "   </td>\n",
    "   <td>English, French\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>9\n",
    "   </td>\n",
    "   <td>French, Sports\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>10\n",
    "   </td>\n",
    "   <td>French, Maths\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Here you can see that it would be: $\\frac{3}{7}$. We just verified the formula of conditional probability :\n",
    "\n",
    "## $P_{A}(B) = \\frac{P(A \\cap B)}{P(A)} = \\frac{3/10}{7/10} = \\frac{3}{7}$ \n",
    "\n",
    "\n",
    "\n",
    "### Probability distributions\n",
    "\n",
    "\n",
    "#### Random variable\n",
    "\n",
    "To understand probability distributions, it is important to understand what a random variable is: it is a variable that can take all the values that correspond to the possible outcomes of a given experiment. Let's take an example:\n",
    "\n",
    "We throw a dice. Every time the value is even, the player wins 3‚Ç¨. If the value is odd, the player loses 3‚Ç¨.\n",
    "\n",
    "Let $X$ be the random variable that represents the player's possible earnings. So what are the possible values for $X$?\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td>Result for the dice throwed\n",
    "   </td>\n",
    "   <td>1\n",
    "   </td>\n",
    "   <td>2\n",
    "   </td>\n",
    "   <td>3\n",
    "   </td>\n",
    "   <td>4\n",
    "   </td>\n",
    "   <td>5\n",
    "   </td>\n",
    "   <td>6\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>X values\n",
    "   </td>\n",
    "   <td>-3‚Ç¨\n",
    "   </td>\n",
    "   <td>+3‚Ç¨\n",
    "   </td>\n",
    "   <td>-3‚Ç¨\n",
    "   </td>\n",
    "   <td>+3‚Ç¨\n",
    "   </td>\n",
    "   <td>-3‚Ç¨\n",
    "   </td>\n",
    "   <td>+3‚Ç¨\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "In this case, the random variable $X$ can only take two values : -3 and 3.\n",
    "\n",
    "\n",
    "The probability distribution of $X$ corresponds to the probability that the random variable \"$X$\" takes as a function of the possible values of \"$X$\". If we take the example from above:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td>Possible values for <b>$X$</b>\n",
    "\n",
    "   </td>\n",
    "   <td>-3\n",
    "   </td>\n",
    "   <td>+3\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Probability to obtain the value for <b>$x1$</b>\n",
    "\n",
    "   </td>\n",
    "   <td>\n",
    "3 / 6\n",
    "   </td>\n",
    "   <td>\n",
    "3 / 6\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Generally, we will write:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td>Possible values for <b>$X$</b>\n",
    "   </td>\n",
    "   <td>\n",
    "<b>x1</b>\n",
    "   </td>\n",
    "   <td>\n",
    "<b>x2</b>\n",
    "   </td>\n",
    "   <td>...\n",
    "   </td>\n",
    "   <td>\n",
    "<b>xn</b>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Probability to obtain the value for <b>$x1$</b>\n",
    "\n",
    "   </td>\n",
    "   <td>\n",
    "$P(X = x1)$\n",
    "   </td>\n",
    "   <td>\n",
    "$P(X = x2)$\n",
    "   </td>\n",
    "   <td>...\n",
    "   </td>\n",
    "   <td>\n",
    "$P(X = xn)$\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "In our example above, the random variable was discrete as it took only two possible values (-3 and 3). But keep in mind that we can also work with continuous random variables. In this case, the probability distribution becomes a function of X, and is usually called \"probability density\".\n",
    "\n",
    "There exist some usual probability distributions that are very often used when you're doing statistics or machine learning. For example, the Binomial distribution or the Normal distribution.\n",
    "\n",
    "\n",
    "#### Expected Value\n",
    "\n",
    "Before we talk about the most famous probability distribution, we need to explain what is the expectation as well as the variance.\n",
    "\n",
    "\n",
    "##### If the random variable is discrete\n",
    "\n",
    "If the random variable is discrete then the expectation is simply the weighted mean :\n",
    "\n",
    "## $E(X) = \\frac{\\sum_{0}^{n} p(X = x_i) x_{i} }{n}$\n",
    "\n",
    "\n",
    "##### If the random variable is continuous\n",
    "\n",
    "If our variable follow a continuous law then the expected value is: \n",
    "\n",
    "## $E(X) = \\int_{-\\infty}^{+\\infty} xf(x)$\n",
    "\n",
    "\n",
    "\n",
    "#### Variance\n",
    "\n",
    "The variance is the typical \"squared deviation\" between each point in your sample and the expected value. The formula is written as follows:\n",
    "\n",
    "## $V(X) = E((X - E(X))^2)$\n",
    "\n",
    "\n",
    "\n",
    "## Normal distribution\n",
    "\n",
    "\n",
    "##### What is that?\n",
    "\n",
    "There is a lot of talk about the normal distribution, but it is often difficult to understand why. Let's explain what it is first.\n",
    "\n",
    "The normal distribution follows a Gaussian function:\n",
    "\n",
    "\n",
    "## $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma })^{2}}$\n",
    "\n",
    "\n",
    "The function is quite complex but just imagine graphically that you get a bell curve.\n",
    "\n",
    "\n",
    "##### The Central Theorem Limit\n",
    "\n",
    "The reason why the Normal distribution is so famous, is because of the central limit theorem which says that, whatever the probability distribution that a random variable $X$ follows, if we take a very large number of samples, the mean of all these samples follows a normal distribution.  \n",
    "\n",
    "So this means that it doesn't matter which initial law our random variable follows, if we have enough measurements and consider the mean value, it will follow a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "What is a probability law - [https://www.youtube.com/watch?v=SiG-hFEN_iQ](https://www.youtube.com/watch?v=SiG-hFEN_iQ)\n",
    "\n",
    "Conditionnal probability - [https://www.youtube.com/watch?v=5oBnmZVrOXE](https://www.youtube.com/watch?v=5oBnmZVrOXE)\n",
    "\n",
    "Vectors explicated - [https://www.youtube.com/watch?v=fNk_zzaMoSs](https://www.youtube.com/watch?v=fNk_zzaMoSs)\n",
    "\n",
    "Vectors colinearity - [https://www.youtube.com/watch?v=k7RM-ot2NWY&t=2s](https://www.youtube.com/watch?v=k7RM-ot2NWY&t=2s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
