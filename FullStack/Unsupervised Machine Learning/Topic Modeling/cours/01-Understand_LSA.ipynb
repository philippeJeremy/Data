{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA \n",
    "\n",
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "LSA is short for *Latent Semantic Analysis*. It is a method mostly used for extracting and representing the contextual-usage meaning of words by statistical computations applied to a large corpus of text (Landauer and Dumais, 1997).\n",
    "\n",
    "This led to one main use case of this algorithm üëâ *Topic Modeling*. In this course, you will learn: \n",
    "\n",
    "* What is Topic Modeling \n",
    "* The difference between SVD and Truncated SVD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling üíÉüíÉ \n",
    "\n",
    "Here is a proper definition from <a href=\"https://en.wikipedia.org/wiki/Topic_model\" target=\"_blank\">Wikipedia</a>:\n",
    "\n",
    "*In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents*\n",
    "\n",
    "The whole idea is to create groups that we will call *topics* that each document in a corpus will belong to. \n",
    "\n",
    "There are **a lot** of use cases that uses Topic Modeling. Among them are: \n",
    "\n",
    "* **News:**  Labeling topics of articles i.e reading a headline and being able to automatically tell whether the article is talking about *Politics*, *Gaming*, *Sport* etc. üì∞\n",
    "\n",
    "* **Customer Support:** Imagine your customer has a problem and you need to what the topic is so that you redirect automatically to the right person. Topic modeling is great! üíÅüèº\n",
    "\n",
    "* **Meeting Summary:** Let's say you recorded a meeting, you can then give a summary using LSA üíº"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TruncatedSVD ‚úÇÔ∏è‚úÇÔ∏è\n",
    "\n",
    "### SVD Reminder üí°\n",
    "\n",
    "Let's consider the matrix X:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & \\ldots & x_{1,j} & \\ldots & x_{1,n}\\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{i,1} & \\ldots & x_{i,j} & \\ldots & x_{i,n}\\\\\n",
    "\\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{m,1} & \\ldots & x_{m,j} & \\ldots & x_{m,n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's note :\n",
    "* $t_i^T$ the vector $[x_{i,1}, \\ldots, x_{i,j}, \\ldots, x_{i,n}]$ representing the weight of word $i$ in each of the $n$ documents.\n",
    "* $d_j$ the vector\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{1,j}\\\\\n",
    "\\vdots\\\\\n",
    "x_{i,j}\\\\\n",
    "\\vdots\\\\\n",
    "x_{m,j} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "that describes the weight of each of the $m$ terms in document $j$.\n",
    "\n",
    "Therefore the dot product $t_i^T t_p$ gives the correlation between terms $i$ and $p$ across the corpus of documents. Equivalently $d_q^T d_j$ gives the correlation between documents $q$ and $j$. \n",
    "\n",
    "SVD states the existence of two orthogonal matrices $U$ and $V$ and a diagonal matrix $\\Sigma$ such that:\n",
    "\n",
    "$$X = U\\Sigma V^T$$\n",
    "\n",
    "An orthogonal matrix is a matrix which column vectors are orthogonal to one another and of vector norm equal to 1. Let's consider the matrix $O$ orthognal:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix}  \\\\ O_1 \\\\ \\\\ \\end{bmatrix} & \\ldots & \\begin{bmatrix} \\\\ O_i \\\\ \\\\ \\end{bmatrix} & \\ldots & \\begin{bmatrix} \\\\ O_l \\\\ \\\\ \\end{bmatrix}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $O_i$ is the column vector $i$ of O. The following equalities are given thanks to $O$ being orthognal:\n",
    "\n",
    "$$\n",
    "O_i^T Oj = 0 \\; \\forall \\; i\\neq j\n",
    "$$\n",
    "\n",
    "$$\n",
    "O_i^TO_i = 1 \\; \\forall i\n",
    "$$\n",
    "\n",
    "Which means that:\n",
    "\n",
    "$$\n",
    "O^TO=I_p\n",
    "$$\n",
    "\n",
    "The Identity square matrix of $p$ vectors.\n",
    "\n",
    "This major property of the singular value decomposition helps us write the following:\n",
    "\n",
    "$$X^TX = (U\\Sigma V^T)^T U\\Sigma V^T = V\\Sigma U^TU\\Sigma V^T = V\\Sigma^2V^T$$\n",
    "\n",
    "$$XX^T = U\\Sigma V^T(U\\Sigma V^T) = U\\Sigma V^TV\\Sigma U^T = U\\Sigma^2U^T$$\n",
    "\n",
    "Since $\\Sigma^2$ is diagonal it means that $V$ is made out of the <a href=\"https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors\" target=\"_blank\">eigenvectors</a> of $X^TX$ and U from the eigenvector of $XX^T$.\n",
    "\n",
    "If we rewrite slightly $U$ and $V$ in terms of their column vectors:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "\\begin{bmatrix}  \\\\ U_1 \\\\ \\\\ \\end{bmatrix} & \\ldots & \\begin{bmatrix} \\\\ U_i \\\\ \\\\ \\end{bmatrix} & \\ldots & \\begin{bmatrix} \\\\ U_l \\\\ \\\\ \\end{bmatrix} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 & \\ldots & 0 \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "0 & \\ldots & \\sigma_l\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\begin{bmatrix} & V_1 & & \\end{bmatrix} \\\\ \\vdots \\\\ \\begin{bmatrix} & V_j & & \\end{bmatrix} \\\\ \\vdots \\\\ \\begin{bmatrix} & V_l & & \\end{bmatrix} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $\\sigma_i$ are called the singular values, $U_i$ are the left singular vectors and $V_j$ are the right singular vectors. Notice the only part of $U$ that contributes to $t_i^T$ is the $i^{th}$ row, let's call this row vector from $U$ : $\\hat{t_i^T}$. Similarly the only part from $V$ contributing to $d_j$ is the $j^{th}$ column, let's note this column vector from $V$: $\\hat{d_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with sparse matrices üêö\n",
    "\n",
    "<a href=\"https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\" target=\"_blank\">Sparse matrices</a> are basically matrices with a lot of `0`. Even if it does not seem like a big deal, it actually is for computers because it requires a lot more calculation. \n",
    "\n",
    "Therefore the bigger your matrix, the slower your computer will be to make calculus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculus took for sparse matrix 0.00403 seconds\n",
      "Calculus took for dense matrix 0.03961 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import time to keep track of time\n",
    "import time\n",
    "# Import numpy to create matrices\n",
    "import numpy as np \n",
    "# import sparse module from SciPy package \n",
    "from scipy import sparse\n",
    "# import uniform module to create random numbers\n",
    "from scipy.stats import uniform\n",
    "# import NumPy\n",
    "import numpy as np\n",
    "\n",
    "sparse_matrix = sparse.random(100,100)\n",
    "\n",
    "# Calculate time to create dot product\n",
    "start_time = time.time()\n",
    "\n",
    "sparse_matrix.dot(sparse_matrix)\n",
    "\n",
    "print(\"Calculus took for sparse matrix {:.5f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "# Create a dense matrix\n",
    "dense_matrix = sparse_matrix.toarray()\n",
    "\n",
    "\n",
    "# Calculate time to perform a dot product on dense matrix\n",
    "start_time = time.time()\n",
    "\n",
    "dense_matrix.dot(dense_matrix)\n",
    "\n",
    "print(\"Calculus took for dense matrix {:.5f} seconds\".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for 2 100x100 matrices, it took almost 10x more time to calculate a dot product for a sparse matrix. üòÆ \n",
    "\n",
    "The problem is that when we perform SVD, we do a lot of dot products... üëâüëâ You guessed the problem. \n",
    "\n",
    "Indeed, when dealing with NLP or recommandation systems for example, you might end up dealing with sparse matrices. So how can we solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!(https://vimeo.com/486595658)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TruncatedSVD algorithm\n",
    "\n",
    "TruncatedSVD (AKA LSA) is perfect when dealing with sparse matrices. Instead of using perfect SVD like this: \n",
    "\n",
    "$$A = U \\Sigma V^\\intercal$$\n",
    "\n",
    "Where $A$ is an $n \\times m$ matrice \n",
    "\n",
    "Each matrices look like this: \n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/SVD_no_legend.png\" alt=\"SVD\"/>\n",
    "\n",
    "\n",
    "Especially let's pay attention to $\\Sigma$ that has $r$ singular values. Well, what TruncatedSVD says is that we can approximate $A$ simply by taking $k$ highest singular values (where $k < r$). \n",
    "\n",
    "Another particularity that is specified in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html?highlight=truncated%20svd#sklearn.decomposition.TruncatedSVD\" target=\"_blank\">`sklearn`</a> is that truncatedSVD does not center data. Which definitely make calculations faster when dealing with sparse matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!(https://vimeo.com/486596019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Topic_model\" target=\"_blank\">Topic Model</a>\n",
    "\n",
    "* <a href=\"https://medium.com/@fatmafatma/industrial-applications-of-topic-model-100e48a15ce4\" target=\"_blank\">Industrial Applications of Topic Model</a>\n",
    "\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors\" target=\"_blank\">Eigenvectors</a> \n",
    "\n",
    "* <a href=\"https://cmdlinetips.com/2018/03/sparse-matrices-in-python-with-scipy/\" target=\"_blank\">Sparse Matrices with scipy</a>\n",
    "\n",
    "* <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html?highlight=truncated%20svd#sklearn.decomposition.TruncatedSVD\" target=\"_blank\">TruncatedSVD</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "16196ea7eff63910081d4e10ae1bdb1eb18fd83cb470bb8efbb9fa6b0c724af5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
