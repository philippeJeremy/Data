{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZJ6IVK_g_x1"
   },
   "source": [
    "# Convolutional Neural Networks üåÜüåÜ\n",
    "\n",
    "Convolutional neural networks revolutionized the field of image processing in machine learning thanks to a very simple yet very powerful idea: images have hierarchical structures organized in space. Convolutional neural networks are actually applicable to any data that has a hierarchical structure along at least one dimension (think sound for 1D, images for 2D and volumes for 3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GACMRBtZ1e9m"
   },
   "source": [
    "## What will you learn in this course? üßêüßê\n",
    "\n",
    "This lesson will focus on convolutional neural networks and the various layers that are often used in model architectures using convolutional neurons. We will start by giving a general introduction then move on to a demo with code, and additional ressources that help interpret what these models are \"seeing\".\n",
    "\n",
    "* Convolution principle\n",
    "  * Spatial structure of images\n",
    "  * Hierarchical structure of images\n",
    "  * Convolutional layer\n",
    "    * Image representation\n",
    "    * Kernel size\n",
    "    * Stride\n",
    "    * Padding\n",
    "  * Pooling Layers\n",
    "  * General architecture guidelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLzm8Rd13mBw"
   },
   "source": [
    "## Convolution principle üëÄ\n",
    "\n",
    "Convolution relies on the idea that to analyse images efficiently, it is important to take advantage of their characteristics. Images are mutidimensional arrays containing lots of information, each pixel may be interpreted as a feature in itself. When using fully connected neural networks on images, the first layer affects a different parameter to each pixel in the image. This means that fully connected neurons are attempting to create new features using the whole image at once. This is technically feasible as fully connected neural networks are theoretically able to model any function no matter how complicated, however convergence for such neural networks in practice proves to be very difficult.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN6odlNl40rN"
   },
   "source": [
    "### Spatial structure of images ‚ú®\n",
    "Images are spatial objects, you cannot analyse pixels independently because what makes a pattern in an image is a collection of pixels that are spatially organized in a specific fashion. Also it does not matter where such or such pattern is located in the image.\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/flowers-324175_1920.jpg\" />\n",
    "\n",
    "In the example above, it does not matter if the flower is located in the center of the image or a little to the right, it is still the same flower, rougly the same size, color and shape, you can recognize no matter where it is. However, when you think in terms of fully connected neural networks, a different parameter is affected to each and every pixel in the image, therefore in order for the model to be able to recognize a flower in the center of the image or on the side, it will have to optimize to sets of parameters in order to detect the same pattern twice, that would be inneficient. This is one motivation for adding constraints on neurons in order to take advantage of these spatial properties. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "414xO10T7ezE"
   },
   "source": [
    "### Hierarchical structure of images üëë \n",
    "\n",
    "Patterns in an image are not all of the same size, when you think about the the cherry tree branch example from before, we recognize the cherry tree branch because we see several flowers sitting next to each other, each flower is composed of petals, these petals are recognizable thanks to some texture that can be observed on them. All these different examples are just as many patterns that are organized hierarchically to form meaning in the image. Then again, thinking about fully connected neurons, they would try to detect all these levels of patterns indinstinctly, which is a rather difficult task. Hence the motivation to find a way to gradually detect bigger and bigger partterns in images while moving forward in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C49rdVxm3q6I"
   },
   "source": [
    "### Image representation ‚úèÔ∏è\n",
    "\n",
    "Before we get to the details of how convolutional neurons work, let's take a moment to understand how images are represented in order to be analysed by computer programs.\n",
    "\n",
    "Images are composed of pixels, the pixels are memory slots that are organized specially and contain information about what the computer should display at every position on the image.\n",
    "\n",
    "Pixels are organized along three dimensions in images, the width and height of the image, and on different channels. Channels represent the \"colors\" used in the image.\n",
    "* Black and White images: all the pixels along the width and the height contain only one piece of information, its an integer usually between 0 and 255 indicating the intenisty of the pixel on a greyscale. 0 represents black and 255 represents white. Black and white images are therefore represented on only one channel, leading to a shape `(height, width, 1)`\n",
    "* Color images: pixels in those images contain either three values indicating the intensity of Red, Green and Blue (RGB) with an integer ranging from 0 to 255. Some images contain and extra information called Alpha for transparency properties. Therefore color images are represented on three or four channels, with shape  `(height, width, 3)` or `(height, width, 4)`.\n",
    "\n",
    "Note that a black and white image can easily be represented with 3 channels by repeating the value on each pixel on all three colors (e.g. a pixel with a greyscale value of `[56]` will be represented in RGB by `[56,56,56]`) which will code for the same shade of grey with the RGB convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJbjJQjozr5l"
   },
   "source": [
    "### Convolutional layer üî¨\n",
    "Convolutional layers are an answer to both difficulties that fully connected neurons are facing when extracting information from images, they are able to take advantage of both the spatial structure of images and their hierarchical structure.\n",
    "\n",
    "A convolutional layer will not carry a parameter for each element in the input, instead it will carry a filter with a fixed shape that will be determined by the data scientist using is. It can be seen as a window with a given width and height with a parameter on each position and along each input channel.\n",
    "\n",
    "The figure below represent an example of convolutional neuron with three input channels:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/filter.png\" />\n",
    "\n",
    "This convolutional neuron is composed of three structures (one for each incoming channel) with their own parameters, plus a bias.\n",
    "\n",
    "Before we go into more details let's see how a convolutional layer transforms an input image with three channels into an output:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/conv_gif.gif\" />\n",
    "\n",
    "The weights on the convolution filters can be interpreted as patterns that the convolution layer will be able to detect in the image. Because these filters move around the input while the parameters stay constant, they are able to detect the same pattern anywhere in the image, and therefore take advantage of the spatial structure of images. The idea that a given pattern can be interpreted in the same way wherever it is located in the image is one of the reasons why convolutional neurons are better at analysing images than fully-connected neurons.\n",
    "\n",
    "Just to give you some perspective in terms of complexity and to explain why we say that building convolutional layers consists in adding constraints on how fully connected layer work let's think about the number of parameters.\n",
    "A fully-connected neuron with an input of shape `(200,200,3)` has a total numberof parameters equal to $200 \\times 200 \\times 3 + 1 = 120 001$ parameters, where a convolutional neuron with a kernel size of `(3,3)` with the same input will only have $3 \\times 3 \\times 3 + 1 = 10$ parameters. The constraint on the number of parameters on a single layer is enormous!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pikSVyz15ElS"
   },
   "source": [
    "#### Kernel size üìè\n",
    "\n",
    "The kernel size represents the height and the width of the filter, in the example the kernel size is `(3,3)`, three pixels wide and three pixels high.\n",
    "\n",
    "The kernel size represent the size of the patterns that the neuron will be able to detect in an image, it maps the incoming image into an output image that will show all the coordinates where the filter pattern and the image pattern are well aligned. A high value on the output image indicates a position where the input values and the filter described similar patterns.\n",
    "\n",
    "That means that convolution layers transform image inputs into image outputs that map all the positions in the input image where a certain pattern was detected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b74kfIpxzM43"
   },
   "source": [
    "#### Stride üëû\n",
    "\n",
    "The stride represents how the convolution filter moves across the input image. In the example above, the stride is equal to 2, the convolution filter moves two pixels away between producing two different outputs. A higher stride means more reduction of the output shape compared to the input shape, although this is usually not the preferred way to shrink the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IxQU-Y60nhF"
   },
   "source": [
    "#### Padding üí†\n",
    "\n",
    "Padding is a technique consisting in adding pixels filled with zeros all around the original input across all convolution dimensions. In the example the padding is represented by the white squares surrounding the blue squares.\n",
    "\n",
    "The padding has two purpose, the first one is to force the convolution layer not to shrink the spatial dimension of the output compared to the input, this makes it possible to create very deep convolutional networks without being limited by the input spatial dimension. The second advantage of the padding is that it gives more room for the convolution filter to move around the input's border pixels, and therefore detect information located near the border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYl5aqlxhQyB"
   },
   "source": [
    "### Pooling Layers üèä\n",
    "\n",
    "Sometimes we do not want to use convolution layers to reduce the size of the input data, in those cases we will use a stride of 1 and some padding. But when you think of classification tasks for example, we need to predict a few classes from images which are objects with enormous numbers of pixels aka input variables. So it's important to start summarize the information flowing through the network at some point! For this purpose we can use Pooling layers. Pooling layers take the input image and summarize its content according to an agregation function, usually `max` or `sum`.\n",
    "\n",
    "The figure below illustrates how pooling can be apply to an input.\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/pooling.gif\" />\n",
    "\n",
    "Pooling layers may be setup pretty much like convolutional layer, with a pool size (the equivalent of the kernel size), and a stride.\n",
    "\n",
    "MaxPooling extracts the max value of the input elements, it can be interpreted like so: let's say the previous neuron was able to detect petal texture anywhere in the input image, then the biggest elements of the output would correspond to positions in the image where this pattern was detected, then the pooling would say \"if I saw petal in one of these four places, then there's petal here\" it gives all its attention to the most activated positions of the input and forgets the rest. The Average pooling is softer and will give equal importance to all positions in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FklA33t3kmC"
   },
   "source": [
    "### General architecture guidelines üè∞\n",
    "\n",
    "It is generally admitted that for fully connected neural network, experience shows that even though 2 Dense layer with enough neurons on them are theoretically able to approximate any function with arbitrary precision it usually works better to make fully connected networks with three Dense layers, but more does not really help.\n",
    "\n",
    "For convolutional networks it's the complete opposite, the deeper the better! Building networks with more and more conolutional layers following one another yields to better prediction in practice, and data scientists have worked on several tricks to train deeper and deeper neural networks. Recent research shows however that neural networks do not necessarily need to be deep, but deeper network have an easier time understanding patterns in images than their more shallow counterpart without help.\n",
    "\n",
    "Indeed, chaining several convolutional layers helps taking advantage of the hierarchical structure of images, let's explain why. Let's say the first convolutional layer has a kernel size of $3 \\times 3$ pixels. For every position it lands on, it produces one output out of the 9 pixels it analyses. That means one element in the output image represents 9 pixels in the original image. Now this output image gets fed to another convolution layer with kernel size $3 \\times 3$, it will produce one output with each position it's able to move to. Each element of the second output therefore represents 9 pixels from the first output, which each represent 9 pixels from the input image. In total after just two convolution layers, each element of the second output may represent a patter of $9 \\times 9$ pixels from the original image, after a third layer like this $27 \\times 27$ pixels and so on...\n",
    "\n",
    "In terms of interpretation, we can say that filters on the first layer may detect small patterns in the image, like texture, then the deeper we go into the network, the bigger the patterns that may be detected by the convolution layers, like petals, flowers, groups of flowers, tree branch, tree, and so on...\n",
    "\n",
    "In the figure below we represent schematically what happens to input images after going through two different convolution layers, the convolution filters take a collection of pixels from the input image to form a single output. The red square on the leftmost image represents the information that has gone into forming the output square on the rightmost image.\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/convolution_output.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhNazM6nvH6n"
   },
   "source": [
    "## Ressources üìöüìö\n",
    "\n",
    "* <a href=\"https://cs231n.github.io/convolutional-networks/\">A lecture from Stanford that can give another perspective on CNN</a>\n",
    "\n",
    "* <a href=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/references/M08/deep+feed+forward+networks.pdf\">A formal math intensive lecture to understand CNN in more depth </a>\n",
    "\n",
    "* <a href=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/references/M08/hints+for+thin+deep+nets.pdf\">Some perspective on how to simplify CNN </a>\n",
    "\n",
    "* <a href=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/references/M08/do+deep+nets+reaaly+need+to+be+deep.pdf\">An insteresting lecture that gives you more in depths knowledge about why deep CNN work better and how you can make shallow CNN work</a>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "01-Understand_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
