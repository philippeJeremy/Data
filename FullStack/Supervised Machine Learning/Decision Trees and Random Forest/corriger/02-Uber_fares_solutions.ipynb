{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb92346-c6cf-47e0-a7d9-5bc509e5c93a",
   "metadata": {},
   "source": [
    "# Uber Fares ðŸš™ðŸš™\n",
    "In this exercise, we'll use Random Forests in order to estimate the price of a Uber ride.\n",
    "\n",
    "## Importing libraries and dataset\n",
    "0. Import the usual libraries and read the dataset from this url:\n",
    "\"https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Machine+Learning+Supervis%C3%A9/Decision+trees/uber.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c474b9b-2cb3-4b74-a905-479eacf0e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) # to avoid deprecation warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# setting Jedha color palette as default\n",
    "pio.templates[\"jedha\"] = go.layout.Template(\n",
    "    layout_colorway=[\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    ")\n",
    "pio.templates.default = \"jedha\"\n",
    "pio.renderers.default = \"svg\" # to be replaced by \"iframe\" if working on JULIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba06a183-f0f8-4056-bea5-dbe4fb8170f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = pd.read_csv(\"https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Machine+Learning+Supervis%C3%A9/Decision+trees/uber.csv\")\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f66efd-b8e1-4fcb-9d4b-de46057fd269",
   "metadata": {},
   "source": [
    "## Basic exploring and cleaning\n",
    "1. Display basic statistics about the dataset. Do you notice some inconsistent values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a052614-58b2-4689-99ca-429450d0a64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 20000\n",
      "\n",
      "Display of dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48462598</td>\n",
       "      <td>2015-05-07 10:24:44.0000004</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2015-05-07 10:24:44 UTC</td>\n",
       "      <td>-73.971664</td>\n",
       "      <td>40.797035</td>\n",
       "      <td>-73.958939</td>\n",
       "      <td>40.777649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6637611</td>\n",
       "      <td>2014-07-09 09:14:04.0000002</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2014-07-09 09:14:04 UTC</td>\n",
       "      <td>-73.991635</td>\n",
       "      <td>40.749855</td>\n",
       "      <td>-73.988250</td>\n",
       "      <td>40.741341</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8357193</td>\n",
       "      <td>2013-11-11 18:51:00.000000240</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2013-11-11 18:51:00 UTC</td>\n",
       "      <td>-73.982352</td>\n",
       "      <td>40.777042</td>\n",
       "      <td>-73.995912</td>\n",
       "      <td>40.759757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40466112</td>\n",
       "      <td>2014-05-22 01:54:00.00000069</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2014-05-22 01:54:00 UTC</td>\n",
       "      <td>-73.991455</td>\n",
       "      <td>40.751700</td>\n",
       "      <td>-73.936357</td>\n",
       "      <td>40.812327</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35405035</td>\n",
       "      <td>2011-06-21 23:37:33.0000002</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2011-06-21 23:37:33 UTC</td>\n",
       "      <td>-73.974749</td>\n",
       "      <td>40.756255</td>\n",
       "      <td>-73.952276</td>\n",
       "      <td>40.778332</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                            key  fare_amount  \\\n",
       "0    48462598    2015-05-07 10:24:44.0000004         13.0   \n",
       "1     6637611    2014-07-09 09:14:04.0000002          5.5   \n",
       "2     8357193  2013-11-11 18:51:00.000000240          8.5   \n",
       "3    40466112   2014-05-22 01:54:00.00000069         19.0   \n",
       "4    35405035    2011-06-21 23:37:33.0000002          7.7   \n",
       "\n",
       "           pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0  2015-05-07 10:24:44 UTC        -73.971664        40.797035   \n",
       "1  2014-07-09 09:14:04 UTC        -73.991635        40.749855   \n",
       "2  2013-11-11 18:51:00 UTC        -73.982352        40.777042   \n",
       "3  2014-05-22 01:54:00 UTC        -73.991455        40.751700   \n",
       "4  2011-06-21 23:37:33 UTC        -73.974749        40.756255   \n",
       "\n",
       "   dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0         -73.958939         40.777649                1  \n",
       "1         -73.988250         40.741341                2  \n",
       "2         -73.995912         40.759757                1  \n",
       "3         -73.936357         40.812327                1  \n",
       "4         -73.952276         40.778332                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basics statistics: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-05-07 10:24:44.0000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-08-28 14:03:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.767949e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.35815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-72.490431</td>\n",
       "      <td>39.918498</td>\n",
       "      <td>-72.459891</td>\n",
       "      <td>39.923345</td>\n",
       "      <td>1.690150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.601123e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.89199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.461597</td>\n",
       "      <td>6.051561</td>\n",
       "      <td>10.564266</td>\n",
       "      <td>6.901520</td>\n",
       "      <td>1.311384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.949000e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-23.70000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-75.419276</td>\n",
       "      <td>-74.006190</td>\n",
       "      <td>-75.423067</td>\n",
       "      <td>-73.991765</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.383476e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.992075</td>\n",
       "      <td>40.734733</td>\n",
       "      <td>-73.991423</td>\n",
       "      <td>40.734105</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.769724e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.981904</td>\n",
       "      <td>40.752554</td>\n",
       "      <td>-73.980305</td>\n",
       "      <td>40.752997</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.148082e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.967229</td>\n",
       "      <td>40.767075</td>\n",
       "      <td>-73.963509</td>\n",
       "      <td>40.768348</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.541894e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.803672</td>\n",
       "      <td>41.366138</td>\n",
       "      <td>40.831932</td>\n",
       "      <td>493.533332</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0                          key  fare_amount  \\\n",
       "count   2.000000e+04                        20000  20000.00000   \n",
       "unique           NaN                        20000          NaN   \n",
       "top              NaN  2015-05-07 10:24:44.0000004          NaN   \n",
       "freq             NaN                            1          NaN   \n",
       "mean    2.767949e+07                          NaN     11.35815   \n",
       "std     1.601123e+07                          NaN      9.89199   \n",
       "min     3.949000e+03                          NaN    -23.70000   \n",
       "25%     1.383476e+07                          NaN      6.00000   \n",
       "50%     2.769724e+07                          NaN      8.50000   \n",
       "75%     4.148082e+07                          NaN     12.50000   \n",
       "max     5.541894e+07                          NaN    220.00000   \n",
       "\n",
       "                pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "count                     20000      20000.000000     20000.000000   \n",
       "unique                    19967               NaN              NaN   \n",
       "top     2012-08-28 14:03:00 UTC               NaN              NaN   \n",
       "freq                          2               NaN              NaN   \n",
       "mean                        NaN        -72.490431        39.918498   \n",
       "std                         NaN         10.461597         6.051561   \n",
       "min                         NaN        -75.419276       -74.006190   \n",
       "25%                         NaN        -73.992075        40.734733   \n",
       "50%                         NaN        -73.981904        40.752554   \n",
       "75%                         NaN        -73.967229        40.767075   \n",
       "max                         NaN         40.803672        41.366138   \n",
       "\n",
       "        dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "count        20000.000000      20000.000000     20000.000000  \n",
       "unique                NaN               NaN              NaN  \n",
       "top                   NaN               NaN              NaN  \n",
       "freq                  NaN               NaN              NaN  \n",
       "mean           -72.459891         39.923345         1.690150  \n",
       "std             10.564266          6.901520         1.311384  \n",
       "min            -75.423067        -73.991765         0.000000  \n",
       "25%            -73.991423         40.734105         1.000000  \n",
       "50%            -73.980305         40.752997         1.000000  \n",
       "75%            -73.963509         40.768348         2.000000  \n",
       "max             40.831932        493.533332         6.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of missing values: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0           0.0\n",
       "key                  0.0\n",
       "fare_amount          0.0\n",
       "pickup_datetime      0.0\n",
       "pickup_longitude     0.0\n",
       "pickup_latitude      0.0\n",
       "dropoff_longitude    0.0\n",
       "dropoff_latitude     0.0\n",
       "passenger_count      0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic stats\n",
    "print(\"Number of rows : {}\".format(dataset.shape[0]))\n",
    "print()\n",
    "\n",
    "print(\"Display of dataset: \")\n",
    "display(dataset.head())\n",
    "print()\n",
    "\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = dataset.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n",
    "\n",
    "print(\"Percentage of missing values: \")\n",
    "display(100*dataset.isnull().sum()/dataset.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5121e5-ff44-462c-a1b1-8405260436dc",
   "metadata": {},
   "source": [
    "**There are some negative values in the fare_amount column!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b00f1-2df3-494c-91fe-b44f44618ef3",
   "metadata": {},
   "source": [
    "2. Drop the useless columns and the rows containing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e430eaac-6661-486c-bc33-15b959ac1def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping two first columns...\n",
      "Done.\n",
      "\n",
      "Dropping rows with outliers in fare_amount: \n",
      "...Done.\n",
      "Number of rows:  19998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19998.000000</td>\n",
       "      <td>19998</td>\n",
       "      <td>19998.000000</td>\n",
       "      <td>19998.000000</td>\n",
       "      <td>19998.000000</td>\n",
       "      <td>19998.000000</td>\n",
       "      <td>19998.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-07-14 17:11:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.360472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-72.490283</td>\n",
       "      <td>39.918414</td>\n",
       "      <td>-72.459736</td>\n",
       "      <td>39.923259</td>\n",
       "      <td>1.690069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.889051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.462110</td>\n",
       "      <td>6.051858</td>\n",
       "      <td>10.564783</td>\n",
       "      <td>6.901859</td>\n",
       "      <td>1.311415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-75.419276</td>\n",
       "      <td>-74.006190</td>\n",
       "      <td>-75.423067</td>\n",
       "      <td>-73.991765</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.992074</td>\n",
       "      <td>40.734729</td>\n",
       "      <td>-73.991422</td>\n",
       "      <td>40.734105</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.981904</td>\n",
       "      <td>40.752551</td>\n",
       "      <td>-73.980302</td>\n",
       "      <td>40.752997</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.967234</td>\n",
       "      <td>40.767075</td>\n",
       "      <td>-73.963506</td>\n",
       "      <td>40.768348</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.803672</td>\n",
       "      <td>41.366138</td>\n",
       "      <td>40.831932</td>\n",
       "      <td>493.533332</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fare_amount          pickup_datetime  pickup_longitude  \\\n",
       "count   19998.000000                    19998      19998.000000   \n",
       "unique           NaN                    19965               NaN   \n",
       "top              NaN  2009-07-14 17:11:00 UTC               NaN   \n",
       "freq             NaN                        2               NaN   \n",
       "mean       11.360472                      NaN        -72.490283   \n",
       "std         9.889051                      NaN         10.462110   \n",
       "min         0.010000                      NaN        -75.419276   \n",
       "25%         6.000000                      NaN        -73.992074   \n",
       "50%         8.500000                      NaN        -73.981904   \n",
       "75%        12.500000                      NaN        -73.967234   \n",
       "max       220.000000                      NaN         40.803672   \n",
       "\n",
       "        pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "count      19998.000000       19998.000000      19998.000000     19998.000000  \n",
       "unique              NaN                NaN               NaN              NaN  \n",
       "top                 NaN                NaN               NaN              NaN  \n",
       "freq                NaN                NaN               NaN              NaN  \n",
       "mean          39.918414         -72.459736         39.923259         1.690069  \n",
       "std            6.051858          10.564783          6.901859         1.311415  \n",
       "min          -74.006190         -75.423067        -73.991765         0.000000  \n",
       "25%           40.734729         -73.991422         40.734105         1.000000  \n",
       "50%           40.752551         -73.980302         40.752997         1.000000  \n",
       "75%           40.767075         -73.963506         40.768348         2.000000  \n",
       "max           41.366138          40.831932        493.533332         6.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dropping two first columns...')\n",
    "dataset = dataset.drop([\"Unnamed: 0\", \"key\"], axis = 1)\n",
    "print(\"Done.\")\n",
    "print()\n",
    "print(\"Dropping rows with outliers in fare_amount: \")\n",
    "mask = dataset['fare_amount'] > 0\n",
    "dataset = dataset.loc[mask, :]\n",
    "print(\"...Done.\")\n",
    "print(\"Number of rows: \", dataset.shape[0])\n",
    "dataset.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195e822-88aa-4c2e-a19b-ba1ac474200a",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "### Dealing with datetime objects\n",
    "3. Convert the `pickup_datetime` column into datetime format. Use panda's [dt module](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.html) to create the following columns:\n",
    "* Year\n",
    "* Month\n",
    "* Day\n",
    "* Weekday: contains the **name** of the day of week\n",
    "\n",
    "Then, you can drop the column `pickup_datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d990d19-46d8-4ad1-b237-ac57dccce839",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[:, \"pickup_datetime\"] = pd.to_datetime(dataset[\"pickup_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ddd2bb-6872-46aa-87ab-3d4acabb0122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>-73.971664</td>\n",
       "      <td>40.797035</td>\n",
       "      <td>-73.958939</td>\n",
       "      <td>40.777649</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5</td>\n",
       "      <td>-73.991635</td>\n",
       "      <td>40.749855</td>\n",
       "      <td>-73.988250</td>\n",
       "      <td>40.741341</td>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5</td>\n",
       "      <td>-73.982352</td>\n",
       "      <td>40.777042</td>\n",
       "      <td>-73.995912</td>\n",
       "      <td>40.759757</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.0</td>\n",
       "      <td>-73.991455</td>\n",
       "      <td>40.751700</td>\n",
       "      <td>-73.936357</td>\n",
       "      <td>40.812327</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.7</td>\n",
       "      <td>-73.974749</td>\n",
       "      <td>40.756255</td>\n",
       "      <td>-73.952276</td>\n",
       "      <td>40.778332</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0         13.0        -73.971664        40.797035         -73.958939   \n",
       "1          5.5        -73.991635        40.749855         -73.988250   \n",
       "2          8.5        -73.982352        40.777042         -73.995912   \n",
       "3         19.0        -73.991455        40.751700         -73.936357   \n",
       "4          7.7        -73.974749        40.756255         -73.952276   \n",
       "\n",
       "   dropoff_latitude  passenger_count  year  month  day    weekday  \n",
       "0         40.777649                1  2015      5    7   Thursday  \n",
       "1         40.741341                2  2014      7    9  Wednesday  \n",
       "2         40.759757                1  2013     11   11     Monday  \n",
       "3         40.812327                1  2014      5   22   Thursday  \n",
       "4         40.778332                1  2011      6   21    Tuesday  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[:, \"year\"] = dataset[\"pickup_datetime\"].dt.year\n",
    "dataset.loc[:, \"month\"] = dataset[\"pickup_datetime\"].dt.month\n",
    "dataset.loc[:, \"day\"] = dataset[\"pickup_datetime\"].dt.day\n",
    "\n",
    "weekdays_dict = {\n",
    "    0: 'Monday',\n",
    "    1: 'Tuesday',\n",
    "    2: 'Wednesday',\n",
    "    3: 'Thursday',\n",
    "    4: 'Friday',\n",
    "    5: 'Saturday',\n",
    "    6: 'Sunday'\n",
    "}\n",
    "dataset.loc[:, \"weekday\"] = dataset[\"pickup_datetime\"].dt.weekday.map(weekdays_dict)\n",
    "dataset = dataset.drop('pickup_datetime', axis = 1)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76119d-5ad5-4675-8436-347a913044ba",
   "metadata": {},
   "source": [
    "### Haversine formula\n",
    "\n",
    "It would be very interesting to compute the ride distance from the GPS coordinates. [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) allows to do this ðŸ¤“:\n",
    "\n",
    "$$\n",
    "d = 2r \\arcsin \\big(\\sqrt{\\sin^2(\\frac{\\phi_2 - \\phi_1}{2}) + \\cos \\phi_1 \\cos \\phi_2 \\sin^2(\\frac{\\lambda_2 - \\lambda_1}{2})} \\big)\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $d$ is the ride distance in kilometers\n",
    "* $r$ is the Earth's radius in kilometers\n",
    "* $\\phi_1$ is the pickup latitude in radians\n",
    "* $\\phi_2$ is the dropoff latitude in radians\n",
    "* $\\lambda_1$ is the pickup longitude in radians\n",
    "* $\\lambda_2$ is the dropoff longitude in radians\n",
    "\n",
    "We've implemented for you a function that computes this formula for one ride with coordinates `lon_1`, `lon_2`, `lat_1` and `lat_2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1b5f8d-298c-43f1-a43a-11806ae4b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lon_1, lon_2, lat_1, lat_2):\n",
    "    \n",
    "    lon_1, lon_2, lat_1, lat_2 = map(np.radians, [lon_1, lon_2, lat_1, lat_2])  # Convert degrees to Radians\n",
    "    \n",
    "    \n",
    "    diff_lon = lon_2 - lon_1\n",
    "    diff_lat = lat_2 - lat_1\n",
    "    \n",
    "\n",
    "    distance_km = 2*6371*np.arcsin(np.sqrt(np.sin(diff_lat/2.0)**2 + np.cos(lat_1) * np.cos(lat_2) * np.sin(diff_lon/2.0)**2)) # earth radius: 6371km\n",
    "    \n",
    "    return distance_km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cce317-9bbd-4e4f-af00-df34cb0dd90b",
   "metadata": {},
   "source": [
    "4. Apply the `haversine` function to he whole dataset to create a new column `ride_distance`. [This stackoverflow post](https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe?answertab=trending#tab-top) might help you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c92e273-5807-433a-bec1-126ad38d5d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>ride_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>-73.971664</td>\n",
       "      <td>40.797035</td>\n",
       "      <td>-73.958939</td>\n",
       "      <td>40.777649</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2.407225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5</td>\n",
       "      <td>-73.991635</td>\n",
       "      <td>40.749855</td>\n",
       "      <td>-73.988250</td>\n",
       "      <td>40.741341</td>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0.988729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5</td>\n",
       "      <td>-73.982352</td>\n",
       "      <td>40.777042</td>\n",
       "      <td>-73.995912</td>\n",
       "      <td>40.759757</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2.235651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.0</td>\n",
       "      <td>-73.991455</td>\n",
       "      <td>40.751700</td>\n",
       "      <td>-73.936357</td>\n",
       "      <td>40.812327</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>8.183379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.7</td>\n",
       "      <td>-73.974749</td>\n",
       "      <td>40.756255</td>\n",
       "      <td>-73.952276</td>\n",
       "      <td>40.778332</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>3.099698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0         13.0        -73.971664        40.797035         -73.958939   \n",
       "1          5.5        -73.991635        40.749855         -73.988250   \n",
       "2          8.5        -73.982352        40.777042         -73.995912   \n",
       "3         19.0        -73.991455        40.751700         -73.936357   \n",
       "4          7.7        -73.974749        40.756255         -73.952276   \n",
       "\n",
       "   dropoff_latitude  passenger_count  year  month  day    weekday  \\\n",
       "0         40.777649                1  2015      5    7   Thursday   \n",
       "1         40.741341                2  2014      7    9  Wednesday   \n",
       "2         40.759757                1  2013     11   11     Monday   \n",
       "3         40.812327                1  2014      5   22   Thursday   \n",
       "4         40.778332                1  2011      6   21    Tuesday   \n",
       "\n",
       "   ride_distance  \n",
       "0       2.407225  \n",
       "1       0.988729  \n",
       "2       2.235651  \n",
       "3       8.183379  \n",
       "4       3.099698  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[:, 'ride_distance'] = dataset.apply(lambda x: haversine(x['pickup_longitude'], x['dropoff_longitude'], \n",
    "                                                                    x['pickup_latitude'], x['dropoff_latitude']), axis = 1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd11461-0af3-4edd-9ee6-0f39d29913da",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "5. Separate the target from the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e57748-e049-4d72-b5e2-2ec4dbfbec20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "Zn2MYjTjrfBJ",
    "outputId": "720c974a-fdfd-4544-c188-229008b33351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating labels from features...\n",
      "...Done.\n",
      "\n",
      "Y : \n",
      "0    13.0\n",
      "1     5.5\n",
      "2     8.5\n",
      "3    19.0\n",
      "4     7.7\n",
      "Name: fare_amount, dtype: float64\n",
      "\n",
      "X :\n",
      "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "0        -73.971664        40.797035         -73.958939         40.777649   \n",
      "1        -73.991635        40.749855         -73.988250         40.741341   \n",
      "2        -73.982352        40.777042         -73.995912         40.759757   \n",
      "3        -73.991455        40.751700         -73.936357         40.812327   \n",
      "4        -73.974749        40.756255         -73.952276         40.778332   \n",
      "\n",
      "   passenger_count  year  month  day    weekday  ride_distance  \n",
      "0                1  2015      5    7   Thursday       2.407225  \n",
      "1                2  2014      7    9  Wednesday       0.988729  \n",
      "2                1  2013     11   11     Monday       2.235651  \n",
      "3                1  2014      5   22   Thursday       8.183379  \n",
      "4                1  2011      6   21    Tuesday       3.099698  \n"
     ]
    }
   ],
   "source": [
    "# Separate target variable Y from features X\n",
    "print(\"Separating labels from features...\")\n",
    "target_variable = \"fare_amount\"\n",
    "\n",
    "X = dataset.drop(target_variable, axis = 1)\n",
    "Y = dataset.loc[:,target_variable]\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()\n",
    "\n",
    "print('Y : ')\n",
    "print(Y.head())\n",
    "print()\n",
    "print('X :')\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485279af-30bc-4564-bb9a-96ac745f04cb",
   "metadata": {},
   "source": [
    "6. Detect names of numeric/categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e988338-9510-41d4-ba25-f9f5892b3ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found numeric features  ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'year', 'month', 'day', 'ride_distance']\n",
      "Found categorical features  ['weekday']\n"
     ]
    }
   ],
   "source": [
    "# Automatically detect names of numeric/categorical columns\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "for i,t in X.dtypes.iteritems():\n",
    "    if ('float' in str(t)) or ('int' in str(t)) :\n",
    "        numeric_features.append(i)\n",
    "    else :\n",
    "        categorical_features.append(i)\n",
    "\n",
    "print('Found numeric features ', numeric_features)\n",
    "print('Found categorical features ', categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf0cc3-198b-4700-b306-5dc8da5fa444",
   "metadata": {},
   "source": [
    "7. Make a train/test splitting with test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2423c92-0e22-4c6e-bda9-82da2a34eb59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Ra8LgBS5s83i",
    "outputId": "6dd938b3-ca4b-4ddb-c55e-f020aed4eca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing into train and test sets...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "print(\"Dividing into train and test sets...\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6af84-3cf7-479b-a1a3-aa13399f37dd",
   "metadata": {},
   "source": [
    "8. Make all the necessary preprocessings.\n",
    "\n",
    "Hint: in this exercise, we'll first create a baseline model with a multivariate **linear regression**. So don't forget to make all the transformations that are required for this kind of model ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a49eab4f-37d4-41ca-a057-2c0ce3292cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for numeric features\n",
    "numeric_transformer = StandardScaler() # Need to standardize features because we'll first use a linear regression as baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "069d40b6-5c1c-4041-82b3-e3f9ccd03ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for categorical features\n",
    "categorical_transformer = OneHotEncoder(drop='first') # no missing values in categorical data, so we only need the OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21c8b7c0-ba07-4b2d-bbb1-870d7e2391e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ColumnTransformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb3bfe24-cd39-4e6f-880c-9d58656f5bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing preprocessings on train set...\n",
      "       pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "8152         -73.966328        40.757692         -73.958325         40.768067   \n",
      "1380         -73.988100        40.764807         -74.001052         40.746947   \n",
      "14079        -74.003445        40.743692         -74.005457         40.738923   \n",
      "7725         -73.961230        40.760852         -73.957473         40.722320   \n",
      "14918        -73.966034        40.767998         -73.954902         40.783116   \n",
      "\n",
      "       passenger_count  year  month  day    weekday  ride_distance  \n",
      "8152                 1  2012      2   16   Thursday       1.336116  \n",
      "1380                 1  2013     10   23  Wednesday       2.265861  \n",
      "14079                1  2009      5   22     Friday       0.556722  \n",
      "7725                 1  2013     11   10     Sunday       4.296238  \n",
      "14918                1  2014      4    2  Wednesday       1.924727  \n",
      "...Done.\n",
      "[[-0.14344502  0.14080045 -0.14430518  0.12085588 -0.52674365  0.14178757\n",
      "  -1.22949054  0.03136581 -0.04966598  0.          0.          0.\n",
      "   1.          0.          0.        ]\n",
      " [-0.14549195  0.14195277 -0.14828083  0.11791826 -0.52674365  0.68019666\n",
      "   1.08407898  0.84070488 -0.0471955   0.          0.          0.\n",
      "   0.          0.          1.        ]\n",
      " [-0.14693462  0.13853306 -0.1486907   0.11680219 -0.52674365 -1.47343971\n",
      "  -0.36190197  0.72508501 -0.05173695  0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [-0.14296573  0.14131223 -0.1442259   0.11449285 -0.52674365  0.68019666\n",
      "   1.37327517 -0.66235339 -0.04180048  0.          0.          1.\n",
      "   0.          0.          0.        ]\n",
      " [-0.14341738  0.14246957 -0.14398668  0.12294908 -0.52674365  1.21860575\n",
      "  -0.65109816 -1.58731233 -0.04810195  0.          0.          0.\n",
      "   0.          0.          1.        ]]\n",
      "\n",
      "Performing preprocessings on test set...\n",
      "       pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "5159         -73.983856        40.725628         -73.947098         40.783363   \n",
      "11184        -73.997200        40.762907         -73.978555         40.766582   \n",
      "16643        -73.974023        40.789274         -73.974023         40.789274   \n",
      "6806         -73.970758        40.761757         -74.008368         40.745677   \n",
      "16647        -74.002724        40.744602         -74.004112         40.737572   \n",
      "\n",
      "       passenger_count  year  month  day    weekday  ride_distance  \n",
      "5159                 1  2015      5    6  Wednesday       7.127522  \n",
      "11184                1  2013      8   14  Wednesday       1.622559  \n",
      "16643                1  2009      4   30   Thursday       0.000000  \n",
      "6806                 2  2012      6   11     Monday       3.637741  \n",
      "16647                3  2015      4    3     Friday       0.790459  \n",
      "...Done.\n",
      "[[-0.14509296  0.13560747 -0.14326052  0.12298348 -0.52674365  1.75701484\n",
      "  -0.36190197 -1.12483286 -0.03427732  0.          0.          0.\n",
      "   0.          0.          1.        ]\n",
      " [-0.14634749  0.14164505 -0.14618753  0.12064933 -0.52674365  0.68019666\n",
      "   0.5056866  -0.19987392 -0.04890485  0.          0.          0.\n",
      "   0.          0.          1.        ]\n",
      " [-0.14416848  0.14591535 -0.14576584  0.12380561 -0.52674365 -1.47343971\n",
      "  -0.65109816  1.65004395 -0.05321624  0.          0.          0.\n",
      "   1.          0.          0.        ]\n",
      " [-0.14386152  0.1414588  -0.14896156  0.11774162  0.23407658  0.14178757\n",
      "  -0.07270578 -0.54673353 -0.0435502   1.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [-0.14686681  0.13868048 -0.14856558  0.11661424  0.99489681  1.75701484\n",
      "  -0.65109816 -1.47169246 -0.05111587  0.          0.          0.\n",
      "   0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessings on train set\n",
    "print(\"Performing preprocessings on train set...\")\n",
    "print(X_train.head())\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "print('...Done.')\n",
    "print(X_train[0:5]) # MUST use this syntax because X_train is a numpy array and not a pandas DataFrame anymore\n",
    "print()\n",
    "\n",
    "# Preprocessings on test set\n",
    "print(\"Performing preprocessings on test set...\")\n",
    "print(X_test.head()) \n",
    "X_test = preprocessor.transform(X_test) # Don't fit again !! \n",
    "print('...Done.')\n",
    "print(X_test[0:5,:]) # MUST use this syntax because X_test is a numpy array and not a pandas DataFrame anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1abce6-584e-4d4d-ae24-f7fac8dc9813",
   "metadata": {},
   "source": [
    "## Baseline: Linear Regression\n",
    "9. Train a linear regression model and evaluate its performances. Is it satisfying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f0dc9b3-f587-4a14-ac4c-bd534db31418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n",
      "...Done.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Train model...\")\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, Y_train)\n",
    "print(\"...Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb4cf4bf-8484-4ca5-8fa1-641fd31668d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on training set :  0.02419859579741468\n",
      "R2 score on test set :  0.017058651115981704\n"
     ]
    }
   ],
   "source": [
    "# Print R^2 scores\n",
    "print(\"R2 score on training set : \", regressor.score(X_train, Y_train))\n",
    "print(\"R2 score on test set : \", regressor.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43d6fb-9b36-4f98-9272-21d387df6106",
   "metadata": {},
   "source": [
    "**The baseline is quite disappointing: it doesn't predict anything more than a dummy model!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab759237-02ef-4b63-885d-65d991621343",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "10. Train a Random Forest model with default hyperparameters. Are the performances better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a20536-3b60-48ad-8bbe-7814efa8663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with default hyperparameters...\n",
      "...Done.\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Random Forest with default hyperparameters...\")\n",
    "regressor = RandomForestRegressor() # we must use a regressor here!\n",
    "regressor.fit(X_train, Y_train)\n",
    "print(\"...Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6284029a-a45b-4886-95b2-bbc93f04b077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on training set :  0.9674644284128262\n",
      "R2 score on test set :  0.772221101314326\n"
     ]
    }
   ],
   "source": [
    "# Print R^2 scores\n",
    "print(\"R2 score on training set : \", regressor.score(X_train, Y_train))\n",
    "print(\"R2 score on test set : \", regressor.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7e4a0-219b-42ec-a74a-01019e9a65a6",
   "metadata": {},
   "source": [
    "**This is by far better than the baseline! However we're overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a732fb-e272-46a5-a63a-678f41c05807",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "11. Use grid search to tune the model's hyperparameters. You can try the following values:\n",
    "\n",
    "```\n",
    "params = {\n",
    "    'max_depth': [10, 12, 14],\n",
    "    'min_samples_split': [4, 8],\n",
    "    'n_estimators': [60, 80, 100]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b57615c0-1344-4439-874d-cd1e20c2fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=60; total time=   2.8s\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=60; total time=   2.8s\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=60; total time=   2.8s\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=80; total time=   3.7s\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=80; total time=   3.7s\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=80; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=100; total time=   4.7s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=60; total time=   2.8s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=60; total time=   2.8s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=60; total time=   2.8s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=80; total time=   3.7s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=80; total time=   3.7s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=80; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_split=8, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=10, min_samples_split=8, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=10, min_samples_split=8, n_estimators=100; total time=   4.6s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=60; total time=   3.2s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=60; total time=   3.2s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=60; total time=   3.2s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=80; total time=   4.3s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=80; total time=   4.3s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=80; total time=   4.3s\n",
      "[CV] END max_depth=12, min_samples_split=4, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=12, min_samples_split=4, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=12, min_samples_split=4, n_estimators=100; total time=   5.4s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=60; total time=   3.2s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=60; total time=   3.2s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=60; total time=   3.2s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=80; total time=   4.2s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=80; total time=   4.2s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=80; total time=   4.3s\n",
      "[CV] END max_depth=12, min_samples_split=8, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=12, min_samples_split=8, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=12, min_samples_split=8, n_estimators=100; total time=   5.3s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=60; total time=   3.6s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=60; total time=   3.6s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=60; total time=   3.6s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=80; total time=   4.8s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=80; total time=   4.9s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=80; total time=   4.9s\n",
      "[CV] END max_depth=14, min_samples_split=4, n_estimators=100; total time=   6.1s\n",
      "[CV] END max_depth=14, min_samples_split=4, n_estimators=100; total time=   6.0s\n",
      "[CV] END max_depth=14, min_samples_split=4, n_estimators=100; total time=   6.1s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=60; total time=   3.6s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=60; total time=   3.5s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=60; total time=   3.6s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=80; total time=   4.7s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=80; total time=   4.7s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=80; total time=   4.8s\n",
      "[CV] END max_depth=14, min_samples_split=8, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=14, min_samples_split=8, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=14, min_samples_split=8, n_estimators=100; total time=   5.9s\n",
      "...Done.\n",
      "Best hyperparameters :  {'max_depth': 10, 'min_samples_split': 8, 'n_estimators': 100}\n",
      "Best validation accuracy :  0.7670553013200435\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'max_depth': [10, 12, 14],\n",
    "    'min_samples_split': [4, 8],\n",
    "    'n_estimators': [60, 80, 100]\n",
    "}\n",
    "gridsearch = GridSearchCV(regressor, param_grid = params, cv = 3, verbose = 2)\n",
    "gridsearch.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce953508-2bd9-4aee-bfb8-834bd239cb8a",
   "metadata": {},
   "source": [
    "### Performances\n",
    "12. Display the R2-score and the [mean absolute error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html?highlight=mean%20absolute%20error#sklearn.metrics.mean_absolute_error) on train set and test set. What do you think of this model? Would it be interesting to use it to estimate the fares on new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b30ca00-a80a-44b2-a411-265618145643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on training set :  0.8773412391126457\n",
      "R2 score on test set :  0.7793227198490924\n",
      "Predictions on training set...\n",
      "...Done.\n",
      "[ 6.17142586  9.3272303   4.84357674 ...  7.81504034 24.47627732\n",
      " 10.67468317]\n",
      "\n",
      "Predictions on test set...\n",
      "...Done.\n",
      "[22.10284768  7.84187096 18.5937745  ...  5.27930482  8.80036975\n",
      "  7.59332401]\n",
      "\n",
      "Mean Absolute Error on training set :  1.8398203933642716\n",
      "Mean Fare on training set :  11.330150643830464\n",
      "\n",
      "Mean Absolute Error on test set :  2.1609093314552403\n",
      "Mean Fare on test set :  11.481740000000038\n",
      "Standard-deviation on test set :  9.631479542206511\n"
     ]
    }
   ],
   "source": [
    "# Print R^2 scores\n",
    "print(\"R2 score on training set : \", gridsearch.score(X_train, Y_train))\n",
    "print(\"R2 score on test set : \", gridsearch.score(X_test, Y_test))\n",
    "\n",
    "# Predictions on training set\n",
    "print(\"Predictions on training set...\")\n",
    "Y_train_pred = gridsearch.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print(Y_train_pred)\n",
    "print()\n",
    "\n",
    "# Predictions on test set\n",
    "print(\"Predictions on test set...\")\n",
    "Y_test_pred = gridsearch.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print(Y_test_pred)\n",
    "print()\n",
    "\n",
    "# Print MAE\n",
    "print(\"Mean Absolute Error on training set : \", mean_absolute_error(Y_train, Y_train_pred))\n",
    "print(\"Mean Fare on training set : \", Y_train.mean())\n",
    "print()\n",
    "print(\"Mean Absolute Error on test set : \", mean_absolute_error(Y_test, Y_test_pred))\n",
    "print(\"Mean Fare on test set : \", Y_test.mean())\n",
    "print(\"Standard-deviation on test set : \", Y_test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790368e-5799-487b-ba03-66f6c3d6a217",
   "metadata": {},
   "source": [
    "**If we decided to use this model on new data, the typical error would be about 2.16, which is quite good as the average fare is 11.48 with a standard deviation of 9.63 ðŸ˜Ž**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3162f-c68e-4155-9f52-4ade4b69eb01",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "13. Make a bar plot with the importances of each feature. Are you surprised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3de64c8-3853-4981-8629-b26a4b9fa2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of columns corresponding to each coefficient:  ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'year', 'month', 'day', 'ride_distance', 'weekday_Monday', 'weekday_Saturday', 'weekday_Sunday', 'weekday_Thursday', 'weekday_Tuesday', 'weekday_Wednesday']\n"
     ]
    }
   ],
   "source": [
    "column_names = []\n",
    "for name, step, features_list in preprocessor.transformers_: # loop over steps of ColumnTransformer\n",
    "    if name == 'num': # if pipeline is for numeric variables\n",
    "        features = features_list # just get the names of columns to which it has been applied\n",
    "    else: # if pipeline is for categorical variables\n",
    "        features = step.get_feature_names_out() # get output columns names from OneHotEncoder\n",
    "    column_names.extend(features) # concatenate features names\n",
    "        \n",
    "print(\"Names of columns corresponding to each coefficient: \", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87d9d576-bcc5-48e8-818a-df0242b38dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame\n",
    "feature_importance = pd.DataFrame(index = column_names, data = gridsearch.best_estimator_.feature_importances_, columns=[\"feature_importances\"])\n",
    "feature_importance = feature_importance.sort_values(by = 'feature_importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "324b124d-0151-462a-827c-c04977ba77ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"700\" height=\"500\" style=\"\" viewBox=\"0 0 700 500\"><rect x=\"0\" y=\"0\" width=\"700\" height=\"500\" style=\"fill: rgb(255, 255, 255); fill-opacity: 1;\"/><defs id=\"defs-aee13b\"><g class=\"clips\"><clipPath id=\"clipaee13bxyplot\" class=\"plotclip\"><rect width=\"500\" height=\"360\"/></clipPath><clipPath class=\"axesclip\" id=\"clipaee13bx\"><rect x=\"120\" y=\"0\" width=\"500\" height=\"500\"/></clipPath><clipPath class=\"axesclip\" id=\"clipaee13by\"><rect x=\"0\" y=\"60\" width=\"700\" height=\"360\"/></clipPath><clipPath class=\"axesclip\" id=\"clipaee13bxy\"><rect x=\"120\" y=\"60\" width=\"500\" height=\"360\"/></clipPath></g><g class=\"gradients\"/><g class=\"patterns\"/></defs><g class=\"bglayer\"/><g class=\"layer-below\"><g class=\"imagelayer\"/><g class=\"shapelayer\"/></g><g class=\"cartesianlayer\"><g class=\"subplot xy\"><g class=\"layer-subplot\"><g class=\"shapelayer\"/><g class=\"imagelayer\"/></g><g class=\"gridlayer\"><g class=\"x\"><path class=\"xgrid crisp\" transform=\"translate(230.45999999999998,0)\" d=\"M0,60v360\" style=\"stroke: rgb(238, 238, 238); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(340.93,0)\" d=\"M0,60v360\" style=\"stroke: rgb(238, 238, 238); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(451.39,0)\" d=\"M0,60v360\" style=\"stroke: rgb(238, 238, 238); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(561.86,0)\" d=\"M0,60v360\" style=\"stroke: rgb(238, 238, 238); stroke-opacity: 1; stroke-width: 1px;\"/></g><g class=\"y\"/></g><g class=\"zerolinelayer\"><path class=\"xzl zl crisp\" transform=\"translate(120,0)\" d=\"M0,60v360\" style=\"stroke: rgb(68, 68, 68); stroke-opacity: 1; stroke-width: 1px;\"/></g><path class=\"xlines-below\"/><path class=\"ylines-below\"/><g class=\"overlines-below\"/><g class=\"xaxislayer-below\"/><g class=\"yaxislayer-below\"/><g class=\"overaxes-below\"/><g class=\"plot\" transform=\"translate(120,60)\" clip-path=\"url(#clipaee13bxyplot)\"><g class=\"barlayer mlayer\"><g class=\"trace bars\" style=\"opacity: 1;\"><g class=\"points\"><g class=\"point\"><path d=\"M0,357.6V338.4H0.22V357.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,333.6V314.4H0.35V333.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,309.6V290.4H0.35V309.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,285.6V266.4H0.41V285.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,261.6V242.4H0.62V261.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,237.6V218.4H0.66V237.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,213.6V194.4H0.73V213.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,189.6V170.4H2.27V189.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,165.6V146.4H3.65V165.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,141.6V122.4H5.95V141.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,117.6V98.4H10.28V117.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,93.6V74.4H13.69V93.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,69.6V50.4H14.8V69.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,45.6V26.4H23.36V45.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,21.6V2.4H475V21.6Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(75, 154, 199); fill-opacity: 1;\"/></g></g></g></g></g><g class=\"overplot\"/><path class=\"xlines-above crisp\" d=\"M0,0\" style=\"fill: none;\"/><path class=\"ylines-above crisp\" d=\"M0,0\" style=\"fill: none;\"/><g class=\"overlines-above\"/><g class=\"xaxislayer-above\"><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"433\" transform=\"translate(120,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">0</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"433\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(230.45999999999998,0)\">0.2</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"433\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(340.93,0)\">0.4</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"433\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(451.39,0)\">0.6</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"433\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(561.86,0)\">0.8</text></g></g><g class=\"yaxislayer-above\"><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,408)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">weekday_Saturday</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,384)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">weekday_Tuesday</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,360)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">weekday_Sunday</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,336)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">weekday_Wednesday</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,312)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">weekday_Monday</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,288)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">weekday_Thursday</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,264)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">passenger_count</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,240)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">month</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,216)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">day</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,192)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">pickup_latitude</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,168)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">pickup_longitude</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,144)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">year</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,120)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">dropoff_latitude</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,96)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">dropoff_longitude</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"119\" y=\"4.199999999999999\" transform=\"translate(0,72)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(68, 68, 68); fill-opacity: 1; white-space: pre; opacity: 1;\">ride_distance</text></g></g><g class=\"overaxes-above\"/></g></g><g class=\"polarlayer\"/><g class=\"smithlayer\"/><g class=\"ternarylayer\"/><g class=\"geolayer\"/><g class=\"funnelarealayer\"/><g class=\"pielayer\"/><g class=\"iciclelayer\"/><g class=\"treemaplayer\"/><g class=\"sunburstlayer\"/><g class=\"glimages\"/><defs id=\"topdefs-aee13b\"><g class=\"clips\"/></defs><g class=\"layer-above\"><g class=\"imagelayer\"/><g class=\"shapelayer\"/></g><g class=\"infolayer\"><g class=\"g-gtitle\"/><g class=\"g-xtitle\"><text class=\"xtitle\" x=\"370\" y=\"472\" text-anchor=\"middle\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 14px; fill: rgb(68, 68, 68); opacity: 1; font-weight: normal; white-space: pre;\">value</text></g><g class=\"g-ytitle\" transform=\"translate(-60.078125,0)\"><text class=\"ytitle\" transform=\"rotate(-90,82,240)\" x=\"82\" y=\"240\" text-anchor=\"middle\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 14px; fill: rgb(68, 68, 68); opacity: 1; font-weight: normal; white-space: pre;\">index</text></g></g></svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot coefficients\n",
    "fig = px.bar(feature_importance, orientation = 'h')\n",
    "fig.update_layout(showlegend = False, \n",
    "                  margin = {'l': 120} # to avoid cropping of column names\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1ee35-5e3d-45a3-9264-c532cc5143ee",
   "metadata": {},
   "source": [
    "**To estimate the fare, the ride distance is by far the most important feature! That seems fair ðŸ˜Œ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f456418-343e-490b-98fc-9cba3b9ae71a",
   "metadata": {},
   "source": [
    "14. Would the model be able to make good predictions if we hadn't included the ride distance by hand? Train a new Random Forest model (with grid search) by dropping the `ride_distance` column from the features, and conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4da33e5d-2b42-4326-ad77-2a89ae5022ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ride_distance\n",
    "X = X.drop('ride_distance', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21a338cf-736d-4fd4-910c-b6685db6194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found numeric features  ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'year', 'month', 'day']\n",
      "Found categorical features  ['weekday']\n"
     ]
    }
   ],
   "source": [
    "# Automatically detect names of numeric/categorical columns\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "for i,t in X.dtypes.iteritems():\n",
    "    if ('float' in str(t)) or ('int' in str(t)) :\n",
    "        numeric_features.append(i)\n",
    "    else :\n",
    "        categorical_features.append(i)\n",
    "\n",
    "print('Found numeric features ', numeric_features)\n",
    "print('Found categorical features ', categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "552aa0a2-d250-441b-aaa0-1579358fdfb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Ra8LgBS5s83i",
    "outputId": "6dd938b3-ca4b-4ddb-c55e-f020aed4eca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing into train and test sets...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "print(\"Dividing into train and test sets...\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40c3b784-e8c3-4b63-b800-592022641164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for numeric features\n",
    "numeric_transformer = StandardScaler() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d543cc22-2be4-4ca5-8731-8fa5249caf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for categorical features\n",
    "categorical_transformer = OneHotEncoder(drop='first') # no missing values in categorical data, so we only need the OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f10068f-e6d3-4787-aa57-6fd08e049a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ColumnTransformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e363a7a6-2c2c-4529-a075-2cf0c0cc053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing preprocessings on train set...\n",
      "       pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "8152         -73.966328        40.757692         -73.958325         40.768067   \n",
      "1380         -73.988100        40.764807         -74.001052         40.746947   \n",
      "14079        -74.003445        40.743692         -74.005457         40.738923   \n",
      "7725         -73.961230        40.760852         -73.957473         40.722320   \n",
      "14918        -73.966034        40.767998         -73.954902         40.783116   \n",
      "\n",
      "       passenger_count  year  month  day    weekday  \n",
      "8152                 1  2012      2   16   Thursday  \n",
      "1380                 1  2013     10   23  Wednesday  \n",
      "14079                1  2009      5   22     Friday  \n",
      "7725                 1  2013     11   10     Sunday  \n",
      "14918                1  2014      4    2  Wednesday  \n",
      "...Done.\n",
      "[[-0.14344502  0.14080045 -0.14430518  0.12085588 -0.52674365  0.14178757\n",
      "  -1.22949054  0.03136581  0.          0.          0.          1.\n",
      "   0.          0.        ]\n",
      " [-0.14549195  0.14195277 -0.14828083  0.11791826 -0.52674365  0.68019666\n",
      "   1.08407898  0.84070488  0.          0.          0.          0.\n",
      "   0.          1.        ]\n",
      " [-0.14693462  0.13853306 -0.1486907   0.11680219 -0.52674365 -1.47343971\n",
      "  -0.36190197  0.72508501  0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.14296573  0.14131223 -0.1442259   0.11449285 -0.52674365  0.68019666\n",
      "   1.37327517 -0.66235339  0.          0.          1.          0.\n",
      "   0.          0.        ]\n",
      " [-0.14341738  0.14246957 -0.14398668  0.12294908 -0.52674365  1.21860575\n",
      "  -0.65109816 -1.58731233  0.          0.          0.          0.\n",
      "   0.          1.        ]]\n",
      "\n",
      "Performing preprocessings on test set...\n",
      "       pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "5159         -73.983856        40.725628         -73.947098         40.783363   \n",
      "11184        -73.997200        40.762907         -73.978555         40.766582   \n",
      "16643        -73.974023        40.789274         -73.974023         40.789274   \n",
      "6806         -73.970758        40.761757         -74.008368         40.745677   \n",
      "16647        -74.002724        40.744602         -74.004112         40.737572   \n",
      "\n",
      "       passenger_count  year  month  day    weekday  \n",
      "5159                 1  2015      5    6  Wednesday  \n",
      "11184                1  2013      8   14  Wednesday  \n",
      "16643                1  2009      4   30   Thursday  \n",
      "6806                 2  2012      6   11     Monday  \n",
      "16647                3  2015      4    3     Friday  \n",
      "...Done.\n",
      "[[-0.14509296  0.13560747 -0.14326052  0.12298348 -0.52674365  1.75701484\n",
      "  -0.36190197 -1.12483286  0.          0.          0.          0.\n",
      "   0.          1.        ]\n",
      " [-0.14634749  0.14164505 -0.14618753  0.12064933 -0.52674365  0.68019666\n",
      "   0.5056866  -0.19987392  0.          0.          0.          0.\n",
      "   0.          1.        ]\n",
      " [-0.14416848  0.14591535 -0.14576584  0.12380561 -0.52674365 -1.47343971\n",
      "  -0.65109816  1.65004395  0.          0.          0.          1.\n",
      "   0.          0.        ]\n",
      " [-0.14386152  0.1414588  -0.14896156  0.11774162  0.23407658  0.14178757\n",
      "  -0.07270578 -0.54673353  1.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [-0.14686681  0.13868048 -0.14856558  0.11661424  0.99489681  1.75701484\n",
      "  -0.65109816 -1.47169246  0.          0.          0.          0.\n",
      "   0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessings on train set\n",
    "print(\"Performing preprocessings on train set...\")\n",
    "print(X_train.head())\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "print('...Done.')\n",
    "print(X_train[0:5]) # MUST use this syntax because X_train is a numpy array and not a pandas DataFrame anymore\n",
    "print()\n",
    "\n",
    "# Preprocessings on test set\n",
    "print(\"Performing preprocessings on test set...\")\n",
    "print(X_test.head()) \n",
    "X_test = preprocessor.transform(X_test) # Don't fit again !! \n",
    "print('...Done.')\n",
    "print(X_test[0:5,:]) # MUST use this syntax because X_test is a numpy array and not a pandas DataFrame anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16e553e7-bca2-478f-ab86-ac1c515521b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=60; total time=   2.6s\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=60; total time=   2.5s\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=60; total time=   2.5s\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=80; total time=   3.4s\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=80; total time=   3.4s\n",
      "[CV] END .max_depth=10, min_samples_split=4, n_estimators=80; total time=   3.4s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=100; total time=   4.3s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=60; total time=   2.6s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=60; total time=   2.6s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=60; total time=   2.5s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=80; total time=   3.4s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=80; total time=   3.4s\n",
      "[CV] END .max_depth=10, min_samples_split=8, n_estimators=80; total time=   3.3s\n",
      "[CV] END max_depth=10, min_samples_split=8, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, min_samples_split=8, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=10, min_samples_split=8, n_estimators=100; total time=   4.2s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=60; total time=   2.9s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=60; total time=   2.9s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=60; total time=   3.0s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=80; total time=   4.1s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=80; total time=   3.9s\n",
      "[CV] END .max_depth=12, min_samples_split=4, n_estimators=80; total time=   4.0s\n",
      "[CV] END max_depth=12, min_samples_split=4, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=12, min_samples_split=4, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=12, min_samples_split=4, n_estimators=100; total time=   4.9s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=60; total time=   3.0s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=60; total time=   2.9s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=60; total time=   3.0s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=80; total time=   3.9s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=80; total time=   4.0s\n",
      "[CV] END .max_depth=12, min_samples_split=8, n_estimators=80; total time=   3.9s\n",
      "[CV] END max_depth=12, min_samples_split=8, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=12, min_samples_split=8, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=12, min_samples_split=8, n_estimators=100; total time=   4.9s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=60; total time=   3.4s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=60; total time=   3.4s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=60; total time=   3.4s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=80; total time=   4.5s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=80; total time=   4.4s\n",
      "[CV] END .max_depth=14, min_samples_split=4, n_estimators=80; total time=   4.5s\n",
      "[CV] END max_depth=14, min_samples_split=4, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=14, min_samples_split=4, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=14, min_samples_split=4, n_estimators=100; total time=   5.7s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=60; total time=   3.4s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=60; total time=   3.3s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=60; total time=   3.4s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=80; total time=   4.5s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=80; total time=   4.4s\n",
      "[CV] END .max_depth=14, min_samples_split=8, n_estimators=80; total time=   4.4s\n",
      "[CV] END max_depth=14, min_samples_split=8, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=14, min_samples_split=8, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=14, min_samples_split=8, n_estimators=100; total time=   5.6s\n",
      "...Done.\n",
      "Best hyperparameters :  {'max_depth': 14, 'min_samples_split': 8, 'n_estimators': 100}\n",
      "Best validation accuracy :  0.7098054091539289\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "print(\"Grid search...\")\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Grid of values to be tested\n",
    "params = {\n",
    "    'max_depth': [10, 12, 14],\n",
    "    'min_samples_split': [4, 8],\n",
    "    'n_estimators': [60, 80, 100]\n",
    "}\n",
    "gridsearch = GridSearchCV(regressor, param_grid = params, cv = 3, verbose = 2)\n",
    "gridsearch.fit(X_train, Y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best validation accuracy : \", gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5798b3f-b448-4c72-8cff-a1ef037459da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on training set :  0.850107087831722\n",
      "R2 score on test set :  0.713239229422019\n",
      "Predictions on training set...\n",
      "...Done.\n",
      "[ 7.45782206  8.61356464  7.42620888 ...  7.48197772 25.77105996\n",
      "  8.8216215 ]\n",
      "\n",
      "Predictions on test set...\n",
      "...Done.\n",
      "[14.02542084  8.62151493  7.51974234 ...  7.44094536  7.82347779\n",
      "  8.03063538]\n",
      "\n",
      "Mean Absolute Error on training set :  2.4255949901205986\n",
      "Mean Fare on training set :  11.330150643830464\n",
      "\n",
      "Mean Absolute Error on test set :  2.860988261156146\n",
      "Mean Fare on test set :  11.481740000000038\n",
      "Standard-deviation on test set :  9.631479542206511\n"
     ]
    }
   ],
   "source": [
    "# Print R^2 scores\n",
    "print(\"R2 score on training set : \", gridsearch.score(X_train, Y_train))\n",
    "print(\"R2 score on test set : \", gridsearch.score(X_test, Y_test))\n",
    "\n",
    "# Predictions on training set\n",
    "print(\"Predictions on training set...\")\n",
    "Y_train_pred = gridsearch.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print(Y_train_pred)\n",
    "print()\n",
    "\n",
    "# Predictions on test set\n",
    "print(\"Predictions on test set...\")\n",
    "Y_test_pred = gridsearch.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print(Y_test_pred)\n",
    "print()\n",
    "\n",
    "# Print MAE\n",
    "print(\"Mean Absolute Error on training set : \", mean_absolute_error(Y_train, Y_train_pred))\n",
    "print(\"Mean Fare on training set : \", Y_train.mean())\n",
    "print()\n",
    "print(\"Mean Absolute Error on test set : \", mean_absolute_error(Y_test, Y_test_pred))\n",
    "print(\"Mean Fare on test set : \", Y_test.mean())\n",
    "print(\"Standard-deviation on test set : \", Y_test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38017532-469a-4f2d-8988-fe8bd090dd43",
   "metadata": {},
   "source": [
    "**Without the ride distance explicitely included as an input feature in the model, the performances are still way better than the baseline! This happens thanks to the non-linearity of Random Forest. The model is able to extract from the GPS coordinates some information that is somehow correlated with the ride distance. However, the predictions are not as good as the ones obtained before, which shows the importance of feature engineering, as Random Forest is limited by the type of functions it can approximate.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
