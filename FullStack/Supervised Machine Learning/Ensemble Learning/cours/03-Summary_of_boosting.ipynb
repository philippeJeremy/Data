{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "\n",
    "## The main thing ##\n",
    "\n",
    "\n",
    "## General principle\n",
    "\n",
    "In the literature, it is often said that boosting allows to transform a _weak learner_ into a _strong learner_, i.e. a model adapted to the data into a model that achieves the best possible performance on the data in question.\n",
    "\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "Boosting starts by training the selected model on the training data and stores it in memory. Then it calculates predictions for each observation. For Adaboost, it will then assign weights to each observation according to the prediction errors made. It will therefore give a higher weight to the observations for which an error was made and a lower weight to the one he was able to predict well.  The same model is again applied to the weighted data and kept in memory. This process is repeated several times. The end result is a sequence of models that focus on different groups of observations based on previous performance. The models in question are aggregated to form the final model, which theoretically should perform better on the data than the original model.\n",
    "Another method of boosting, gradient boosting, the sequence of models is trained differently : the first model in the sequences is trained to predict $y$ the target variable, and every subsequent model is trained on the residuals from the last model in the sequence. The final model is the sum of all the elementary models trained in the sequence.\n",
    "\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The essential prerequisite for boosting to work is to find a suitable model for the Machine Learning problem being addressed. The base model used in boosting must be relatively low variance and high bias, but this is just a rule of thumb, boosting algorithms in python are usually implemented using decision trees by default. The boosting can be adjusted by varying the number of models to train and the learning rate (which controls the speed at which the weight of the observations is varied: if it is too low, the boosting will take too long, if it is too high, the behaviour of the models may be chaotic). We often find ourselves limited in terms of the number of models for reasons of computing time; on the other hand, we can find an optimal learning rate by grid search. A reasonnable learning rate should be less than 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}