{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-Code_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4N9XyPIoWHV"
      },
      "source": [
        "# Code Attention\n",
        "\n",
        "The goal of this demo is to teach you how to code an encoder decoder model with attention mechanism!\n",
        "Since this is just a demo we will use generated data, the same generated data we used to demonstrate the encoder decoder. You'll be able to tackle the real problem during the exercise, the goal here is to focus on building the model and the training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CqUIHNG_w-j"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyeOCpH_yRRe"
      },
      "source": [
        "# Import Tensorflow & Pathlib librairies\n",
        "import tensorflow as tf \n",
        "import pathlib \n",
        "import pandas as pd \n",
        "import os\n",
        "import io\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import json\n",
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import array_equal\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZhjrVO_6aw"
      },
      "source": [
        "## Generate data\n",
        "\n",
        "We will generate random input and target data for the purpose of the demonstration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDwgQyLDHzbN"
      },
      "source": [
        "input_dim = 100\n",
        "input_seq_len = 10\n",
        "target_seq_len = 5"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HQ-jiYSAD0D"
      },
      "source": [
        "# generate a sequence of random integers from 2 to n_unique\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(2, n_unique-1) for _ in range(length)]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-N8q1gaBCsy",
        "outputId": "23017cd8-c562-4418-cfed-7357c15e4e16"
      },
      "source": [
        "generate_sequence(input_seq_len,input_dim)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[65, 13, 20, 68, 85, 85, 4, 20, 26, 26]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01pWZiHjAJFV"
      },
      "source": [
        "# prepare data\n",
        "def get_dataset(n_in, n_out, cardinality, n_samples, printing=False):\n",
        "  X1, y = list(), list()\n",
        "  for _ in range(n_samples):\n",
        "    # generate source sequence\n",
        "    source = generate_sequence(n_in, cardinality)\n",
        "    source_pad = source\n",
        "    if printing:\n",
        "      print(\"source:\", source_pad)\n",
        "    # define padded target sequence\n",
        "    # we add the <start> token at the beginning of each sequence\n",
        "    # here we'll simply consider that the start token will coded\n",
        "    # by the index 0\n",
        "    target = source[:n_out]\n",
        "    target.reverse()\n",
        "    target = [0] + target\n",
        "    if printing:\n",
        "      print(\"target:\", target)\n",
        "    # store\n",
        "    X1.append(source_pad)\n",
        "    y.append(target)\n",
        "  return array(X1), array(y)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWkizklpBJpz",
        "outputId": "01993fa9-8560-4df0-d688-b2707b2dacae"
      },
      "source": [
        "input, target =  get_dataset(input_seq_len,target_seq_len,input_dim,1,True)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source: [46, 49, 66, 45, 49, 28, 84, 84, 82, 63]\n",
            "target: [0, 49, 45, 66, 49, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-6_AV5lD-6Y"
      },
      "source": [
        "The data we are generating consists in a random sequence of numbers (they could very well represent encoded letters, words, sentences or anything you could think of).\n",
        "\n",
        "The target is built using the first elements of the input in reversed order. We add a special token at the beginning of every target sequence for teacher.\n",
        "\n",
        "Now that we understand this, let's create the training data and validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeOFZcCeFFGj"
      },
      "source": [
        "X_train, y_train = get_dataset(input_seq_len,target_seq_len,input_dim,10000)\n",
        "X_val, y_val = get_dataset(input_seq_len,target_seq_len,input_dim,5000)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azNpl-pyQgwo"
      },
      "source": [
        "Let's transform these train sets into batch datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSkcX2CUQoHb"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "train_batch = tf.data.Dataset.from_tensor_slices((X_train,y_train)).shuffle(len(X_train)).batch(BATCH_SIZE)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VdBoFXRzYrn"
      },
      "source": [
        "## Create the encoder decoder with attention\n",
        "\n",
        "In what follows we will code a model that will reproduce the following architecture for an encoder decoder model with Bahdanau style attention\n",
        "\n",
        "![bahdanau](https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Deep+Learning/attention/Attention-encoder-decoder.drawio.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPzJHFiSFaeo"
      },
      "source": [
        "### Create encoder model\n",
        "\n",
        "In this step we will define the encoder model.\n",
        "\n",
        "The goal of the encoder is to create a representation of the input data, to extract information from the input data which will then be interpreted by the decoder model.\n",
        "\n",
        "The encoder receives sequence inputs and will output sequences with a given depth of representation (we  usually called that dimension channels before)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D_EKSjfLxd4"
      },
      "source": [
        "# let's start by defining the number of units needed for the embedding and\n",
        "# the lstm layers\n",
        "\n",
        "n_embed = 32\n",
        "n_gru = 32"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kquiEvuTHfYw"
      },
      "source": [
        "class encoder_maker(tf.keras.Model):\n",
        "  def __init__(self, in_vocab_size, embed_dim, n_units):\n",
        "    super().__init__()\n",
        "    # instanciate an embedding layer\n",
        "    self.n_units = n_units\n",
        "    self.embed = tf.keras.layers.Embedding(input_dim=in_vocab_size,\n",
        "                                      output_dim=embed_dim)\n",
        "    # instantiate GRU layer\n",
        "    self.gru = tf.keras.layers.GRU(units=n_units,\n",
        "                              return_sequences=True,\n",
        "                              return_state=True)\n",
        "  def __call__(self, input_batch):\n",
        "    # each output will be saved as a class attribute so we can easily access\n",
        "    # them to control the shapes throughout the demo\n",
        "    self.embed_out = self.embed(input_batch)\n",
        "    self.gru_out, self.gru_state = self.gru(self.embed_out)#, initial_state=initial_state)\n",
        "\n",
        "    return self.gru_out, self.gru_state\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxjlemPeKTt8"
      },
      "source": [
        "That's it, it does not need to be anymore complicated than this, note though that we did not preserve the sequential nature of the data, but we output the cell state, which will serve as input state for the decoder!\n",
        "\n",
        "Let's try it out on an input to see what comes out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpIHpEbDxzHz"
      },
      "source": [
        "encoder = encoder_maker(input_dim, n_embed, n_gru)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkgQs5mXJmHO"
      },
      "source": [
        "encoder_output, encoder_state = encoder(tf.expand_dims(X_train[0],0))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Y4AA5U61cT",
        "outputId": "dd81a47b-64d1-434c-be72-eb2810c77748"
      },
      "source": [
        "encoder_output"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10, 32), dtype=float32, numpy=\n",
              "array([[[-1.63941365e-02,  9.70075652e-03, -1.08925989e-02,\n",
              "          3.06798983e-03, -2.46530259e-03, -3.24438722e-03,\n",
              "          1.00749983e-02,  6.95796916e-04, -3.49409529e-03,\n",
              "         -5.35424566e-03, -4.43874579e-03,  1.00749237e-02,\n",
              "          7.29551772e-03, -6.67085266e-03, -1.57688204e-02,\n",
              "         -7.24462187e-03, -5.81210013e-03, -1.13154585e-02,\n",
              "          3.73269990e-03, -4.04958799e-03, -7.74870813e-03,\n",
              "         -6.87822513e-03,  5.98435709e-03, -2.97807511e-02,\n",
              "          1.00458157e-03,  4.80343215e-03,  7.21000042e-03,\n",
              "          2.84949690e-03,  3.60998698e-03, -1.88084392e-04,\n",
              "         -7.55666243e-03,  1.11027174e-02],\n",
              "        [-1.08593851e-02,  2.59605353e-03, -1.62866805e-02,\n",
              "         -4.22731275e-03, -4.47256025e-05,  1.09004055e-03,\n",
              "         -1.86565565e-03, -2.59361975e-03,  3.89671233e-03,\n",
              "         -1.25934202e-02, -1.30948974e-02, -9.14700236e-03,\n",
              "          2.23703310e-03,  1.16870622e-03,  1.03402119e-02,\n",
              "         -6.73987065e-03,  1.81670338e-02,  4.21081623e-03,\n",
              "          1.80222392e-02,  1.87981175e-03, -2.09577270e-02,\n",
              "          3.12152761e-03, -4.41213092e-03, -1.13909421e-02,\n",
              "          3.04640038e-03,  1.53156836e-03,  9.07066045e-04,\n",
              "          1.01613551e-02,  6.78791571e-03,  6.29417226e-03,\n",
              "         -9.16359667e-03,  2.09533866e-03],\n",
              "        [-1.20202769e-02,  1.78187620e-05, -3.73309106e-03,\n",
              "         -3.66885704e-03,  3.51638463e-03,  1.89546577e-03,\n",
              "         -5.89840207e-03,  6.44017057e-03, -6.84871338e-05,\n",
              "          8.54707323e-05, -1.19383615e-02, -1.43106226e-02,\n",
              "          2.04387382e-02,  3.14973341e-03,  9.22951661e-03,\n",
              "         -1.44252554e-02,  3.09696142e-03,  6.23162510e-03,\n",
              "         -9.39594302e-03,  7.71486759e-03, -7.03662354e-03,\n",
              "          1.48919877e-02,  1.33096497e-03,  9.86083597e-03,\n",
              "          6.97667943e-04, -9.61843762e-04,  3.60897486e-03,\n",
              "         -8.70203599e-04,  3.30176624e-03, -4.11073957e-03,\n",
              "         -2.06842776e-02,  1.50723271e-02],\n",
              "        [-2.37519946e-02, -4.35577193e-03,  5.83064975e-03,\n",
              "          3.76435649e-03, -1.38908299e-02,  1.55501778e-03,\n",
              "          9.26416181e-03,  2.08118465e-02,  1.94410235e-02,\n",
              "         -4.06996161e-03, -7.00363424e-04, -2.25352217e-03,\n",
              "         -3.29032540e-04,  5.62253408e-03, -8.57595820e-04,\n",
              "         -4.48421855e-03, -6.23895042e-03,  8.40260088e-03,\n",
              "         -1.57315023e-02,  1.10547123e-02,  1.74573041e-03,\n",
              "          6.75836485e-03,  1.00817438e-02,  8.13948177e-03,\n",
              "         -9.80552193e-03, -6.03785692e-03,  8.12731124e-03,\n",
              "         -2.50437716e-03, -4.96752281e-03, -1.31423678e-02,\n",
              "         -2.26676632e-02,  1.69501081e-02],\n",
              "        [-9.89819597e-03, -1.65097918e-02,  1.74951449e-03,\n",
              "          5.99170686e-04, -2.07541604e-03, -2.59858393e-03,\n",
              "         -6.22641761e-04,  1.96466595e-02,  3.07557657e-02,\n",
              "         -5.92157710e-04,  7.30924588e-03, -8.98713619e-03,\n",
              "          1.38124721e-02,  2.32854001e-02, -5.29683940e-03,\n",
              "         -8.52172170e-03, -6.14617579e-03,  7.11645838e-03,\n",
              "         -4.19232994e-04,  3.66932643e-03,  7.58758187e-03,\n",
              "          9.50165745e-03, -4.10398562e-03,  1.82770565e-02,\n",
              "         -2.14220956e-02, -6.63972972e-03,  2.34376639e-03,\n",
              "         -1.45706655e-02, -2.14358587e-02, -4.35961271e-03,\n",
              "          8.32489692e-04,  5.67511655e-04],\n",
              "        [-1.38635263e-02,  4.98333946e-04,  8.80684145e-03,\n",
              "         -2.24307342e-03,  6.26134593e-03, -8.20648391e-03,\n",
              "          3.05125979e-03,  3.96182435e-03,  1.23783499e-02,\n",
              "          5.74935786e-03, -1.89996744e-03, -3.49456165e-03,\n",
              "          2.21650489e-02,  4.13210224e-03, -1.16832089e-02,\n",
              "          2.49963235e-02,  3.68491677e-03,  8.98188259e-03,\n",
              "          7.51087442e-03, -1.16115892e-02, -1.42339738e-02,\n",
              "         -8.88710842e-04, -1.40201813e-03,  9.05442052e-04,\n",
              "         -6.76862430e-03,  8.25267285e-03, -1.37672005e-02,\n",
              "         -1.71141466e-03, -1.18837319e-03, -7.52505474e-03,\n",
              "         -1.27812885e-02,  1.36777433e-02],\n",
              "        [-7.62624852e-03,  4.31571330e-04,  1.62503570e-02,\n",
              "         -1.67344278e-03, -1.02041028e-02, -5.58899436e-03,\n",
              "          1.31172445e-02,  1.17020123e-02,  2.65283827e-02,\n",
              "          1.61051024e-02, -1.32034365e-02,  5.20413928e-03,\n",
              "          1.53948963e-02, -1.21916439e-02, -2.08400376e-02,\n",
              "          2.75523178e-02, -1.13899484e-02,  4.11800575e-03,\n",
              "         -2.31371447e-02, -2.30718255e-02,  3.53565766e-03,\n",
              "          3.84314847e-03, -5.35823172e-04,  3.87236057e-03,\n",
              "         -2.07529999e-02, -6.85863197e-04, -7.79657811e-03,\n",
              "          3.03015392e-03, -9.96035431e-03, -6.27887109e-03,\n",
              "          5.79089765e-03,  7.78118614e-04],\n",
              "        [-2.44426932e-02, -2.97788228e-03,  1.21558830e-02,\n",
              "         -7.85692688e-03, -3.59834172e-04, -1.21724512e-02,\n",
              "          1.62242958e-03,  1.57066137e-02, -1.31185409e-02,\n",
              "          4.84894868e-03, -1.98934712e-02,  7.17986887e-03,\n",
              "         -1.82718178e-03, -8.07967316e-03, -1.34565998e-02,\n",
              "          8.60091485e-03,  2.60377908e-03, -3.05884331e-03,\n",
              "         -1.42648350e-02, -7.32341129e-03, -5.00313053e-03,\n",
              "         -1.92675111e-03, -4.99185733e-03,  3.88837652e-03,\n",
              "          4.26848233e-03, -2.20720097e-02, -9.19116475e-03,\n",
              "         -3.79276415e-03, -5.05993655e-03, -1.53015163e-02,\n",
              "          2.44009448e-03, -3.08431825e-03],\n",
              "        [-1.89646948e-02,  1.45330103e-02,  2.43811402e-02,\n",
              "         -1.51595636e-03,  5.38787339e-03,  4.34785616e-03,\n",
              "          6.16399338e-04,  9.42521077e-03, -8.20870791e-03,\n",
              "          7.41188135e-03, -1.43898251e-02, -5.64630469e-03,\n",
              "          1.41244521e-02, -1.08921919e-02,  5.55435615e-03,\n",
              "          1.13352644e-03, -7.77916750e-04,  9.27669648e-03,\n",
              "         -1.73558760e-03, -3.92449833e-03, -7.21581187e-03,\n",
              "         -2.21540499e-03, -1.56346355e-02, -3.22679291e-03,\n",
              "          5.72360586e-03, -4.55621164e-03,  4.96364012e-03,\n",
              "          3.68945068e-03,  1.09290304e-02, -1.33398883e-02,\n",
              "         -1.84669122e-02, -6.41691685e-03],\n",
              "        [-8.52073543e-04,  5.28938044e-03,  2.44472418e-02,\n",
              "         -1.05327684e-02,  5.95644256e-03, -2.55919807e-03,\n",
              "          4.35244525e-03,  2.58204639e-02, -5.02041867e-03,\n",
              "          9.40072350e-03, -1.35830566e-02, -1.17686335e-02,\n",
              "          7.13067409e-03,  1.07246730e-02,  6.00604899e-03,\n",
              "         -5.53970551e-03, -3.64128454e-03,  1.30721442e-02,\n",
              "         -4.30903258e-03,  1.32448645e-03, -6.13171142e-03,\n",
              "         -2.43293848e-02, -3.25846486e-04, -1.56097384e-02,\n",
              "          1.67091861e-02,  1.90784782e-02, -6.13984093e-03,\n",
              "         -2.50900048e-04, -6.97607640e-03, -1.44487470e-02,\n",
              "         -7.29234423e-03,  1.35021331e-02]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrrjGUr263yS",
        "outputId": "9e5bf764-13ae-44a3-a3b1-cfccfd2f0437"
      },
      "source": [
        "encoder_state"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
              "array([[-0.00085207,  0.00528938,  0.02444724, -0.01053277,  0.00595644,\n",
              "        -0.0025592 ,  0.00435245,  0.02582046, -0.00502042,  0.00940072,\n",
              "        -0.01358306, -0.01176863,  0.00713067,  0.01072467,  0.00600605,\n",
              "        -0.00553971, -0.00364128,  0.01307214, -0.00430903,  0.00132449,\n",
              "        -0.00613171, -0.02432938, -0.00032585, -0.01560974,  0.01670919,\n",
              "         0.01907848, -0.00613984, -0.0002509 , -0.00697608, -0.01444875,\n",
              "        -0.00729234,  0.01350213]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhWJAuuGyXOL"
      },
      "source": [
        "The first output as a shape of (1,12,16) which is normal because we applied the encoder to 1 input sequence of 12 elements (we chose return_sequences = True for the gru layer) and 16 channels since we have 16 units on the gru layer.\n",
        "\n",
        "The second output is the gru state which has shape (1,16) for one input sequence and 16 units on the gru layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9sEGN-Ly0Lb"
      },
      "source": [
        "### Create the Attention layer\n",
        "\n",
        "Let's now create the attention layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb-bJzRXz49d"
      },
      "source": [
        "class Bahdanau_attention_maker(tf.keras.layers.Layer):\n",
        "  def __init__(self, attention_units):\n",
        "    super().__init__()\n",
        "\n",
        "    # The attention layer contains three dense layers\n",
        "    self.W1 = tf.keras.layers.Dense(units=attention_units)\n",
        "    self.W2 = tf.keras.layers.Dense(units=attention_units)\n",
        "    self.V = tf.keras.layers.Dense(units=1)\n",
        "\n",
        "  def __call__(self, enc_out, state):\n",
        "    # the choice of name of the arguments here is not random, enc_out\n",
        "    # will represent the encoder output which will be used to create\n",
        "    # the attention weights and then used to create the context vector once we\n",
        "    # apply the attention weights\n",
        "    # the state will be a hidden state from a recurrent unit coming either\n",
        "    # from the encoder at first, and from the decoder as we make further \n",
        "    # predictions\n",
        "    self.W1_out = self.W1(enc_out) # shape (1,12,attention_units)\n",
        "\n",
        "    # If you have taken a close look the model's schema you would have noticed\n",
        "    # that we are going to sum the outputs from W1 and W2, though the shapes\n",
        "    # are incompatible\n",
        "    # the enc_out is (batch_size,12,16) -> W1 -> (batch_size,12,attention_units)\n",
        "    # the state is (batch_size,16) -> W2 -> (batch_size,attention_units)\n",
        "    # thus we need to artificially add a dimension to the stata along axis 1\n",
        "    self.state = tf.expand_dims(state, axis = 1)\n",
        "    self.W2_out = self.W2(self.state) # shape (batch_size,1,attention_units)\n",
        "\n",
        "    self.sum = self.W1_out + self.W2_out  # shape (batch_size,12,attention_units)\n",
        "    self.sum_scale = tf.nn.tanh(self.sum) # shape (batch_size,12,attention_units)\n",
        "\n",
        "    self.score = self.V(self.sum_scale) # shape (batch_size,12,1)\n",
        "\n",
        "    self.attention_weights = tf.nn.softmax(self.score, axis=1) # shape (batch_size,12,1)\n",
        "\n",
        "    self.weighted_enc_out = enc_out * self.attention_weights # shape (batch_size,12,16)\n",
        "\n",
        "    self.context_vector = tf.reduce_sum(self.weighted_enc_out, axis=1) # shape (batch_size,16)\n",
        "\n",
        "    return self.context_vector, self.attention_weights"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U49YxP_s6v11"
      },
      "source": [
        "attention_layer = Bahdanau_attention_maker(8)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfDNyXdQ6_7W",
        "outputId": "af0f949e-7e0d-4eb4-cb66-8966d4586118"
      },
      "source": [
        "attention_layer(encoder_output, encoder_state)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
              " array([[-0.01388177,  0.0008263 ,  0.00609155, -0.00237381, -0.0008466 ,\n",
              "         -0.00252712,  0.00335837,  0.0111282 ,  0.00649133,  0.00201039,\n",
              "         -0.00846961, -0.00334105,  0.01006896,  0.00112595, -0.00367409,\n",
              "          0.00145448, -0.00064366,  0.00469347, -0.00394646, -0.00235454,\n",
              "         -0.00551376,  0.00033716, -0.00133298, -0.00139826, -0.00287167,\n",
              "         -0.00075292, -0.00091411, -0.00043784, -0.00254322, -0.00717109,\n",
              "         -0.00899556,  0.00648828]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 10, 1), dtype=float32, numpy=\n",
              " array([[[0.10051886],\n",
              "         [0.10073073],\n",
              "         [0.10188538],\n",
              "         [0.10225762],\n",
              "         [0.10266352],\n",
              "         [0.10044947],\n",
              "         [0.09940907],\n",
              "         [0.0978561 ],\n",
              "         [0.09729224],\n",
              "         [0.09693695]]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Bd5XPCPrTJ"
      },
      "source": [
        "### Create decoder\n",
        "\n",
        "The goal of the decoder is to use the encoder output and the previous target element to predict the next target element!\n",
        "Which means its output is a sequence with as many elements as the target (this is where the padded target comes in, it will serve as input) and must have a number of channels equals to the number of possible values for target elements.\n",
        "\n",
        "Here we can't use the standard Sequential framework to build the model because the initial state of the decoder as to be set as the encoder states.\n",
        "\n",
        "In addition to this, two versions of the same model (with the same weights) have to be prepared, one of them for training, and one of them for inference (prediction on new unknown data). We'll detail the reason for this in what follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk0-USwT8k1o"
      },
      "source": [
        "class decoder_maker(tf.keras.Model):\n",
        "  def __init__(self, tar_vocab_size, embed_dim, n_units):\n",
        "    super().__init__()\n",
        "    # The decoder contains an embedding layer to play with the teacher forcing\n",
        "    # input, which comes from the target data\n",
        "    # A gru layer\n",
        "    # A dense layer to make the predictions\n",
        "    # And an attention layer\n",
        "    self.embed = tf.keras.layers.Embedding(input_dim=tar_vocab_size, \n",
        "                                    output_dim=embed_dim)\n",
        "    self.gru = tf.keras.layers.GRU(units=n_units, return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.pred = tf.keras.layers.Dense(units=tar_vocab_size,activation=\"softmax\")\n",
        "    self.attention = Bahdanau_attention_maker(attention_units=n_units)\n",
        "\n",
        "  def __call__(self, dec_in, enc_out, state):\n",
        "    # first let's apply the attention layer\n",
        "    self.context_vector, self.attention_weights = self.attention(enc_out,state)\n",
        "\n",
        "    # now the decoder will ingest one sequence element from the teacher forcing\n",
        "    # this will be of shape (bacth_size, 1)\n",
        "    self.embed_out = self.embed(dec_in) # shape (batch_size,1,embed_dim)\n",
        "\n",
        "    # then we need to concatenate the embedding output and the context vector\n",
        "    # though their shapes are incompatible\n",
        "    # embed out (batch_size, 1, embed_dim)\n",
        "    # context vector (batch_size, n_units) where n_units was defined in the encoder\n",
        "    # so we need to add one dimension along axis 1\n",
        "    self.context_vector_expanded = tf.expand_dims(self.context_vector, axis=1)\n",
        "    # shape (batch_size,1,n_units)\n",
        "    self.concat = tf.keras.layers.concatenate([self.embed_out,\n",
        "                                               self.context_vector_expanded])\n",
        "    # shape (bacth_size,1, embed_dim + n_units)\n",
        "    \n",
        "    # now we get to apply the gru layer\n",
        "    self.gru_out, self.gru_state = self.gru(self.concat) \n",
        "    # shapes (batch_size, 1, n_units) and (batch_size, n_units)\n",
        "\n",
        "    # let's reshape the gru output before feeding it to the dense layer\n",
        "    self.gru_out_reshape = tf.reshape(self.gru_out, shape=(-1,\n",
        "                                                           self.gru_out.shape[2]))\n",
        "\n",
        "    # now let's make a prediction\n",
        "    self.pred_out = self.pred(self.gru_out_reshape) # shape (batch_size, 1, tar_vocab_size)\n",
        "\n",
        "    return self.pred_out, self.gru_state, self.attention_weights"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmKLDPIaFvT9"
      },
      "source": [
        "Let's now try and use the decoder using the encoder output, the encoder state and the first element of the teacher forcing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPLQRrgfF49C"
      },
      "source": [
        "decoder = decoder_maker(tar_vocab_size=input_dim, embed_dim=n_embed, n_units=n_gru)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj54hhGHHHeJ"
      },
      "source": [
        "decoder_input = tf.expand_dims(tf.expand_dims(y_train[0][0], axis=0), axis=0) # the teacher forcing is\n",
        "# the first element of the target sequence which corresponds to the <start> token\n",
        "# we use expand dim to artificially add the batch size dimension"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XY5Sz4gJVbJ",
        "outputId": "d09e3052-087d-4928-e079-a9e171f18c9f"
      },
      "source": [
        "decoder_input"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsQTURM5IynC",
        "outputId": "16d875ec-b18c-414d-e882-c38c05ea032f"
      },
      "source": [
        "decoder(decoder_input,encoder_output,encoder_state)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
              " array([[0.0100098 , 0.00999187, 0.01010906, 0.0098903 , 0.01004692,\n",
              "         0.01004892, 0.00997533, 0.01002377, 0.01002064, 0.009948  ,\n",
              "         0.00996668, 0.00998718, 0.01004508, 0.00991621, 0.01001625,\n",
              "         0.01001755, 0.0099857 , 0.00996253, 0.01001414, 0.01004905,\n",
              "         0.01008093, 0.00995257, 0.01008273, 0.00997252, 0.01001269,\n",
              "         0.00998041, 0.00991401, 0.00998602, 0.01005298, 0.01002457,\n",
              "         0.01004699, 0.01004811, 0.00999949, 0.01005429, 0.01000264,\n",
              "         0.00997646, 0.01002604, 0.01000415, 0.01002245, 0.01002534,\n",
              "         0.01002086, 0.01004102, 0.0099708 , 0.01000799, 0.00997931,\n",
              "         0.0099346 , 0.01006611, 0.01005744, 0.01004413, 0.01000439,\n",
              "         0.00995415, 0.01003723, 0.00990904, 0.01002757, 0.00990901,\n",
              "         0.01008085, 0.00995578, 0.00999056, 0.00999638, 0.00993594,\n",
              "         0.01005547, 0.01000185, 0.01005805, 0.00989978, 0.00999695,\n",
              "         0.0100296 , 0.01008035, 0.01002653, 0.00998717, 0.00994343,\n",
              "         0.01001329, 0.00993119, 0.00999362, 0.01005223, 0.00995898,\n",
              "         0.00993111, 0.00998839, 0.00988625, 0.00995054, 0.01003195,\n",
              "         0.00994146, 0.00996227, 0.01003124, 0.0100182 , 0.0099715 ,\n",
              "         0.00997485, 0.01002343, 0.00993852, 0.01005202, 0.00993949,\n",
              "         0.01000439, 0.01001927, 0.0100256 , 0.01003299, 0.00999773,\n",
              "         0.01008141, 0.00998374, 0.00993254, 0.01003435, 0.01000475]],\n",
              "       dtype=float32)>, <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
              " array([[ 0.00013435,  0.01235631,  0.00167277, -0.00260927, -0.00581981,\n",
              "          0.0057932 ,  0.00545633,  0.0051255 , -0.00996244, -0.00975685,\n",
              "         -0.00361065, -0.00072026,  0.00490248, -0.00735054,  0.00397322,\n",
              "          0.00978062,  0.00025757,  0.00666655,  0.00318476,  0.00352036,\n",
              "          0.00103053,  0.01536935,  0.00483811,  0.00931912,  0.00498438,\n",
              "         -0.00645966, -0.0058653 , -0.0010595 , -0.00989136, -0.00240407,\n",
              "         -0.0013282 , -0.00503053]], dtype=float32)>, <tf.Tensor: shape=(1, 10, 1), dtype=float32, numpy=\n",
              " array([[[0.10038333],\n",
              "         [0.1009044 ],\n",
              "         [0.10039373],\n",
              "         [0.0999226 ],\n",
              "         [0.10071298],\n",
              "         [0.10030876],\n",
              "         [0.10003604],\n",
              "         [0.09824154],\n",
              "         [0.09979974],\n",
              "         [0.09929693]]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl3Rm6GPJzwu"
      },
      "source": [
        "Everything worked well, now all there is to do is to apply the decoder again to the second element of the teacher forcing and replacing the encoder state with the decoder state to produce the subsequent predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBbC19IPxGju"
      },
      "source": [
        "## Training the encoder decoder model\n",
        "\n",
        "We are almost there, but contrary to the classic encoder decoder architecture, using attention forces us to manually code the training steps because the encoder output is used for each prediction once weighted by the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X6jktGKyIok"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb4UZsQMS8Lg"
      },
      "source": [
        "import os\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFSjvz22KxsW"
      },
      "source": [
        "def train_step(inp, targ):#, enc_initial_state):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape: # we use the gradient tape to track all\n",
        "  # the different operations happening in the network in order to be able\n",
        "  # to compute the gradients later\n",
        "\n",
        "    enc_output, enc_state = encoder(inp)#,enc_initial_state) # the input sequence is fed to the \n",
        "    # encoder to produce the encoder output and the encoder state\n",
        "\n",
        "    dec_state = enc_state # the initial state used in the decoder is the encoder\n",
        "    # state\n",
        "\n",
        "    dec_input = tf.expand_dims(targ[:,0], axis=1) # the first decoder input\n",
        "    # is the first sequence element of the target batch, which in our case\n",
        "    # represents the <start> token for each sequence in the batch. This is\n",
        "    # what we call the teacher forcing!\n",
        "\n",
        "    # Everything is set up for the first step, now we need to loop over the\n",
        "    # teacher forcing sequence to produce the predictions, we already have \n",
        "    # defined the first step (element 0) so we will loop from 1 to targ.shape[1]\n",
        "    # which is the target sequence length\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing dec_input, dec_state and enc_output to the decoder\n",
        "      # in order to produce the prediction, the new state, and the attention\n",
        "      # weights which we will not need explicitely here\n",
        "      pred, dec_state, _ = decoder(dec_input, enc_output, dec_state)\n",
        "\n",
        "      loss += loss_function(targ[:, t], pred) # we compare the prediction\n",
        "      # produced by teacher forcing with the next element of the target and\n",
        "      # increment the loss\n",
        "\n",
        "      # The new decoder input becomes the next element of the target sequence\n",
        "      # which we just attempted to predict (teacher forcing)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1])) # we divide the loss by the target\n",
        "  # sequence's length to get the average loss across the sequence\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables # here\n",
        "  # we concatenate the lists of trainable variables for the encoder and the\n",
        "  # decoder\n",
        "\n",
        "  gradients = tape.gradient(loss, variables) # compute the gradient based on the\n",
        "  # loss and the trainable variables\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables)) # then update the model's\n",
        "  # parameters\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_829K5bKQS6z",
        "outputId": "4854c9ea-5789-41fe-c40d-17666dafab32"
      },
      "source": [
        "import time\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_batch):\n",
        "    batch_loss = train_step(inp, targ)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  \n",
        "  # saving (checkpoint) the model every epoch\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss))\n",
        "  print('Time taken for 1 epoch {} sec'.format(time.time() - start))\n",
        "\n",
        "  enc_input = X_val\n",
        "  #classic encoder input\n",
        "\n",
        "  dec_input = tf.zeros(shape=(len(X_val),1))\n",
        "  # the first decoder input is the special token 0\n",
        "\n",
        "  enc_out, enc_state = encoder(enc_input)#, initial_state)\n",
        "  # we compute once and for all the encoder output and the encoder\n",
        "  # h state and c state\n",
        "\n",
        "  dec_state = enc_state\n",
        "  # The encoder h state and c state will serve as initial states for the\n",
        "  # decoder\n",
        "\n",
        "  pred = []  # we'll store the predictions in here\n",
        "\n",
        "  # we loop over the expected length of the target, but actually the loop can run\n",
        "  # for as many steps as we wish, which is the advantage of the encoder decoder\n",
        "  # architecture\n",
        "  for i in range(y_val.shape[1]-1):\n",
        "    dec_out, dec_state, attention_w = decoder(dec_input, enc_out, dec_state)\n",
        "    # the decoder state is updated and we get the first prediction probability \n",
        "    # vector\n",
        "    decoded_out = tf.expand_dims(tf.argmax(dec_out, axis=-1), axis=1)\n",
        "    # we decode the softmax vector into and index\n",
        "    pred.append(tf.expand_dims(dec_out,axis=1)) # update the prediction list\n",
        "    dec_input = decoded_out # the previous pred will be used as the new input\n",
        "\n",
        "  pred = tf.concat(pred, axis=1).numpy()\n",
        "  print(\"\\n val loss :\", loss_function(y_val[:,1:],pred),\"\\n\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 3.8377\n",
            "Epoch 1 Batch 10 Loss 3.8359\n",
            "Epoch 1 Batch 20 Loss 3.8363\n",
            "Epoch 1 Batch 30 Loss 3.8337\n",
            "Epoch 1 Batch 40 Loss 3.8321\n",
            "Epoch 1 Batch 50 Loss 3.8298\n",
            "Epoch 1 Batch 60 Loss 3.8225\n",
            "Epoch 1 Batch 70 Loss 3.8067\n",
            "Epoch 1 Loss 302.2648\n",
            "Time taken for 1 epoch 12.676093101501465 sec\n",
            "\n",
            " val loss : tf.Tensor(4.5288196, shape=(), dtype=float32) \n",
            "\n",
            "Epoch 2 Batch 0 Loss 3.7625\n",
            "Epoch 2 Batch 10 Loss 3.7373\n",
            "Epoch 2 Batch 20 Loss 3.6997\n",
            "Epoch 2 Batch 30 Loss 3.6930\n",
            "Epoch 2 Batch 40 Loss 3.6447\n",
            "Epoch 2 Batch 50 Loss 3.6297\n",
            "Epoch 2 Batch 60 Loss 3.6223\n",
            "Epoch 2 Batch 70 Loss 3.6046\n",
            "Epoch 2 Loss 289.9551\n",
            "Time taken for 1 epoch 20.49025797843933 sec\n",
            "\n",
            " val loss : tf.Tensor(4.3099375, shape=(), dtype=float32) \n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.5790\n",
            "Epoch 3 Batch 10 Loss 3.5631\n",
            "Epoch 3 Batch 20 Loss 3.5155\n",
            "Epoch 3 Batch 30 Loss 3.4761\n",
            "Epoch 3 Batch 40 Loss 3.4861\n",
            "Epoch 3 Batch 50 Loss 3.4497\n",
            "Epoch 3 Batch 60 Loss 3.4529\n",
            "Epoch 3 Batch 70 Loss 3.4275\n",
            "Epoch 3 Loss 275.5563\n",
            "Time taken for 1 epoch 20.501012563705444 sec\n",
            "\n",
            " val loss : tf.Tensor(4.10917, shape=(), dtype=float32) \n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.3918\n",
            "Epoch 4 Batch 10 Loss 3.3193\n",
            "Epoch 4 Batch 20 Loss 3.3106\n",
            "Epoch 4 Batch 30 Loss 3.2123\n",
            "Epoch 4 Batch 40 Loss 3.1753\n",
            "Epoch 4 Batch 50 Loss 3.0696\n",
            "Epoch 4 Batch 60 Loss 2.9234\n",
            "Epoch 4 Batch 70 Loss 2.7937\n",
            "Epoch 4 Loss 245.9218\n",
            "Time taken for 1 epoch 20.499784469604492 sec\n",
            "\n",
            " val loss : tf.Tensor(3.3044991, shape=(), dtype=float32) \n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.6335\n",
            "Epoch 5 Batch 10 Loss 2.4622\n",
            "Epoch 5 Batch 20 Loss 2.2505\n",
            "Epoch 5 Batch 30 Loss 2.0542\n",
            "Epoch 5 Batch 40 Loss 1.8375\n",
            "Epoch 5 Batch 50 Loss 1.6811\n",
            "Epoch 5 Batch 60 Loss 1.5188\n",
            "Epoch 5 Batch 70 Loss 1.4006\n",
            "Epoch 5 Loss 150.8954\n",
            "Time taken for 1 epoch 20.490219116210938 sec\n",
            "\n",
            " val loss : tf.Tensor(1.5494775, shape=(), dtype=float32) \n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.2598\n",
            "Epoch 6 Batch 10 Loss 1.1760\n",
            "Epoch 6 Batch 20 Loss 1.0824\n",
            "Epoch 6 Batch 30 Loss 1.0032\n",
            "Epoch 6 Batch 40 Loss 0.9091\n",
            "Epoch 6 Batch 50 Loss 0.8533\n",
            "Epoch 6 Batch 60 Loss 0.7846\n",
            "Epoch 6 Batch 70 Loss 0.7183\n",
            "Epoch 6 Loss 75.3110\n",
            "Time taken for 1 epoch 20.491679430007935 sec\n",
            "\n",
            " val loss : tf.Tensor(0.8399025, shape=(), dtype=float32) \n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.6805\n",
            "Epoch 7 Batch 10 Loss 0.6196\n",
            "Epoch 7 Batch 20 Loss 0.5723\n",
            "Epoch 7 Batch 30 Loss 0.5443\n",
            "Epoch 7 Batch 40 Loss 0.4952\n",
            "Epoch 7 Batch 50 Loss 0.4632\n",
            "Epoch 7 Batch 60 Loss 0.4251\n",
            "Epoch 7 Batch 70 Loss 0.4028\n",
            "Epoch 7 Loss 40.4778\n",
            "Time taken for 1 epoch 12.655481100082397 sec\n",
            "\n",
            " val loss : tf.Tensor(0.45538533, shape=(), dtype=float32) \n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.3786\n",
            "Epoch 8 Batch 10 Loss 0.3488\n",
            "Epoch 8 Batch 20 Loss 0.3387\n",
            "Epoch 8 Batch 30 Loss 0.3449\n",
            "Epoch 8 Batch 40 Loss 0.3054\n",
            "Epoch 8 Batch 50 Loss 0.2847\n",
            "Epoch 8 Batch 60 Loss 0.2655\n",
            "Epoch 8 Batch 70 Loss 0.2498\n",
            "Epoch 8 Loss 25.1659\n",
            "Time taken for 1 epoch 20.487923860549927 sec\n",
            "\n",
            " val loss : tf.Tensor(0.2873464, shape=(), dtype=float32) \n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2324\n",
            "Epoch 9 Batch 10 Loss 0.2174\n",
            "Epoch 9 Batch 20 Loss 0.2192\n",
            "Epoch 9 Batch 30 Loss 0.2056\n",
            "Epoch 9 Batch 40 Loss 0.1948\n",
            "Epoch 9 Batch 50 Loss 0.1830\n",
            "Epoch 9 Batch 60 Loss 0.2001\n",
            "Epoch 9 Batch 70 Loss 0.1695\n",
            "Epoch 9 Loss 15.6434\n",
            "Time taken for 1 epoch 12.399498224258423 sec\n",
            "\n",
            " val loss : tf.Tensor(0.20269683, shape=(), dtype=float32) \n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1843\n",
            "Epoch 10 Batch 10 Loss 0.2017\n",
            "Epoch 10 Batch 20 Loss 0.6118\n",
            "Epoch 10 Batch 30 Loss 0.1735\n",
            "Epoch 10 Batch 40 Loss 0.1553\n",
            "Epoch 10 Batch 50 Loss 0.1907\n",
            "Epoch 10 Batch 60 Loss 0.1824\n",
            "Epoch 10 Batch 70 Loss 0.1357\n",
            "Epoch 10 Loss 17.4177\n",
            "Time taken for 1 epoch 20.489232778549194 sec\n",
            "\n",
            " val loss : tf.Tensor(0.16092637, shape=(), dtype=float32) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaF316p_02jy"
      },
      "source": [
        "Nice! The training is over, and it looks as though the model performs really well both on train and validation sets!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4C6vHoo1NqW"
      },
      "source": [
        "## Make predictions with the inference model\n",
        "\n",
        "To make predictions on the validation set, we cannot use teacher forcing, the model has to base itself on its own predictions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgymx0a41s-g",
        "outputId": "ad51b7b1-50d5-4986-e6e4-d97898cc5adb"
      },
      "source": [
        "enc_input = X_val\n",
        "#classic encoder input\n",
        "\n",
        "dec_input = tf.zeros(shape=(len(X_val),1))\n",
        "# the first decoder input is the special token 0\n",
        "\n",
        "#initial_state = encoder.state_initializer(len(X_val))\n",
        "\n",
        "enc_out, enc_state = encoder(enc_input)#, initial_state)\n",
        "# we compute once and for all the encoder output and the encoder\n",
        "# h state and c state\n",
        "\n",
        "dec_state = enc_state\n",
        "# The encoder h state and c state will serve as initial states for the\n",
        "# decoder\n",
        "\n",
        "pred = []  # we'll store the predictions in here\n",
        "\n",
        "# we loop over the expected length of the target, but actually the loop can run\n",
        "# for as many steps as we wish, which is the advantage of the encoder decoder\n",
        "# architecture\n",
        "for i in range(y_val.shape[1]-1):\n",
        "  dec_out, dec_state, attention_w = decoder(dec_input, enc_out, dec_state)\n",
        "  # the decoder state is updated and we get the first prediction probability \n",
        "  # vector\n",
        "  decoded_out = tf.expand_dims(tf.argmax(dec_out, axis=-1), axis=1)\n",
        "  # we decode the softmax vector into and index\n",
        "  pred.append(decoded_out) # update the prediction list\n",
        "  dec_input = decoded_out # the previous pred will be used as the new input\n",
        "\n",
        "pred = tf.concat(pred, axis=-1).numpy()\n",
        "for i in range(10):\n",
        "  print(\"pred:\", pred[i,:].tolist())\n",
        "  print(\"true:\", y_val[i,:].tolist()[1:])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred: [51, 91, 60, 65, 68]\n",
            "true: [51, 91, 60, 65, 68]\n",
            "\n",
            "\n",
            "pred: [33, 98, 88, 13, 65]\n",
            "true: [33, 98, 88, 13, 65]\n",
            "\n",
            "\n",
            "pred: [68, 14, 76, 70, 80]\n",
            "true: [68, 14, 76, 70, 80]\n",
            "\n",
            "\n",
            "pred: [50, 69, 60, 37, 32]\n",
            "true: [50, 69, 60, 37, 32]\n",
            "\n",
            "\n",
            "pred: [4, 97, 70, 74, 86]\n",
            "true: [4, 97, 70, 74, 86]\n",
            "\n",
            "\n",
            "pred: [24, 39, 66, 11, 36]\n",
            "true: [24, 39, 66, 11, 36]\n",
            "\n",
            "\n",
            "pred: [48, 88, 55, 11, 72]\n",
            "true: [48, 88, 55, 11, 72]\n",
            "\n",
            "\n",
            "pred: [6, 82, 8, 68, 18]\n",
            "true: [6, 82, 8, 68, 18]\n",
            "\n",
            "\n",
            "pred: [14, 49, 65, 90, 2]\n",
            "true: [14, 49, 65, 90, 2]\n",
            "\n",
            "\n",
            "pred: [41, 55, 19, 9, 43]\n",
            "true: [41, 55, 19, 9, 43]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN9kGOmC3E57"
      },
      "source": [
        "The results do not look so bad, almost perfect actually! This is a clear improvement from the encoder decoder! Attention must be really powerful!\n",
        "\n",
        "The fact that the model reuses the encoder output at each step with different weights is helping the model achieve better predictions in a shorter amount of time (understand epochs).\n",
        "\n",
        "I hope you found this demonstration useful! Now it is time for you to apply what you have learned to a real world automatic translation problem!"
      ]
    }
  ]
}