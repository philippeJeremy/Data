{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Computing Part 2 üóÑÔ∏èüîÄ\n",
    "\n",
    "This short lecture focuses on the high API components of Spark which make it easy to use for various tasks.\n",
    "\n",
    "## What will you learn in this course? üßêüßê\n",
    "\n",
    "In this course we will quickly review the different components of Spark and what they are useful for:\n",
    "\n",
    "* The Spark stack\n",
    "    * Spark Core - the main functionnalities of the framework\n",
    "    * Spark SQL - to handle structured data and run queries\n",
    "    * GraphX - Spark's toolbox for graph data structures\n",
    "    * MLlib - The machine learning toolbox for Spark\n",
    "    * Spark Streaming - An API to handle continuous inflow of data\n",
    "\n",
    "## The Spark Stack ‚ú®‚öôÔ∏è\n",
    "\n",
    "One of Spark's promises is to deliver a unified analytics system. On top of its powerful distributed processing engine (Spark Core), sits a collection of higher-level libraries that all benefit from the improvements of the core library, which are low latency, and lazy execution.\n",
    "\n",
    "*That's true in general, but can suffer from some caveats, in particular Spark Streaming's performances can't rival those of Storm and Flink which are other framework for running streaming jobs.*\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/spark-stack-oreilly-674376df-ecdf-45f2-8ef7-539393568c0e.png\" />\n",
    "\n",
    "Source: Learning Spark (O'Reilly - Holden Karau, Andy Konwinski, Patrick Wendell & Matei Zaharia)\n",
    "\n",
    "### Spark Core üíñ\n",
    "\n",
    "Spark Core is the underlying general execution engine for the Spark platform that all other functionalities are built on top of.\n",
    "\n",
    "It provides many core functionalities such as task dispatching and scheduling, memory management and basic I/O (input/output) functionalities, exposed through an application programming interface.\n",
    "\n",
    "### Spark SQL üî¢\n",
    "\n",
    "Spark module for structured data processing.\n",
    "\n",
    "Spark SQL provides a programming abstraction called DataFrame and can also act as a distributed SQL query engine. DataFrames are the other main data format in Spark. Spark DataFrames are column oriented, they have a data schema which describes the name and type of all the available columns. It allows for easier processing but adds contraints on the cleanliness and structure of the data.\n",
    "\n",
    "Also they're called \"DataFrames\", Spark's DataFrame are quite different from those of pandas that you might be familiar with.\n",
    "\n",
    "At the core of Spark SQL is the Catalyst optimizer, which leverages advanced programming language features (such as Scala‚Äôs pattern matching and quasi quotes) to build an extensible query optimizer.\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/Catalyst-Optimizer-diagram-152974c4-e1fc-4bb5-a788-c1ee71657ecd.png\" />\n",
    "\n",
    "Source: [https://databricks.com/glossary/catalyst-optimizer](https://databricks.com/glossary/catalyst-optimizer)\n",
    "\n",
    "### GraphX üìä\n",
    "\n",
    "Spark module for Graph data structure.\n",
    "\n",
    "GraphX is a graph computation engine built on top of Spark that enables users to interactively build, transform and reason about graph structured data at scale.\n",
    "\n",
    "### MLlib üîÆ\n",
    "\n",
    "Machine Learning library for Spark, inspired by Scikit-Learn (in particular, its pipelines system).\n",
    "\n",
    "Historically a RDD-based API, it now comes with a DataFrame-based API that has become the primary API while the RDD-based API is now in [maintenance mode](https://spark.apache.org/docs/latest/ml-guide.html#announcement-dataframe-based-api-is-primary-api).\n",
    "\n",
    "### Spark streaming üåä\n",
    "\n",
    "Spark module for stream processing.\n",
    "\n",
    "Streaming, also called Stream Processing is used to query continuous data stream and process this data within a small time period from the time of receiving the data. This is the opposite of batch processing, which occurs at a previously scheduled time independently from the data influx.\n",
    "\n",
    "Spark Streaming uses Spark Core's fast scheduling capability to perform streaming analytics. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data. This design enables the same set of application code written for batch analytics to be used in streaming analytics, this comes at the cost of having to wait for the full mini-batch to be processed while alternatives like Apache Storm and Apache Flink process data by event and provide better speed.\n",
    "\n",
    "## Ressources üìöüìö\n",
    "\n",
    "- [What is Spark SQL](https://databricks.com/glossary/what-is-spark-sql)\n",
    "- [Deep dive into Spark SQL's Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)\n",
    "- [SparkSqlAstBuilder](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkSqlAstBuilder.html)\n",
    "- [A Gentle Introduction to Stream Processing](https://medium.com/stream-processing/what-is-stream-processing-1eadfca11b97)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
