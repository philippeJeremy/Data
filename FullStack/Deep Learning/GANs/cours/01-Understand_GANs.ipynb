{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAZncdY2nuX8"
   },
   "source": [
    "# Generative Adversarial Networks (GANs) ‚ú®\n",
    "\n",
    "Generative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks.\n",
    "\n",
    "The general idea and end goal of this technique is to generate objects that are similar to a set example objects as if they were taken from the same population. For example, if you have a collection of images of human faces, GANs would make it possible for you to generate realistic artificial images of human faces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOS4TZoTOBBR"
   },
   "source": [
    "## What will you learn in this course? üßêüßê\n",
    "\n",
    "In this course, you will discover how Generative Adversarial Networks, or GANs work!\n",
    "\n",
    "While reading this lecture, you will learn about:\n",
    "\n",
    "* Generative modeling\n",
    "* Usescases for GANs\n",
    "  * Realistic Image Generation\n",
    "  * Transformation of existing images\n",
    "  * Text-based generated images\n",
    "  * Data Augmentation\n",
    "  * Data Privacy\n",
    "  * Anomaly detection\n",
    "  * Domain adaptation\n",
    "  * Adversarial training\n",
    "* Generator vs. Discriminator model\n",
    "  * Generator\n",
    "    * ConvTranspose\n",
    "  * Discriminator\n",
    "* Trainign GANs\n",
    "  * Discriminator Loss\n",
    "  * Generator Loss\n",
    "  * Training Step\n",
    "* Conclusion\n",
    "* Ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSW_6GxiPHjy"
   },
   "source": [
    "# Generative Modeling ‚õ≤‚õ≤\n",
    "\n",
    "Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.\n",
    "\n",
    "In the introduction, we mentionned an example of artificially generated faces, it is indeed possible to train GANs using a large image bank of human faces and use them to generate pictures of people who do not actually exist! If you're sceptical <a href=\"https://thispersondoesnotexist.com/\"> check this out!</a>\n",
    "\n",
    "The use of generative models is not limited to images, you can use generative models to simulate observations from any distribution! Sounds great right! However, the difficulty and ressources needed to generate realistic candidates for a given distribution vary immensely depending on the problem.\n",
    "\n",
    "For example, as we will see in the demo, it is fairly easy to generate realistic images based on a bank of low resolution images with little variation. But in order to produce HD artificial images, generative models need to be trained for months using super-calculators! This is called the curse of dimensionality, the difficulty of a problem increases exponentially with the number of variables!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjClQKdJWFaz"
   },
   "source": [
    "# Usecases of GANs üõ†Ô∏èüõ†Ô∏è\n",
    "\n",
    "GANs and generative models can be used for various application, here we will attempt to give you a brief yet rich overview of such usecases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFHlM4xSvoqp"
   },
   "source": [
    "## Realistic image generation üñºÔ∏è\n",
    "\n",
    "We have mentionned this one already but it's still one the most well-known applications of GANs, and it has made tremendous progress in the past couple of years so it definitely deserves a shout out!\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-the-Progression-in-the-Capabilities-of-GANs-from-2014-to-2017.webp\" />\n",
    "\n",
    "Artificial image generation is not limited to faces but can also expand to drawings or more generally artworks, landscapes or miscellanous daily life scenes:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-GAN-Generated-Anime-Character-Faces.webp\" />\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-Realistic-Synthetic-Photographs-Generated-with-BigGAN.webp\" />\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Examples-of-GANs-used-to-Generate-New-Plausible-Examples-for-Image-Datasets.webp\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Imddg4ujwj3I"
   },
   "source": [
    "## Transformation of existing images (Image translation) üñºÔ∏è->üñºÔ∏è\n",
    "\n",
    "Image translation refers to any generative technique that bases itself on an original image in order to produce a generated image based on the prior distribution.\n",
    "\n",
    "For example it is possible to generate someone's frontal portrait based on pictures taken from different angles:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-GAN-based-Face-Frontal-View-Photo-Generation.webp\" />\n",
    "\n",
    "We can generate pictures where the subject's pose has changed:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-GAN-Generated-Photographs-of-Human-Poses.webp\" />\n",
    "\n",
    "It is possible to convert daytime pictures into their nighttime equivalent:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-Photographs-of-Daytime-Cityscapes-to-Nighttime-with-pix2pix.webp\" />\n",
    "\n",
    "Other examples of image translation are photograph to painting, horse to zebra or satellite view to schematic map:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-Four-Image-to-Image-Translations-Performed-with-CycleGAN.webp\" />\n",
    "\n",
    "It is also possible to generate images from annotated schema or drawings:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-Semantic-Image-and-GAN-Generated-Cityscape-Photograph.webp\" />\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-Sketches-to-Color-Photographs-with-pix2pix.webp\" />\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-Three-Dimensional-Reconstructions-of-a-Chair-from-Two-Dimensional-Images.webp\" />\n",
    "\n",
    "One last image translation example consists in changing details in an image, like adding smiles or sunglasses:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/image_translation.gif\" />\n",
    "\n",
    "GANs also have the power to complete partially missing data in images:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-GAN-Generated-Photograph-Inpainting-using-Context-Encoders.webp\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUpfNZj2389w"
   },
   "source": [
    "## Text-based generated images üìù->üñºÔ∏è\n",
    "\n",
    "GANs have been used in order to generate images from prior distributions of textual data as opposed to image data, as shown in the examples below:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-Textual-Descriptions-and-GAN-Generated-Photographs-of-Birds.webp\" />\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/Example-of-Photos-of-Object-Generated-from-Text-and-Position-Hints-with-a-GAN.webp\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5-Jza286ThC"
   },
   "source": [
    "## Data Augmentation üñºÔ∏è->üñºÔ∏èüñºÔ∏èüñºÔ∏è\n",
    "\n",
    "The data generation capabilities of GANs may be used for broader applications than strictly making beautiful pictures. For example, they may proove useful for building complex data augmentation pipelines. Properly trained GANs make it possible for data scientist to produce a vurtually infinite variety of examples from the original population, and therefore may simplify data collection in situations where data is costly or hard to get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7Xq4Tpb8OIO"
   },
   "source": [
    "## Data Privacy and anonymisation üòé\n",
    "\n",
    "In the context of increasing regulation on data usage, specifically in the european union, privacy and anonymisation issues are becoming very pressing in the financial or medical sector for example. Situation where parties would need to share their data to other entities for analysis are common, and GANs could play a role in maintaining data privacy.\n",
    "\n",
    "Indeed, GANs could be used to generate let's say user data that would be congruent with the true distribution while being entirely artificial and disconnected from actual customer private information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hoQsYQM9WOX"
   },
   "source": [
    "## Anomaly detection üîµüîµüî¥üîµüîµüîµüîµ\n",
    "As we will learn later in the lecture, GANs achieve their understanding of the true distribution of the example data by comparing generated examples to actual observations and discriminates between them. Therefore GANs have a built-in anomaly/outlier detection capability that can be channeled in order to pin point observations from the original data that differ from the overall distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFor3xcx-IJD"
   },
   "source": [
    "## Domain adaptation üñºÔ∏è->üñºÔ∏è->üíπ\n",
    "\n",
    "Imagine you‚Äôre dealing with an application that is supposed to work on some kind of CCTV cameras, but you have trained your model on high resolution images, you can try to use GANs to preprocess images: de-noising and enhancing them. A more radical example comes from the signal processing area: there are a lot of datasets related to accelerometer data from mobile phones that describe different people's activities. But what if you want to apply your models trained on phone data on wristband devices? GANs can try to help you to translate different kinds of movements. \n",
    "\n",
    "GANs can learn to generate data that look like a certain distribution from data stemming from another prior distribution, this ability makes them interesting tools for adapting data from a given source in order to use a model trained on a different source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk8SlkZm_jng"
   },
   "source": [
    "## Adversarial Training üí£\n",
    "\n",
    "This application looks like data augmentation in the sense that it fits a similar purpose. The contextis the following: lots of very high performing classification models may be completely tipped by small changes in the input data, even changes that our eyes would not even be able to see. Let's see an example of what I am talking about below:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/adversarial_training.png\" />\n",
    "\n",
    "In the above example, we add some random noise to the panda image, forming a new image that is percievably identical to the original one, however the value of the pixels have been perturbated in such a way that the model now predicts the wrong class!\n",
    "\n",
    "You can think of this as data augmentation, why not add some random noise to all our images and let the model train in a way that is perturbation resistant? You absolutely could, the difference is that by using GANs, the generative model will constantly opitmize itself to trick the classification model, leading to potentially even more stable predictions in the end, which let's not forget is the whole purpose of machine learning and even statistics (infer general laws from limited examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHZt7K4QBH9f"
   },
   "source": [
    "# Generator vs. Discirminator model üëπvs.üëÆ\n",
    "\n",
    "It is now time to get into the details of how GANs actually work! You will come to find out that the general idea is very quite simple! As it is often the case in deep learning the difficulty is finding an architecture that will work and train it well! (This why transfer learning comes in very handy in numerous cases and GANs are no exception)\n",
    "\n",
    "The central idea of GANs is the co-existence of two models, one of them is called the generator and the other one is the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmrJOlpgJcL6"
   },
   "source": [
    "## Generator üëπ\n",
    "\n",
    "The role of the generator is to generate data from either noise or from a prior distribution (which is the case for image translation for example). The model will map the input to a feature space that has the same dimension as the real data in order to produce the artificial examples, which we will call the *fake data* in what follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfKpSylwUxTy"
   },
   "source": [
    "### ConvTranspose\n",
    "\n",
    "In order for the generator model to be able to map its input to a feature space that's the same dimension as the real data we will sometimes need to use layers which outputs are bigger than their inputs. Such layers exist and are called Transpose Convolution layers (convolution layers shrink their inputs, these layers expand them).\n",
    "\n",
    "In order to give you an idea of how they work, let us show you an animated example:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/conv2DTranspose.gif\" />\n",
    "\n",
    "The caracteristics of this layer are pretty much the same as the convolution layers:\n",
    "* The kernel size indicates the dimension of the filter, and therefore how many parameters lie there\n",
    "* The stride corresponds to the movement of the filter on the output\n",
    "* Each element of the input is mapped to a *kernel size* dimensional space on the output, forming therefore an output that is larger than the input.\n",
    "\n",
    "This type of layer will be very useful in order to produce artificially generated images! Note that the mechanism is the complete opposite of convolution, convolution layers are able to detect a given pattern anywhere in an input image, convolution transpose on the contrary is able to output a given pattern at different locations on the output depending on the values found on the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-VXrHSWfzBU"
   },
   "source": [
    "## Discriminator üëÆ\n",
    "\n",
    "The goal of the discriminator model is to separate real inputs from fake inputs. **Real inputs** are observation that are taken from the reference dataset, and **Fake inputs** are inputs generated by the generator model.\n",
    "\n",
    "We mentionned earlier that generative models belong to the field of unsupervised learning, that's entirely true, you do not need to label the observations from the reference dataset for themodel to work. However, the clever trick that makes it all work is to convert our problem into a supervised one!\n",
    "\n",
    "In order to translate the data generation problem into a supervised one we do the following: we assign a label to the data we feed to the discriminator model. **Real inputs** are labelled as **real** or $1$ and **fake inputs** are labelled as **fake** or $0$. The mission of the discriminator therfore becomes a supervised classification problem between real and fake inputs.\n",
    "\n",
    "For image generation problems, the discriminator usually is a convolutional neural network for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzE9IXzVhn3o"
   },
   "source": [
    "# Training GANs üèÉüí¶\n",
    "\n",
    "Now that we know how GANs work and what the generator and discriminator models' roles are, it's time to learn how we can train such an architecture. To make everything clearer let's take a look at what the generation problem now looks like:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/GAN1.png\" />\n",
    "\n",
    "1. We feed some random noise (or some data extracted from an input reference dataset if we think of image translation or text to image applications) to the generator.\n",
    "2. The generator produces fake data\n",
    "3. Fake data and real data are fed to the discriminator\n",
    "4. The discriminator produces **fake outputs** (the output formed with the fake data) and **real outputs** (the outputs formed with the real data)\n",
    "\n",
    "The **Real outputs** and **Fake outputs** are what we will base ourselves upon in order to optimise both networks (note that the two networks will train simultaneously).\n",
    "\n",
    "Think of this as a mafia vs. cop kind of situation, the mafia is trying to commit their crimes without getting caught, and the cop is trying to dismantle the mafia while leaving innocent people alone. You guessed is the mafia is the generator and the cop is the discriminator, the criminals are the fake data and the innocent people are the real data. At the initialization step, both the generator/mafia and the discriminator/cop are untrained and terrible, the mafia is not very good a hiding its members and the cops do not know the difference between innoncent people and criminals.\n",
    "\n",
    "Then during training, the discriminator learns to separate fake from real, and the generator is learning how to improve its generation strategy to produce more and more realistic fakes.\n",
    "\n",
    "The conclusion of this metaphora is the following, the mafia is only as good as the cops are. If the cops can be very easily confused, then the mafia does not need to make much of an effort to get away with their crimes, therefore the fake data does not need to be too convincing to get good performance. The cops need to get inceasingly better at detecting criminals to force the mafia to improve its practices! However, if the discriminator trains to fast and is able to arrest all criminals without making any mistake, then the mafia will be stuck and won't be able to train at all. Finding a fine balance between the two models is the key to building good generator models.\n",
    "\n",
    "In order for a model to train, we need to define a loss function that we can optimize by adapting the model's parameters through gradient descent, since we are trying to train two models, we need two loss functions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K-ctYmxms9d"
   },
   "source": [
    "## Discriminator Loss üëÆüí∞\n",
    "\n",
    "We will start with the discriminator loss since it's easier to understand. The discriminator is trying to separate the fake from the real data, therefore its loss function will be the same as any classification network. Since there are to modalities for the target variable the loss function will be a binary cross entropy!\n",
    "\n",
    "If we denote the binary cross entropy as $BCE$ the loss function for the discriminator becomes:\n",
    "\n",
    "$$\n",
    "disc\\_loss = BCE(real\\_output , \\vec{1}) + BCE(fake\\_output , \\vec{0})\n",
    "$$\n",
    "\n",
    "We compare the real output with a vector of the same shape (the shape is equal to the batch size) composed only of ones, and we compare the fake output to a vector composed only of zeros.\n",
    "\n",
    "The discriminator will train in order to get better and better at seperating fake data from real data. Meaning it wins points for classifying real data as real AND classifying fake data as false!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iosBfV-sSB2"
   },
   "source": [
    "## Generator Loss üëπüí∞\n",
    "\n",
    "The generator loss is little trickier to understand, the idea is that we do not really know what a good fake observation looks like (if we did we would not be building this model) the only thing we know is what the discriminator tells us, was the fake data good enough to fool the discriminator or not!\n",
    "\n",
    "That's the idea behind the generator loss, the generator wants to produce fake data that the discriminator will predict as if they were real data, giving the following loss function:\n",
    "\n",
    "$$\n",
    "gen\\_loss = BCE(fake\\_output , \\vec{1})\n",
    "$$\n",
    "\n",
    "This loss function varies in the opposite direction as the discriminator loss! The generator wins when the discriminator's fake outputs are close to one, meaning when the discriminator thinks fake data is real data!\n",
    "\n",
    "Note that the generator loss is not the strict opposite of the discriminator loss, the generator does not care how the discriminator classifies the real data (actually we implicitely want the discriminator to know what real data looks like so the generator has to produce very realistic fakes!) the generator just wants to fool the discriminator (just like the mafia does not care what happens to innocent people as long as they do not get caught)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsZTWvVPwLSZ"
   },
   "source": [
    "## Traning Step üèÉ\n",
    "\n",
    "The following figure sums up the training process for GANs:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/M08-DeepLearning/gans/GAN2.png\" />\n",
    "\n",
    "The training will happen in the following way, a batch of noise (or data from a prior distribution) is fed to the generator, which produces a batch of fake data. The batch of fake data is fed to the discriminator and used to compute the $gen\\_loss$ and a gradient descent step updates the generator's parameters. Then a batch of real data is fed to the discriminator, the real and fake outputs are used to compute the $disc\\_loss$ and the parameters of the discriminator model are updated through a gradient descent step.\n",
    "\n",
    "This process is then iterated several times until the fake outputs \"look\" realistic enough (this is the part where human intervention is needed, don't forget that this is unsupervised learning and that the models have no idea how to measure their actual performance, only human expertise can tell wether the generated data is convincing or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhglbNOow1Rl"
   },
   "source": [
    "## Conclusion üîöüîö\n",
    "\n",
    "GANs are an exciting and rapidly changing field, delivering on the promise of generative models in their ability to generate realistic examples across a range of problem domains, most notably in image-to-image translation tasks such as translating photos of summer to winter or day to night, and in generating photorealistic photos of objects, scenes, and people that even humans cannot tell are fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "922M4bgzNyuo"
   },
   "source": [
    "## Ressources üìöüìö\n",
    "\n",
    "* <a href=\"https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/\"> A blog post introducing GANs </a>\n",
    "* <a href=\"https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/\"> Another blog post detailing applications of GANs </a>\n",
    "* <a href=\"https://alexrachnog.medium.com/gans-beyond-generation-7-alternative-use-cases-725c60ba95e8\"> Non generative applications of GANs </a>\n",
    "* <a href=\"https://arxiv.org/abs/1406.2661\"> Original paper introducing GANs </a>\n",
    "* <a href=\"https://www.deeplearningbook.org/\"> One of the reference books in the field of deep learning, the last chapter talks about generative models </a>\n",
    "* <a href=\"https://arxiv.org/pdf/1706.08224.pdf\"> A fascinating paper on the limits of GANs and how to evaluate performance </a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPJS1pj7WcTS/PpXEahCcOS",
   "collapsed_sections": [],
   "name": "Understand_GANs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
